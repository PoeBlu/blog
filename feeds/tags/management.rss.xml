<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jason Antman's Blog</title><link>http://blog.jasonantman.com/</link><description></description><atom:link href="http://blog.jasonantman.com/feeds/tags/management.rss.xml" rel="self"></atom:link><lastBuildDate>Sat, 07 Feb 2009 01:15:00 -0500</lastBuildDate><item><title>My Dream Network</title><link>http://blog.jasonantman.com/2009/02/my-dream-network/</link><description>&lt;p&gt;On the same thread as the &lt;a href="/2009/02/community-datacenter/"&gt;last
post&lt;/a&gt;, some
thoughts on my ideal network, or the hosts on that&amp;nbsp;network:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One &amp;#8220;gold master&amp;#8221; installation/kickstart file of a single chosen
    distro, with a base set of packages, including site-specific
    packages. (or something like this implemented in a configuration
    management&amp;nbsp;system)&lt;/li&gt;
&lt;li&gt;All new installations performed over the network in an automated
    fashion, and from a local&amp;nbsp;repository.&lt;/li&gt;
&lt;li&gt;Software updates are automated (either through a configuration
    management tool or something like the behemoth I talked about
    &lt;a href="/2008/10/my-biggest-problem-with-linux/"&gt;here&lt;/a&gt;)
    and pulled from a local repository (perhaps one which mirrors the
    mainline repos, but only downloads a package the first time it&amp;#8217;s&amp;nbsp;requested?).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://reductivelabs.com/trac/puppet"&gt;Puppet&lt;/a&gt; or
    &lt;a href="http://www.cfengine.org/"&gt;CFengine&lt;/a&gt; on each machine. Better than
    just having them is having each machine automatically added when
    it&amp;#8217;s created. Even better yet would be to have Puppet or CFengine
    combined with something like
    &lt;a href="https://fedorahosted.org/cobbler/"&gt;Cobbler&lt;/a&gt;, so I can define a new
    machine in {puppet|cfengine}, list its&amp;#8217; &lt;span class="caps"&gt;MAC&lt;/span&gt; address, then netboot
    the box and come back in a few hours to have an &lt;span class="caps"&gt;OS&lt;/span&gt; installed,
    packages installed and the machine configured, monitored in Nagios,
    monitored for security and backed&amp;nbsp;up.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.tripwire.com/"&gt;Tripwire&lt;/a&gt; or some other sort of security
    software, as well as centralized logging and auditing, on every&amp;nbsp;box.&lt;/li&gt;
&lt;li&gt;A small number of additional &amp;#8220;package groups&amp;#8221; to add to the &amp;#8220;gold
    master&amp;#8221; via config management - something like &amp;#8220;web server&amp;#8221;
    (Apache2, &lt;span class="caps"&gt;PHP&lt;/span&gt;, MySQL, log analysis for them, etc.), &amp;#8220;development
    server&amp;#8221; (&lt;span class="caps"&gt;CVS&lt;/span&gt;, debuggers, etc.). These would also update the backup
    system to include appropriate directories, update Nagios configs,&amp;nbsp;etc.&lt;/li&gt;
&lt;li&gt;A good way - if even a human making notes in a per-machine text file&lt;ul&gt;
&lt;li&gt;of tracking the &amp;#8220;little stuff&amp;#8221; like that one cron script that
makes everything work right, the location of that hacked-together
Python script, etc. A way to easily remember the things needed to
recreate a box which aren&amp;#8217;t found in &lt;code&gt;rpm -qa&lt;/code&gt; or any obvious&amp;nbsp;overviews.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.bacula.org"&gt;Bacula&lt;/a&gt; or &lt;a href="http://www.amanda.org/"&gt;&lt;span class="caps"&gt;AMANDA&lt;/span&gt;&lt;/a&gt;
    setup to backup every box, perhaps with some sort of template system
    for server types - every machine gets &lt;code&gt;/root&lt;/code&gt; and &lt;code&gt;/etc&lt;/code&gt; backed up,
    but web servers get &lt;code&gt;/srv/www&lt;/code&gt; and mail servers get &lt;code&gt;/var/mail&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Nagios setup to monitor everything logical on every box. Perhaps
    this would use a configuration management engine to handle Nagios
    configs, so that for example if any Proliant hardware is used,
    {config management program} will figure this out, install the &lt;span class="caps"&gt;HPASM&lt;/span&gt;
    packages, put the appropriate check scripts on the box, and update
    the Nagios configs. Likewise, adding Apache to a machine should
    cause it to be monitored in the Nagios&amp;nbsp;configs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Unfortunately, as I&amp;#8217;m not independently wealthy, I don&amp;#8217;t have the time
to quit my job, wipe every machine I own, and start from scratch. But it
sure would be nice to be able to, one day, start a server farm from
scratch and be able to implement some of these cool&amp;nbsp;things&amp;#8230;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Sat, 07 Feb 2009 01:15:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2009-02-07:2009/02/my-dream-network/</guid><category>configuration</category><category>dream network</category><category>ideal network</category><category>management</category></item><item><title>Managing G1 Proliant Servers with modern Linux</title><link>http://blog.jasonantman.com/2007/03/managing-g1-proliant-servers-with-modern-linux/</link><description>&lt;p&gt;Not much of an &amp;#8220;upgrade&amp;#8221; for anyone who&amp;#8217;s in &lt;span class="caps"&gt;IT&lt;/span&gt;, but jasonantman.com is
currently being upgraded from old desktops used as servers to a pile of
generation-1 (G1) &lt;span class="caps"&gt;HP&lt;/span&gt;/Compaq Proliants. I know that there are utilities
for Linux to manage the servers, specifically control fan speed and
monitor hardware-level health for Linux. However, the most recent
download on &lt;span class="caps"&gt;HP&lt;/span&gt;&amp;#8217;s site is for &lt;span class="caps"&gt;SLES9&lt;/span&gt;. All of my boxes will be running
openSuSE 10.2, and the &lt;span class="caps"&gt;SLES9&lt;/span&gt; version wouldn&amp;#8217;t install on&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;After an hour long phone call to &lt;span class="caps"&gt;HP&lt;/span&gt; support, I ended up speaking with
Paulo, the third support person I was transferred to. #1 read off the
web site, #2 knew what Linux was, but Paulo (#3) actually told me that
he was experimenting with installing &lt;span class="caps"&gt;HPASM&lt;/span&gt; (&lt;span class="caps"&gt;HP&lt;/span&gt;&amp;#8217;s server
administration/management utility) on an older Proliant as well. He
spent about half an hour walking me through it. Here&amp;#8217;s what I&amp;nbsp;found:&lt;/p&gt;
&lt;p&gt;The most compatible version of &lt;span class="caps"&gt;HPASM&lt;/span&gt; (I guess it&amp;#8217;s some hidden feature
for people who know it) is the version for the &lt;span class="caps"&gt;DL380&lt;/span&gt; G4. Paulo
instructed me to download this &lt;span class="caps"&gt;RPM&lt;/span&gt; from their site. I did, choosing the
&lt;span class="caps"&gt;SLES10&lt;/span&gt; (x86) download (hpasm-7.7.0-115.sles10.i586.rpm). This installed
fine. Running &lt;code&gt;hpasm status&lt;/code&gt; from the command line asks us to activate
it first. Do the activation. Now, running &lt;code&gt;hpasm status&lt;/code&gt; still asks us
to activate. Paulo confirmed this as happening on his machine too. Try
&lt;code&gt;/etc/init.d/hpasm status&lt;/code&gt; and you should see that all of the modules
are&amp;nbsp;working.&lt;/p&gt;
&lt;p&gt;Now, the install is complete. I&amp;#8217;m not sure if the &lt;span class="caps"&gt;SNMP&lt;/span&gt; works, but it
should as long as your snmpd is running. The &lt;code&gt;hpasm activate&lt;/code&gt; command
modifies snmpd.conf appropriately. and you will be queried for the
currect configuration&amp;nbsp;information.&lt;/p&gt;
&lt;p&gt;To give it a test, run &lt;code&gt;hplog -f&lt;/code&gt; or &lt;code&gt;hplog -p&lt;/code&gt; and you should see fan
and power status,&amp;nbsp;respectively.&lt;/p&gt;
&lt;p&gt;Paulo also told me that I could download the hpadu package (also &lt;span class="caps"&gt;DL380&lt;/span&gt;
G4 / &lt;span class="caps"&gt;SLES10&lt;/span&gt;) to get array diagnostics, He warned me that some of the
install scripts in &lt;span class="caps"&gt;HPADU&lt;/span&gt; look for the web management homepage, which we
haven&amp;#8217;t installed. To get around this, install the &lt;span class="caps"&gt;HPADU&lt;/span&gt; &lt;span class="caps"&gt;RPM&lt;/span&gt; file
(hpadu-7.70-12.linux.rpm)
&lt;code&gt;rpm -ivh --force --nodeps --noscripts hpadu-7.70-12.linux.rpm&lt;/code&gt;. Be
aware, though, that this package is supposed to be web-based. It
installs to&amp;nbsp;/opt/hp/hpadu.&lt;/p&gt;
&lt;p&gt;The web interface, luckily for me, is written in &lt;span class="caps"&gt;PHP&lt;/span&gt;. It is pretty
complex so it might take me a while to figure out the workings, but when
I do, I&amp;#8217;ll post as much info as I can on how to make a &lt;span class="caps"&gt;CLI&lt;/span&gt; interface, or
where one exists if I can find&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;Also, I&amp;#8217;ll most likely develop a Python check script to use with
&lt;a href="http://www.nagios.org"&gt;Nagios&lt;/a&gt; to monitor most of the hpasm-enabled&amp;nbsp;components.&lt;/p&gt;
&lt;p&gt;For the use of anyone else, here are some of the links that &lt;span class="caps"&gt;HP&lt;/span&gt; Support
sent me after the&amp;nbsp;call:&lt;/p&gt;
&lt;p&gt;Link for users guide for Proliant Support Pack, which includes
documentation on &lt;span class="caps"&gt;HPASM&lt;/span&gt; from the&amp;nbsp;&lt;span class="caps"&gt;CLI&lt;/span&gt;:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://h18000.www1.hp.com/support/files/server/us/WebDoc/720/psp-users-guide.pdf"&gt;http://h18000.www1.hp.com/support/files/server/us/WebDoc/720/psp-users-guide.pdf&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Product&amp;nbsp;manuals:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://h20180.www2.hp.com/apps/Nav?h_pagetype=s-003&amp;amp;h_lang=en&amp;amp;h_cc=us&amp;amp;h_product=241435&amp;amp;h_page=hpcom&amp;amp;h_client=z-a-r1002-3&amp;amp;cc=us〈=en"&gt;http://h20180.www2.hp.com/apps/Nav?h_pagetype=s-003&amp;amp;h_lang=en&amp;amp;h_cc=us&amp;amp;h_product=241435&amp;amp;h_page=hpcom&amp;amp;h_client=z-a-r1002-3&amp;amp;cc=us〈=en&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;ML370&lt;/span&gt;&amp;nbsp;G1&lt;/p&gt;
&lt;p&gt;&lt;a href="http://h20000.www2.hp.com/bc/docs/support/UCR/SupportManual/TPM_143091-004/TPM_143091-004.pdf"&gt;http://h20000.www2.hp.com/bc/docs/support/&lt;span class="caps"&gt;UCR&lt;/span&gt;/SupportManual/TPM_143091-004/TPM_143091-004.pdf&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Thu, 01 Mar 2007 16:27:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2007-03-01:2007/03/managing-g1-proliant-servers-with-modern-linux/</guid><category>compaq</category><category>hp</category><category>hpadu</category><category>hpasm</category><category>linux</category><category>management</category><category>proliant</category><category>psp</category></item></channel></rss>