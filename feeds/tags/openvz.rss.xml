<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jason Antman's Blog</title><link>http://blog.jasonantman.com/</link><description></description><atom:link href="http://blog.jasonantman.com/feeds/tags/openvz.rss.xml" rel="self"></atom:link><lastBuildDate>Fri, 19 Mar 2010 20:55:00 -0400</lastBuildDate><item><title>VirtualizationÂ Options</title><link>http://blog.jasonantman.com/2010/03/virtualization-options/</link><description>&lt;p&gt;As I mentioned in &lt;a href="/2010/03/downtime-past-few-days-coping-with-storms/"&gt;Downtime past few days, coping with
storms&lt;/a&gt;, as a
result of some things I noticed with a recent power outage, I&amp;#8217;ve decided
to take the leap to virtualization. Given the cost of current hardware
that supports &lt;span class="caps"&gt;HVM&lt;/span&gt; (Intel &lt;span class="caps"&gt;VT&lt;/span&gt;-x or &lt;span class="caps"&gt;AMD&lt;/span&gt;-V ), I immediately decided that I
might as well give up on any thoughts of doing full virtualization or
getting new-ish hardware. So I settled on the next step up from what
have now - a set of &lt;a href="http://h18000.www1.hp.com/products/quickspecs/11504_na/11504_na.HTML"&gt;&lt;span class="caps"&gt;HP&lt;/span&gt; Proliant &lt;span class="caps"&gt;DL360&lt;/span&gt;
G3&lt;/a&gt;
servers. I got them with a 90 day warranty from a reputable dealer, dual
2.8GHz Xeon (512K cache), 2Gb &lt;span class="caps"&gt;RAM&lt;/span&gt;, dual 36.4Gb U320 15k &lt;span class="caps"&gt;RPM&lt;/span&gt; &lt;span class="caps"&gt;SCSI&lt;/span&gt; disks
and dual power supplies for $99 each. My next step is to decide what
virtualization software to&amp;nbsp;use.&lt;/p&gt;
&lt;p&gt;My main goals for the project&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Lower power consumption through consolidation of&amp;nbsp;servers.&lt;/li&gt;
&lt;li&gt;Possibility to add capacity or resources by remotely powering up an
    idle server and migrating VMs to&amp;nbsp;it.&lt;/li&gt;
&lt;li&gt;Limited fault tolerance - ability to manually restore a &lt;span class="caps"&gt;VM&lt;/span&gt; that was
    running on failed hardware, onto an idle&amp;nbsp;server.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I originally thought Xen, just out of reflex. However, given that all of
my servers have the same base - the same distribution and, ideally, the
same kernel and patch level - it seemed like a lot of overhead to
duplicate that for multiple VMs. So I started looking into &lt;a href="http://en.wikipedia.org/wiki/Operating_system-level_virtualization"&gt;&lt;span class="caps"&gt;OS&lt;/span&gt;-level
virtualization&lt;/a&gt;.
There are relatively few options, and I&amp;#8217;ll admit that aside from Solaris
Containers (which I learned about while working at Sun) I don&amp;#8217;t know
much about it. But &lt;a href="http://www.openvz.org/"&gt;OpenVZ&lt;/a&gt; seems to be the
front runner in that area. My initial impression was that it made a lot
of sense - keep one common kernel, but allow containers/virtual
environments (CTs/VEs) to have, essentially, their own userland.
Unfortunately, it doesn&amp;#8217;t seem to be as hyped as Xen, and I haven&amp;#8217;t
heard very much about it in the enterprise context. And it requires
running a kernel from the OpenVZ project, which means I can&amp;#8217;t just
script updates through yum as easily as&amp;nbsp;normal.&lt;/p&gt;
&lt;p&gt;On the up size, OpenVZ would allow me to eliminate the duplication of
the kernel, and seems to have much less overhead than Xen (and logically
so). On the down side, I lose the ability to virtualize other OSes,
kernel versions, or make pre-packaged VMs. I&amp;#8217;ve decided that if I wanted
to do that, I could dedicate a single&amp;nbsp;machine.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve spent the last day or so doing a lot of research, and have come up
with the following questions and concerns about OpenVZ which I hope to
be able to answer (I&amp;#8217;ll post the answers in a&amp;nbsp;follow-up).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How do I handle distribution and kernel upgrades? The logical
    solution would be to migrate the &lt;span class="caps"&gt;CT&lt;/span&gt; to another host while I upgrade
    &lt;span class="caps"&gt;CT0&lt;/span&gt; (the hardware &lt;span class="caps"&gt;OS&lt;/span&gt;/host/dom0 in Xen speak). But if the guest and
    host kernels must match, how does this&amp;nbsp;work?&lt;/li&gt;
&lt;li&gt;Can I do package upgrades within the guest/&lt;span class="caps"&gt;CT&lt;/span&gt; easily? WIll this play
    well with&amp;nbsp;Puppet?&lt;/li&gt;
&lt;li&gt;How will I handle backups? Is it logical to run
    &lt;a href="http://www.bacula.org"&gt;bacula&lt;/a&gt; within each &lt;span class="caps"&gt;CT&lt;/span&gt;, or just on &lt;span class="caps"&gt;CT0&lt;/span&gt;? If
    just on &lt;span class="caps"&gt;CT0&lt;/span&gt;, how do I easily verify that a particular &lt;span class="caps"&gt;CT&lt;/span&gt; was backed&amp;nbsp;up?&lt;/li&gt;
&lt;li&gt;WIll everything play well with Puppet? (see&amp;nbsp;below)&lt;/li&gt;
&lt;li&gt;Am I willing to throw away my KickStart-based installs? And,
    similarly, am I willing to give up the possibility of migrating from
    a container to a Xen host or a physical host&amp;nbsp;(easily)?&lt;/li&gt;
&lt;li&gt;OpenVZ live migration relies on rsync. This means that there&amp;#8217;s a
    significant delay (compared to shared storage) and also that I can&amp;#8217;t
    migrate off of a host that&amp;#8217;s down. Is there a way around&amp;nbsp;this?&lt;/li&gt;
&lt;li&gt;Similarly, live migration requires root &lt;span class="caps"&gt;SSH&lt;/span&gt; key exchange
    (passwordless) between the hosts. This seems about equivalent to
    using &lt;code&gt;hosts.equiv&lt;/code&gt;. Do I really want root on one box to mean root
    on another box (and all of the containers on that&amp;nbsp;box)?&lt;/li&gt;
&lt;li&gt;Can I still firewall &lt;span class="caps"&gt;CT0&lt;/span&gt;? How will this&amp;nbsp;work?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It seems to me that OpenVZ may be significantly less enterprise-class
than Xen. Sure, this is just my home setup, but I hold it to the same
standards I use for my work systems. In fact, I usually test new
technologies at home before I suggest them at work. A lot of the writing
on the &lt;a href="http://wiki.openvz.org/"&gt;OpenVZ wiki&lt;/a&gt; seems to be riddled with
spelling errors. They claim &amp;#8220;zero downtime&amp;#8221; live migration, but if they
have to rsync 2Gb of MySQL tables, that sounds like a lot more than
&amp;#8220;zero&amp;#8221;. And, most shockingly, the &lt;a href="http://wiki.openvz.org/Hardware_testing"&gt;Hardware
testing&lt;/a&gt; wiki page talks about
making sure your hosts aren&amp;#8217;t overclocked or undercooled, and running
&lt;code&gt;cpuburn&lt;/code&gt; to test your system under high load. Sorry, but the engineers
at &lt;span class="caps"&gt;HP&lt;/span&gt;, Sun, &lt;span class="caps"&gt;IBM&lt;/span&gt;, etc. handle that for me and most people I know. So, I&amp;#8217;m
a bit worried about the seriousness of the OpenVZ&amp;nbsp;project.&lt;/p&gt;
&lt;p&gt;Most worrisome is a post I found in the &lt;a href="http://forum.openvz.org"&gt;OpenVZ
forum&lt;/a&gt;, &lt;a href="http://forum.openvz.org/index.php?t=msg&amp;amp;goto=14818&amp;amp;"&gt;&amp;#8220;Stopping puppet on hn stops it in all
&lt;span class="caps"&gt;VE&lt;/span&gt;&amp;#8221;&lt;/a&gt;. It seems
that, since &lt;span class="caps"&gt;CT0&lt;/span&gt; is aware of all of the guest container processes, they
show up in ps lists. Most, if not all RedHat init scripts use killproc
to stop and restart services. This means that a &lt;code&gt;service syslog stop&lt;/code&gt; on
the &lt;span class="caps"&gt;CT0&lt;/span&gt; (host) will stop &lt;strong&gt;all&lt;/strong&gt; &lt;code&gt;syslog&lt;/code&gt; processes, including all of
them in the CTs. This seems like a major issue. Sure, I could replace
&lt;code&gt;killproc&lt;/code&gt; on &lt;span class="caps"&gt;CT0&lt;/span&gt; with a script that parses the process list, isolates
the PIDs for those running on &lt;span class="caps"&gt;CT0&lt;/span&gt;, and kills them. But what else needs
to be fixed? Nagios check scripts would need to be adjusted. Is there
anything else that would come back and bite&amp;nbsp;me?&lt;/p&gt;
&lt;p&gt;The bottom line is that (I guess this is logical) it seems that
containers in OpenVZ will seem - and act - a lot less like a logical
host than they would under&amp;nbsp;Xen.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Fri, 19 Mar 2010 20:55:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2010-03-19:2010/03/virtualization-options/</guid><category>OpenVZ</category><category>virtualization</category><category>xen</category></item></channel></rss>