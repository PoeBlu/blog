<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jason Antman's Blog - blackberry</title><link href="https://blog.jasonantman.com/" rel="alternate"></link><link href="https://blog.jasonantman.com/feeds/tags/blackberry.atom.xml" rel="self"></link><id>https://blog.jasonantman.com/</id><updated>2011-10-13T22:20:00-04:00</updated><entry><title>Blackberry Oops</title><link href="https://blog.jasonantman.com/2011/10/blackberry-oops/" rel="alternate"></link><published>2011-10-13T22:20:00-04:00</published><updated>2011-10-13T22:20:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2011-10-13:/2011/10/blackberry-oops/</id><summary type="html">&lt;p&gt;If you&amp;#8217;ve been following the tech news lately, you&amp;#8217;ve probably heard at
least a bit about the
&lt;a href="http://www.cnn.com/2011/10/12/tech/mobile/blackberry-outage/"&gt;massive&lt;/a&gt;
&lt;a href="http://abcnews.go.com/blogs/technology/2011/10/blackberry-outage-spreads-to-u-s/"&gt;blackberry&lt;/a&gt;
&lt;a href="http://www.nytimes.com/2011/10/14/technology/rim-struggles-to-overcome-blackberry-outages.html?_r=1"&gt;outage&lt;/a&gt;
over the past three days. While yes, it&amp;#8217;s the first truly grand failure
of &lt;span class="caps"&gt;RIM&lt;/span&gt;&amp;#8217;s infrastructure in their 12-year history, it&amp;#8217;s also a wonderful
case …&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you&amp;#8217;ve been following the tech news lately, you&amp;#8217;ve probably heard at
least a bit about the
&lt;a href="http://www.cnn.com/2011/10/12/tech/mobile/blackberry-outage/"&gt;massive&lt;/a&gt;
&lt;a href="http://abcnews.go.com/blogs/technology/2011/10/blackberry-outage-spreads-to-u-s/"&gt;blackberry&lt;/a&gt;
&lt;a href="http://www.nytimes.com/2011/10/14/technology/rim-struggles-to-overcome-blackberry-outages.html?_r=1"&gt;outage&lt;/a&gt;
over the past three days. While yes, it&amp;#8217;s the first truly grand failure
of &lt;span class="caps"&gt;RIM&lt;/span&gt;&amp;#8217;s infrastructure in their 12-year history, it&amp;#8217;s also a wonderful
case&amp;nbsp;study.&lt;/p&gt;
&lt;p&gt;Apparently the outage started Monday morning with &lt;span class="caps"&gt;RIM&lt;/span&gt; infrastructure in
Europe, the Middle East and Asia. However, by Wednesday, it had become a
global outage/slowdown of BlackBerry infrastructure (specifically the
parts that go through &lt;span class="caps"&gt;RIM&lt;/span&gt; - email, web browsing, and &lt;span class="caps"&gt;BBM&lt;/span&gt;; voice calls
and &lt;span class="caps"&gt;SMS&lt;/span&gt;/&lt;span class="caps"&gt;MMS&lt;/span&gt; were unaffected). With BlackBerry&amp;#8217;s market share falling,
Android&amp;#8217;s rapidly growing (and Android slowly becoming a viable
enterprise option), and the launch of the iPhone 4S just around the
corner, the timing of this couldn&amp;#8217;t be worse for&amp;nbsp;&lt;span class="caps"&gt;RIM&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;RIM&lt;/span&gt;&amp;#8217;s original
&lt;a href="http://www.rim.com/newsroom/service-update.shtml"&gt;statement&lt;/a&gt; about the
problem, at 21:30 on Tuesday October 11th,&amp;nbsp;was,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The messaging and browsing delays that some of you are still
experiencing were caused by a core switch failure within &lt;span class="caps"&gt;RIM&lt;/span&gt;’s
infrastructure. Although the system is designed to failover to a
back-up switch, the failover did not function as previously tested. As
a result, a large backlog of data was generated and we are now working
to clear that backlog and restore normal service as quickly as
possible. We sincerely apologise for the inconvenience caused to many
of you and we will continue to keep you&amp;nbsp;informed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I haven&amp;#8217;t been able to find much more technical information than that -
a &lt;a href="http://www.computerworld.com/s/article/9220736/RIM_global_outage_caused_by_core_switch_failure_fix_under_way"&gt;&lt;span class="caps"&gt;CNET&lt;/span&gt; article from
Tuesday&lt;/a&gt;
goes into as much depth as anything I could find. I did find one mention
(misplaced the link) that the core switch in question uses technology
from &amp;#8220;multiple vendors&amp;#8221;. So what follows is part common sense (for me&amp;#8230;
why not for a multi-national corporation?) and part speculation. If
you&amp;#8217;re unfamiliar with &lt;span class="caps"&gt;RIM&lt;/span&gt;&amp;#8217;s architecture, the pertinent points are that
all Internet-bound traffic (browsing, email, and &lt;span class="caps"&gt;BBM&lt;/span&gt;) is piped through
&lt;span class="caps"&gt;RIM&lt;/span&gt;&amp;#8217;s data centers, where it&amp;#8217;s encypted and who-knows-what-else&amp;#8217;ed
(perhaps
&lt;a href="http://online.wsj.com/article/SB10001424052970204612504576608561811929654.html"&gt;monitored&lt;/a&gt;)
before going back out onto the &amp;#8216;net. In the Enterprise market, their big
claim is encryption/security, and monitoring/management/policy
enforcement on&amp;nbsp;handsets.&lt;/p&gt;
&lt;p&gt;First main point: &lt;span class="caps"&gt;RIM&lt;/span&gt; is a &lt;em&gt;big&lt;/em&gt; company. The thought that they rely on
an (apparently custom) core switch - a &lt;em&gt;single&lt;/em&gt; core switch for multiple
&lt;em&gt;continents&lt;/em&gt; - is amazing. It&amp;#8217;s even more amazing that they&amp;#8217;d let such a
large part of their infrastructure ride on an architecture with,
apparently, an untested failover mechanism. Of course I don&amp;#8217;t know all
the details, but I&amp;#8217;d hope that for a single piece of hardware which is
so critical, they&amp;#8217;d a) have a cold spare physically nearby so a
replacement wouldn&amp;#8217;t take a day or two, and b) if they can&amp;#8217;t do an
online failover test, at least have a full lab environment to test the
failover&amp;nbsp;in.&lt;/p&gt;
&lt;p&gt;Second main point: Their big claim through all of this is that they
didn&amp;#8217;t lose any data - email, &lt;span class="caps"&gt;BBM&lt;/span&gt;, etc. - it just got delayed. So if
there was a core switch failure in their data center serving &lt;span class="caps"&gt;EMEA&lt;/span&gt;, and
the next day global services slowed to a crawl, the only thing that
comes to mind to me is a waterfall; &lt;span class="caps"&gt;EMEA&lt;/span&gt; went down, and they started
rerouting traffic to their North America data center. The increased load
- probably a disaster recovery plan they never truly tested or even
planned - brought everything to a screeching halt, and caused them to
resort to simply caching messaging and pushing it out bit by bit as the
infrastructure could handle.&amp;nbsp;Oops.&lt;/p&gt;
&lt;p&gt;So what are my (admittedly poorly-informed) thoughts on&amp;nbsp;this?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Scaling out works. Scaling up - especially with single points of
    failure, or N+1 redundancy - is dangerous. If &lt;span class="caps"&gt;RIM&lt;/span&gt; had scaled out and
    used regional data centers, with a close-to-commodity core and as
    much redundancy as possible, this wouldn&amp;#8217;t have happened. Sure,
    infrastructure costs money. But if that one &amp;#8220;core switch&amp;#8221; had been
    1,000 devices spread across multiple racks in multiple data centers,
    this never would have happened. And the devices would be
    comparatively cheap enough to probably keep spares on hand too. And
    regularly test their failover procedures. To all of the big
    businesses (apparently like &lt;span class="caps"&gt;RIM&lt;/span&gt;) who still think that big iron is
    the only way to do things right&amp;#8230; maybe it&amp;#8217;s time to take a hard
    look at that, and compare your architecture to that of the modern,
    new, hip giants like Google and Facebook. Grids and clusters. Nodes
    that can fail without anyone blinking. Scaling out might not fix
    every problem, but having half the world on a single core switch
    with N+1 redundancy probably isn&amp;#8217;t smart&amp;nbsp;either.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Disasters need to be planned for. Every possible contingency needs
    to be planned for. Plans need to be tested, regularly. If your giant
    core switch goes down and the failover doesn&amp;#8217;t &amp;#8220;function as
    previously tested&amp;#8221;, there&amp;#8217;s a serious problem both in your disaster
    planning, and in your validation and test procedure. If you have
    half of your customer base riding on a failover plan that isn&amp;#8217;t
    &lt;em&gt;regularly&lt;/em&gt; tested or otherwise validated, that&amp;#8217;s bad. While I can
    argue that the whole architecture - given this massive point of
    failure - could stand to be re-thought, the real issue here is with
    test/validation methodologies and procedures. For a company with 70
    million users, having a piece of infrastructure this critical fail,
    and the failover &amp;#8216;not work as tested&amp;#8217; is a very serious&amp;nbsp;issue.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Damage control is important. As I said, I can only imagine that the
    severe service degradation outside of &lt;span class="caps"&gt;EMEA&lt;/span&gt; was due to rerouting data
    from &lt;span class="caps"&gt;EMEA&lt;/span&gt; to the remaining functional data center(s). Such a
    solution should only be considered if it has been tested, or at
    least has engineering validation. If it was part of the disaster
    recovery plan, it obviously isn&amp;#8217;t actually a suitable solution, and
    served only to increase the scope of the outage. If it wasn&amp;#8217;t part
    of the disaster recovery plan, and was decided on-the-fly, someone
    really didn&amp;#8217;t do their research and engineering before putting the
    fix in place. A workable solution should have been planned ahead of
    time. And if one wasn&amp;#8217;t, it&amp;#8217;s very bad practice - this outage shows
    the results - to put in place a fix that hasn&amp;#8217;t been thought&amp;nbsp;out.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For a company whose business is so telecom-focused, this seems like a
glaringly bad design that shouldn&amp;#8217;t be acceptable in a&amp;nbsp;telecom.&lt;/p&gt;</content><category term="blackberry"></category><category term="outage"></category></entry></feed>