<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jason Antman's Blog - amazon</title><link href="http://blog.jasonantman.com/" rel="alternate"></link><link href="http://blog.jasonantman.com/feeds/tags/amazon.atom.xml" rel="self"></link><id>http://blog.jasonantman.com/</id><updated>2018-04-01T14:36:00-04:00</updated><entry><title>AWS ElasticSearch for Ad-Hoc ELB LogÂ Analysis</title><link href="http://blog.jasonantman.com/2018/04/aws-elasticsearch-for-ad-hoc-elb-log-analysis/" rel="alternate"></link><published>2018-04-01T14:36:00-04:00</published><updated>2018-04-01T14:36:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2018-04-01:/2018/04/aws-elasticsearch-for-ad-hoc-elb-log-analysis/</id><summary type="html">&lt;p&gt;How I used &lt;span class="caps"&gt;AWS&lt;/span&gt; hosted ElasticSearch for ad-hoc analysis of &lt;span class="caps"&gt;ELB&lt;/span&gt;&amp;nbsp;logs.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;This is a slightly modified and sanitized version of a post that I wrote on my employer&amp;#8217;s internal collaboration site. I&amp;#8217;m sharing it here because I wasn&amp;#8217;t able to find any clear guide to this exact process, and because I&amp;#8217;ve been really bad at keeping up with my blog&amp;nbsp;lately.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;At work my team runs an internal instance of &lt;a href="https://jfrog.com/artifactory/features/"&gt;Artifactory&lt;/a&gt;, a binary artifact repository manager that stores and serves artifacts for multiple software packaging systems (we use it for Ruby Gems, Maven packages, &lt;span class="caps"&gt;NPM&lt;/span&gt;, PyPI and Docker images). Many teams within our company rely on this service in their build and deploy processes. Last week we suffered a severe degradation of Ruby Gem serving from our Artifactory instance. One of the other engineers on my team was able to identify and resolve the problem based on intuition and experience, but I&amp;#8217;d like to walk through a process that I used to collect some detailed data for troubleshooting and confirmation of the&amp;nbsp;cause.&lt;/p&gt;
&lt;p&gt;Our Artifactory service runs in &lt;span class="caps"&gt;AWS&lt;/span&gt; as a &lt;span class="caps"&gt;HA&lt;/span&gt; cluster of three &lt;span class="caps"&gt;EC2&lt;/span&gt; instances fronted by a classic Elastic Load Balancer (&lt;span class="caps"&gt;ELB&lt;/span&gt;), with a private address only so that it&amp;#8217;s only available internally. Artifactory itself is a Java application running under Tomcat Catalina, with repository metadata stored in a MySQL &lt;span class="caps"&gt;RDS&lt;/span&gt; cluster and binary artifacts stored in S3. Artifacts are organized into repositories, with each repository containing a specific set of artifacts of a given type (i.e. Maven, gems, &lt;span class="caps"&gt;NPM&lt;/span&gt;, etc. In addition to storing and serving artifacts locally, Artifactory also has the capability to configure remote repositories that retrieve and cache artifacts from remote servers, and &amp;#8220;virtual&amp;#8221; repositories that combine two or more local or remote repositories into one unified&amp;nbsp;view.&lt;/p&gt;
&lt;p&gt;My first reaction was to begin with the high-level monitoring of the &lt;span class="caps"&gt;ELB&lt;/span&gt; provided by CloudWatch. While I&amp;#8217;d expected to see various metrics (most likely &lt;span class="caps"&gt;HTTP&lt;/span&gt; 4xx or 5xx errors) far from their historical baseline, the only obvious deviations from normal were number of connections and backend request time (the time the actual Artifactory instances were taking to handle requests). While the rest of my team was also investigating, I attempted to start analyzing the Artifactory logs in our Splunk instance (our company&amp;#8217;s official Enterprise Log Solution). Unfortunately, I quickly found that the logs were not very useful. First, the logs contained a high volume of noise: repeated and persistent errors present for a very long time, overly verbose logs for certain actions, and Java stack traces split across dozens or hundreds of separate log entries in Splunk. Second, and more importantly, Artifactory was logging requests as coming from the &lt;span class="caps"&gt;IP&lt;/span&gt; address of the load balancer instead of the actual client and was missing other key pieces of information such as the user-agent and time taken to serve the request. It seemed clear to me that these logs weren&amp;#8217;t going to provide as much information as I&amp;#8217;d&amp;nbsp;hoped.&lt;/p&gt;
&lt;p&gt;Luckily I found that we had &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html"&gt;&lt;span class="caps"&gt;ELB&lt;/span&gt; access logging&lt;/a&gt; enabled on the load balancer, sending access logs to S3 once an hour. Unfortunately, we weren&amp;#8217;t doing anything with those logs and I don&amp;#8217;t believe they had ever been referenced before; we certainly weren&amp;#8217;t ingesting them into Splunk or any other centralized log store. But the &lt;a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/access-log-collection.html#access-log-entry-format"&gt;&lt;span class="caps"&gt;ELB&lt;/span&gt; access log format&lt;/a&gt; contains all of the information I was looking for: timestamp with milliseconds, exact request &lt;span class="caps"&gt;URL&lt;/span&gt;, actual client &lt;span class="caps"&gt;IP&lt;/span&gt; address, user-agent, sizes of request and response, &lt;span class="caps"&gt;ELB&lt;/span&gt; and backend (Artifactory) &lt;span class="caps"&gt;HTTP&lt;/span&gt; status codes, and the time taken to process the request both by the backend and the &lt;span class="caps"&gt;ELB&lt;/span&gt;&amp;nbsp;itself.&lt;/p&gt;
&lt;p&gt;With the useful and relevant logs sitting in S3 and not being ingested to any analysis system, my first instinct was to use &lt;a href="https://aws.amazon.com/athena/"&gt;Amazon Athena&lt;/a&gt;, a tool to execute &lt;span class="caps"&gt;SQL&lt;/span&gt;-like queries directly against data in S3. The results were certainly better than anything else we had access to, but without the ability to graph elements of interest over time they did not prove to be helpful in determining what patterns had changed since the previous hours, days and weeks. After some thought, my next instinct was to turn to the Elasticsearch, Logstash, Kibana (&lt;a href="https://www.elastic.co/elk-stack"&gt;&lt;span class="caps"&gt;ELK&lt;/span&gt;&lt;/a&gt;) stack that I&amp;#8217;m familiar with from previous&amp;nbsp;jobs.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.elastic.co/products/elasticsearch"&gt;Elasticsearch&lt;/a&gt; is a clustered full-text search service built atop Apache Lucene, &lt;a href="https://www.elastic.co/products/logstash"&gt;Logstash&lt;/a&gt; is a log aggregator and shipper with flexible plugin-based inputs, outputs, pre-processing and filters, and &lt;a href="https://www.elastic.co/products/kibana"&gt;Kibana&lt;/a&gt; a dynamic web interface for Elasticsearch geared towards search, analysis and graphing of time-series and log data. They&amp;#8217;re all open source projects now developed and commercially supported by &lt;a href="https://www.elastic.co/"&gt;Elastic&lt;/a&gt;, and together they form what is likely the largest and strongest open-source alternative to Splunk. Amazon has a managed &lt;a href="https://aws.amazon.com/elasticsearch-service/"&gt;Elasticsearch Service&lt;/a&gt; that includes the Kibana web interface, so I knew that getting up and running would be quick and&amp;nbsp;easy.&lt;/p&gt;
&lt;p&gt;I was able to go from start to analyzing a months&amp;#8217; worth of &lt;span class="caps"&gt;ELB&lt;/span&gt; access logs in just over an hour, using the process described below (that time could&amp;#8217;ve been greatly reduced if I changed some default Logstash configuration and didn&amp;#8217;t read the logs from S3 one file at a time in serial). Fortunately one of the other engineers on my team was able to restore Artifactory service while I was going through this process, but I continued anyway in order to see what data we could obtain. &lt;em&gt;Note that none of this would have been needed if we were properly ingesting the &lt;span class="caps"&gt;ELB&lt;/span&gt; access logs into a centralized analysis tool before the&amp;nbsp;incident.&lt;/em&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Identify a suitable subnet in our &lt;span class="caps"&gt;VPC&lt;/span&gt; that has a &lt;span class="caps"&gt;NAT&lt;/span&gt; instance for outgoing Internet connectivity and a suitably high number of free &lt;span class="caps"&gt;IP&lt;/span&gt; addresses. Record the subnet&amp;nbsp;&lt;span class="caps"&gt;ID&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Amazon Elasticsearch and Kibana run over &lt;span class="caps"&gt;HTTPS&lt;/span&gt; on port 443, so identify or create appropriate security groups that allow you and your team to access port 443 (ideally, you access private resources in &lt;span class="caps"&gt;AWS&lt;/span&gt; over &lt;span class="caps"&gt;VPN&lt;/span&gt; or Direct&amp;nbsp;Connect).&lt;/li&gt;
&lt;li&gt;Create a new Elasticsearch Domain (cluster) in &lt;span class="caps"&gt;AWS&lt;/span&gt;. Since I wasn&amp;#8217;t terribly familiar with the service, I did this manually through the &lt;span class="caps"&gt;AWS&lt;/span&gt; Console. For short-lived ad-hoc analysis, leave most options at their default and create a single-instance cluster. Use General Purpose &lt;span class="caps"&gt;SSD&lt;/span&gt; storage; I provisioned storage at just under 10x the size of the months&amp;#8217; raw logs in S3. I set the access policy for Elasticsearch to the canned option to disable requiring &lt;span class="caps"&gt;IAM&lt;/span&gt; signatures on requests and control access only via the security group, so that any engineers with access to the account could use the Kibana &lt;span class="caps"&gt;UI&lt;/span&gt;. I then added the subnet I identified in the first step and the security groups identified in the previous&amp;nbsp;step.&lt;/li&gt;
&lt;li&gt;It took about ten minutes for the Elasticsearch domain to become operational; until then, the &lt;span class="caps"&gt;AWS&lt;/span&gt; Console showed the domain to be in &amp;#8220;Processing&amp;#8221; status. During this time I spun up an &lt;span class="caps"&gt;EC2&lt;/span&gt; instance to run Logstash on. I used the CentOS base &lt;span class="caps"&gt;AMI&lt;/span&gt;, but Amazon Linux would work just as well. I created the instance in the same subnet as the Elasticsearch cluster, added a security group allowing &lt;span class="caps"&gt;SSH&lt;/span&gt; access for our team, and attached an &lt;span class="caps"&gt;IAM&lt;/span&gt; Role allowing S3 access to retrieve the &lt;span class="caps"&gt;ELB&lt;/span&gt;&amp;nbsp;logs.&lt;/li&gt;
&lt;li&gt;Once the &lt;span class="caps"&gt;EC2&lt;/span&gt; instance was running and accessible over &lt;span class="caps"&gt;SSH&lt;/span&gt;, I installed Java 8 with &lt;code&gt;yum install java-1.8.0-openjdk-headless&lt;/code&gt; and then followed the &lt;a href="https://www.elastic.co/guide/en/logstash/current/installing-logstash.html#_yum"&gt;Logstash installation instructions for yum-based systems&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Create &lt;code&gt;/etc/logstash/conf.d/logstash.conf&lt;/code&gt; as shown&amp;nbsp;below.&lt;/li&gt;
&lt;li&gt;Wait for the Elasticsearch domain to status to reach &amp;#8220;Active&amp;#8221;. Once that happens, you should see two URLs for the Domain: a &amp;#8220;&lt;span class="caps"&gt;VPC&lt;/span&gt; endpoint&amp;#8221; where the Elasticsearch &lt;span class="caps"&gt;API&lt;/span&gt; can be reached (mine was &lt;code&gt;https://vpc-jantman-art-elb-test2-ryomklmh5hr7pcmhw7jvx7t374.us-east-1.es.amazonaws.com&lt;/code&gt;) and a &amp;#8220;Kibana&amp;#8221; &lt;span class="caps"&gt;URL&lt;/span&gt; where the Kibana web interface is available (mine was &lt;code&gt;https://vpc-jantman-art-elb-test2-ryomklmh5hr7pcmhw7jvx7t374.us-east-1.es.amazonaws.com/_plugin/kibana/&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;SSH&lt;/span&gt; to the &lt;span class="caps"&gt;EC2&lt;/span&gt; instance and confirm connectivity to Elasticsearch via &lt;code&gt;curl&lt;/code&gt;, substituting in the &lt;span class="caps"&gt;VPC&lt;/span&gt; endpoint &lt;span class="caps"&gt;URL&lt;/span&gt; for your cluster: &lt;code&gt;curl https://vpc-jantman-art-elb-test2-ryomklmh5hr7pcmhw7jvx7t374.us-east-1.es.amazonaws.com&lt;/code&gt;. If all is well, you should get back a &lt;span class="caps"&gt;JSON&lt;/span&gt; response that includes a name, cluster name, version, etc. If this doesn&amp;#8217;t work, troubleshoot connectivity issues until it&amp;nbsp;does.&lt;/li&gt;
&lt;li&gt;Start the Logstash service with &lt;code&gt;sudo systemctl start logstash&lt;/code&gt; and then watch the initial logs with &lt;code&gt;journalctl -u logstash&lt;/code&gt;. It should shortly tell you that it is writing logs to &lt;code&gt;/var/log/logstash&lt;/code&gt;. At this point you can &lt;code&gt;sudo tail -f /var/log/logstash/logstash-plain.log&lt;/code&gt; to follow logstash&amp;#8217;s&amp;nbsp;progress.&lt;/li&gt;
&lt;li&gt;If all went well, Logstash should tell you that it&amp;#8217;s connected to elasticsearch (&lt;code&gt;New Elasticsearch output&lt;/code&gt;) and S3 (&lt;code&gt;Registering s3 input&lt;/code&gt;), and then that it has successfully begun working (&lt;code&gt;Pipeline started succesfully&lt;/code&gt;). On new installations you should see a message similar to &lt;code&gt;[logstash.inputs.s3       ] Using default generated file for the sincedb&lt;/code&gt;, which indicates that the S3 input is starting to process&amp;nbsp;files.&lt;/li&gt;
&lt;li&gt;Browse to the Kibana &lt;span class="caps"&gt;URL&lt;/span&gt; found in the Elasticsearch console, above (step 7). Soon after Logstash begins ingesting data, Kibana should recognize the new indices and prompt you to create an Index Pattern for them (if the interface says that no indices could be found, wait a minute or two and then refresh). Since Logstash creates a separate index for each days&amp;#8217; data named according to a &lt;code&gt;logstash-YYYY.MM.DD&lt;/code&gt; pattern, enter &lt;code&gt;logstash-*&lt;/code&gt; for your index pattern and click &amp;#8220;Next&amp;#8221;. The &amp;#8220;Time Filter field name&amp;#8221; used by our Logstash configuration file is &lt;code&gt;timestamp&lt;/code&gt; (not &lt;code&gt;@timestamp&lt;/code&gt;), so select that from the dropdown and click &amp;#8220;Create Index Pattern&amp;#8221;. That&amp;#8217;s it, Kibana is&amp;nbsp;configured.&lt;/li&gt;
&lt;li&gt;Click the &amp;#8220;Discover&amp;#8221; link on the left sidebar in the Kibana interface, and then click the clock icon in the far top right of the screen and select an appropriate time range for the data you&amp;#8217;re ingesting (I selected the last 30 days, as I was ingesting &lt;span class="caps"&gt;ELB&lt;/span&gt; access logs for the current month). After a brief delay messages should be visible and the graph of messages will populate. Because of how &lt;span class="caps"&gt;ELB&lt;/span&gt; access logs are written to S3 and how Logstash ingests them, logs will be ingested in chronological order by day, so the graph by date is a good indication of ingest progress. &lt;img alt="screenshot of Kibana during initial data load" src="/GFX/kibana-initial-data.png"&gt;&lt;/li&gt;
&lt;li&gt;When all data up to the current time is ingested you can either terminate the &lt;span class="caps"&gt;EC2&lt;/span&gt; instance running Logstash, or keep it running. It will check S3 for new files every minute and keep ingesting them as long as Logstash is running. &lt;em&gt;(Note: the Logstash configuration described here is only for ad-hoc analysis, not a long-running&amp;nbsp;need.)&lt;/em&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I&amp;#8217;ll leave further discussion of how data can be analyzed and visualized to others; the &lt;a href="https://www.elastic.co/guide/en/kibana/current/index.html"&gt;Kibana User Guide&lt;/a&gt; does a great job of it and even links to some introductory videos, and there is much information on the Internet. I find the recent versions of Kibana to be one of the best interfaces I&amp;#8217;ve ever seen for exploring complex log data, and it&amp;#8217;s also designed with graphical visualizations as a first-class citizen with many helpful&amp;nbsp;links.&lt;/p&gt;
&lt;p&gt;For analysis of this particular issue I was focusing on &amp;#8220;backend_processing_time&amp;#8221;, the time it took Artifactory to handle requests. My first real dive into the data was graphing mean and 75th, 95th and 99th percentile processing times over all the&amp;nbsp;data:&lt;/p&gt;
&lt;p&gt;&lt;img alt="graph of mean and percentile processing times" src="/GFX/artifactory_backend_time.png"&gt;&lt;/p&gt;
&lt;p&gt;While there&amp;#8217;s a large difference between the average and even 95th percentile versus the 99th throughout the data, the extreme jump in the 99th percentile value (about 16x the previous value and significantly more than the 95th) confirmed our findings that this was likely not a system-wide failure but something affecting a certain population of&amp;nbsp;users.&lt;/p&gt;
&lt;p&gt;By this time we&amp;#8217;d resolved the issued by disabling a specific remote RubyGems repository (a largely forgotten and unsupported internal one used by a handful of teams) which was found to be offline. We presumed that having that remote repository included in virtual repositories, including our master &amp;#8220;all-gems&amp;#8221; repository, was causing them to fail as well. To evaluate this based on data, I graphed the processing times of requests to repositories that included the problematic one, and requests to repositories that did not. The result is the proverbial &amp;#8220;smoking gun&amp;#8221;. The top graph shows processing time of requests to repositories including the problematic one, which increase over twenty times the baseline and shows a shape very similar to our 99th percentile. The bottom graph shows processing time of requests to repositories that do not include the problematic one, and shows no discernible difference during the outage from the previous thirty&amp;nbsp;days.&lt;/p&gt;
&lt;p&gt;&lt;img alt="graph of processing times for the two groups of repositories" src="/GFX/gems-vs-not.png"&gt;&lt;/p&gt;
&lt;h2 id="logstash-configuration-file"&gt;&lt;a class="toclink" href="#logstash-configuration-file"&gt;Logstash Configuration&amp;nbsp;File&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Logstash&amp;#8217;s configuration format has three main parts: input, filter, and output. Below you&amp;#8217;ll find the configuration file that I used for this project and an explanation of it. I can&amp;#8217;t take credit for most of it, but rather pieced it together under pressure from a few sources&amp;nbsp;online.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;input {
    s3 {
        bucket =&amp;gt; &amp;quot;our-artifactory-log-bucket-name&amp;quot;
        region =&amp;gt; &amp;quot;us-east-1&amp;quot;
        prefix =&amp;gt; &amp;quot;elb_access_logs/AWSLogs/012345678901/elasticloadbalancing/us-east-1/2018/03&amp;quot;
        type   =&amp;gt; &amp;quot;elb&amp;quot;
    }
}

filter {
   if [type] == &amp;quot;elb&amp;quot; {
      grok {
         match =&amp;gt; [&amp;quot;message&amp;quot;, &amp;quot;%{TIMESTAMP_ISO8601:timestamp} %{&lt;span class="caps"&gt;NOTSPACE&lt;/span&gt;:loadbalancer} %{&lt;span class="caps"&gt;IP&lt;/span&gt;:client_ip}:%{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:client_port:int} (?:%{&lt;span class="caps"&gt;IP&lt;/span&gt;:backend_ip}:%{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:backend_port:int}|-) %{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:request_processing_time:float} %{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:backend_processing_time:float} %{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:response_processing_time:float} (?:%{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:elb_status_code:int}|-) (?:%{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:backend_status_code:int}|-) %{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:received_bytes:int} %{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:sent_bytes:int} \&amp;quot;(?:%{&lt;span class="caps"&gt;WORD&lt;/span&gt;:verb}|-) (?:%{&lt;span class="caps"&gt;GREEDYDATA&lt;/span&gt;:request}|-) (?:&lt;span class="caps"&gt;HTTP&lt;/span&gt;/%{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:httpversion}|-( )?)\&amp;quot; \&amp;quot;%{&lt;span class="caps"&gt;DATA&lt;/span&gt;:userAgent}\&amp;quot;( %{&lt;span class="caps"&gt;NOTSPACE&lt;/span&gt;:ssl_cipher} %{&lt;span class="caps"&gt;NOTSPACE&lt;/span&gt;:ssl_protocol})?&amp;quot;]
      }
      grok {
         match =&amp;gt; [&amp;quot;request&amp;quot;, &amp;quot;%{&lt;span class="caps"&gt;URIPROTO&lt;/span&gt;:http_protocol}&amp;quot;]
      }
      if [request] != &amp;quot;-&amp;quot; {
         grok {
            match =&amp;gt; [&amp;quot;request&amp;quot;, &amp;quot;(?&amp;lt;request&amp;gt;[^?]*)&amp;quot;]
            overwrite =&amp;gt; [&amp;quot;request&amp;quot;]
         }
      }
      useragent {
         source =&amp;gt; &amp;quot;userAgent&amp;quot;
      }
      date {
         match =&amp;gt; [&amp;quot;timestamp&amp;quot;, &amp;quot;&lt;span class="caps"&gt;ISO8601&lt;/span&gt;&amp;quot;]
      }
   }
}

output{
    elasticsearch {
        hosts =&amp;gt; &amp;quot;https://vpc-jantman-art-elb-test2-ryomklmh5hr7pcmhw7jvx7t374.us-east-1.es.amazonaws.com:443&amp;quot;
        ssl =&amp;gt; true
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;h3 id="input"&gt;&lt;a class="toclink" href="#input"&gt;Input&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The &lt;code&gt;input&lt;/code&gt; section defines our data source, in this case the &lt;a href="https://www.elastic.co/guide/en/logstash/current/plugins-inputs-s3.html"&gt;S3 input plugin&lt;/a&gt; which reads files from an S3 bucket (after reading the existing files, it polls every 60 seconds for new ones). The above example is configured to read from a bucket named &amp;#8220;our-artifactory-log-bucket-name&amp;#8221; in us-east-1, and read files under the &lt;code&gt;elb_access_logs/AWSLogs/423319072129/elasticloadbalancing/us-east-1/2018/03&lt;/code&gt; prefix. It also assigns a &amp;#8220;type&amp;#8221; of &lt;code&gt;elb&lt;/code&gt; to the logs, for reference later in the&amp;nbsp;configuration.&lt;/p&gt;
&lt;h3 id="filter"&gt;&lt;a class="toclink" href="#filter"&gt;Filter&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The heart and real power of Logstash is in its ability to transform data via both &lt;a href="https://www.elastic.co/guide/en/logstash/current/transformation.html"&gt;built-in capabilities&lt;/a&gt; and &lt;a href="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html"&gt;filter plugins&lt;/a&gt;. Unlike in some other tools such as Splunk, in the &lt;span class="caps"&gt;ELK&lt;/span&gt; stack &lt;em&gt;all&lt;/em&gt; data manipulation, parsing, extraction, and filtering is generally done where events originate in Logstash rather than on the server. Events are sent to Elasticsearch as &lt;span class="caps"&gt;JSON&lt;/span&gt; documents, and Elasticsearch stores, indexes, searches, and returns those &lt;span class="caps"&gt;JSON&lt;/span&gt; documents exactly as they were sent&amp;nbsp;in.&lt;/p&gt;
&lt;p&gt;The filter we use here has a number of statements wrapped inside &lt;code&gt;if [type] == "elb"&lt;/code&gt;, which ensures that they only operate on logs from our S3 input which we assigned the &amp;#8220;elb&amp;#8221; type to. Let&amp;#8217;s examine the statements within the &lt;code&gt;if&lt;/code&gt; one by&amp;nbsp;one:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;grok {
    match =&amp;gt; [&amp;quot;message&amp;quot;, &amp;quot;%{TIMESTAMP_ISO8601:timestamp} %{&lt;span class="caps"&gt;NOTSPACE&lt;/span&gt;:loadbalancer} %{&lt;span class="caps"&gt;IP&lt;/span&gt;:client_ip}:%{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:client_port:int} (?:%{&lt;span class="caps"&gt;IP&lt;/span&gt;:backend_ip}:%{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:backend_port:int}|-) %{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:request_processing_time:float} %{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:backend_processing_time:float} %{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:response_processing_time:float} (?:%{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:elb_status_code:int}|-) (?:%{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:backend_status_code:int}|-) %{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:received_bytes:int} %{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:sent_bytes:int} \&amp;quot;(?:%{&lt;span class="caps"&gt;WORD&lt;/span&gt;:verb}|-) (?:%{&lt;span class="caps"&gt;GREEDYDATA&lt;/span&gt;:request}|-) (?:&lt;span class="caps"&gt;HTTP&lt;/span&gt;/%{&lt;span class="caps"&gt;NUMBER&lt;/span&gt;:httpversion}|-( )?)\&amp;quot; \&amp;quot;%{&lt;span class="caps"&gt;DATA&lt;/span&gt;:userAgent}\&amp;quot;( %{&lt;span class="caps"&gt;NOTSPACE&lt;/span&gt;:ssl_cipher} %{&lt;span class="caps"&gt;NOTSPACE&lt;/span&gt;:ssl_protocol})?&amp;quot;]
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-grok.html"&gt;grok&lt;/a&gt; statements are one of Logstash&amp;#8217;s most commonly used filters. They parse arbitrary text and turn it into structured data via regular expression patterns. They also have many built-in shortcuts for &lt;a href="https://github.com/logstash-plugins/logstash-patterns-core/blob/master/patterns/grok-patterns"&gt;common patterns&lt;/a&gt; (i.e. &lt;code&gt;NUMBER&lt;/code&gt;, &lt;code&gt;IP&lt;/code&gt;, &lt;code&gt;WORD&lt;/code&gt;, &lt;code&gt;NOTSPACE&lt;/code&gt;, etc). The above statement applies regular expression matching to the incoming log line (&lt;code&gt;message&lt;/code&gt;) and assigns the results to named fields in the event (timestamp, loadbalancer, client_ip, client_port, etc.). This forms the basis for our structured &lt;span class="caps"&gt;ELB&lt;/span&gt; access log, and represents the bulk of the work that Logstash does when turning the lines of our text file access logs into &lt;span class="caps"&gt;JSON&lt;/span&gt; documents suitable for&amp;nbsp;Elasticsearch.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note: A Google search should turn up grok patterns for many common log types. If you need to develop your own, &lt;a href="https://grokdebug.herokuapp.com/"&gt;https://grokdebug.herokuapp.com/&lt;/a&gt; is an invaluable tool for&amp;nbsp;experimentation.&lt;/em&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;grok {
    match =&amp;gt; [&amp;quot;request&amp;quot;, &amp;quot;%{&lt;span class="caps"&gt;URIPROTO&lt;/span&gt;:http_protocol}&amp;quot;]
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;In the previous grok statement we set the &lt;code&gt;request&lt;/code&gt; field, the full &lt;span class="caps"&gt;URI&lt;/span&gt; requested from the &lt;span class="caps"&gt;ELB&lt;/span&gt;, using the &lt;code&gt;GREEDYDATA&lt;/code&gt; pattern (&lt;code&gt;.*&lt;/code&gt;) based on its position in the log. Here, we extract the leading protocol from the &lt;span class="caps"&gt;URI&lt;/span&gt; (&lt;code&gt;http&lt;/code&gt; or &lt;code&gt;https&lt;/code&gt; for Artifactory) and store it in a new &lt;code&gt;http_protocol&lt;/code&gt; field.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;if [request] != &amp;quot;-&amp;quot; {
    grok {
        match =&amp;gt; [&amp;quot;request&amp;quot;, &amp;quot;(?&amp;lt;request&amp;gt;[^?]*)&amp;quot;]
        overwrite =&amp;gt; [&amp;quot;request&amp;quot;]
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;I &lt;em&gt;believe&lt;/em&gt; this trims the query string from request, if&amp;nbsp;present.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;useragent {
    source =&amp;gt; &amp;quot;userAgent&amp;quot;
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This runs the &lt;code&gt;userAgent&lt;/code&gt; field of the event through Logstash&amp;#8217;s built-in &lt;a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-useragent.html"&gt;useragent filter plugin&lt;/a&gt;. This parses the user-agent string using BrowserScope data and, for recognized user agents, turns it into structured data including information like browser family, operating system, version, and&amp;nbsp;device.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;date {
    match =&amp;gt; [&amp;quot;timestamp&amp;quot;, &amp;quot;&lt;span class="caps"&gt;ISO8601&lt;/span&gt;&amp;quot;]
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This uses the &lt;a href="https://www.elastic.co/guide/en/logstash/current/plugins-filters-date.html"&gt;date filter&lt;/a&gt; to match an &lt;a href="https://en.wikipedia.org/wiki/ISO_8601"&gt;&lt;span class="caps"&gt;ISO8601&lt;/span&gt;-format date&lt;/a&gt; in the &lt;code&gt;timestamp&lt;/code&gt; field (extracted in our first &lt;code&gt;grok&lt;/code&gt; pattern) and use it to set the event&amp;nbsp;timestamp.&lt;/p&gt;
&lt;h3 id="output"&gt;&lt;a class="toclink" href="#output"&gt;Output&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;We have a single output configured in the example above, the built-in &lt;a href="https://www.elastic.co/guide/en/logstash/current/plugins-outputs-elasticsearch.html"&gt;elasticsearch&lt;/a&gt; output plugin, that sends logs to our Elasticsearch instance. The only configuration it needs (following the instructions above) is the &lt;span class="caps"&gt;URL&lt;/span&gt; to our &lt;span class="caps"&gt;AWS&lt;/span&gt; Elasticsearch domain, specified in the &lt;code&gt;hosts&lt;/code&gt; field, and indication that the cluster uses &lt;span class="caps"&gt;SSL&lt;/span&gt;. It is important to note that the &lt;code&gt;hosts&lt;/code&gt; &lt;span class="caps"&gt;URL&lt;/span&gt; &lt;em&gt;must&lt;/em&gt; include the &lt;code&gt;:443&lt;/code&gt; port specification explicitly, or else it will default to Elasticsearch&amp;#8217;s default port of 9200. Also note that if you configure Elasticsearch with &lt;span class="caps"&gt;IAM&lt;/span&gt; Authentication enabled, you must use the &lt;a href="https://github.com/awslabs/logstash-output-amazon_es"&gt;logstash-output-amazon_es&lt;/a&gt; plugin from Amazon&amp;nbsp;instead.&lt;/p&gt;</content><category term="aws"></category><category term="amazon"></category><category term="elb"></category><category term="elasticsearch"></category><category term="logstash"></category><category term="kibana"></category><category term="logs"></category><category term="analysis"></category></entry></feed>