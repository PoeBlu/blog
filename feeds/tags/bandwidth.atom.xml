<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jason Antman's Blog - bandwidth</title><link href="https://blog.jasonantman.com/" rel="alternate"></link><link href="https://blog.jasonantman.com/feeds/tags/bandwidth.atom.xml" rel="self"></link><id>https://blog.jasonantman.com/</id><updated>2017-04-17T16:11:00-04:00</updated><entry><title>Python script to check xfinity data usage</title><link href="https://blog.jasonantman.com/2017/04/python-script-to-check-xfinity-data-usage/" rel="alternate"></link><published>2017-04-17T16:11:00-04:00</published><updated>2017-04-17T16:11:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2017-04-17:/2017/04/python-script-to-check-xfinity-data-usage/</id><summary type="html">&lt;p&gt;Python/selenium script to check your Xfinity data&amp;nbsp;usage&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday I got one of those invasive, abusive, utterly awful (and idiotic) &lt;a href="https://www.techdirt.com/articles/20161123/10554936126/comcast-takes-heat-injecting-messages-into-internet-traffic.shtml"&gt;injected popups from Xfinity&lt;/a&gt; that I&amp;#8217;m at 75% of my monthly bandwidth allocation. Nevermind the fact that I have a bunch of automated scripts running on my computer and injected &lt;span class="caps"&gt;HTML&lt;/span&gt; might never be seen by a human, or that I work from home and every once in a while I&amp;#8217;ll find myself pulling and pushing multi-&lt;span class="caps"&gt;GB&lt;/span&gt; Docker images, which completely kills my &lt;span class="caps"&gt;1TB&lt;/span&gt; bandwidth limit. But it&amp;#8217;s only half way through the month and, frankly, I&amp;#8217;m pretty mystified how I could have used so much data this quickly. I went to Xfinity&amp;#8217;s site to check my usage meter - after rummaging around in my password manager to find my credentials - and realized that while it shows a graph of the past three months and a progress bar for the current month, it doesn&amp;#8217;t show me any detailed (i.e. daily or hourly) data that would help me figure out the&amp;nbsp;cause.&lt;/p&gt;
&lt;p&gt;So, I wrote a little &lt;a href="https://github.com/jantman/xfinity-usage"&gt;script&lt;/a&gt; using Python and Selenium to log in to their My Account site and screen-scrape the &lt;a href="http://www.xfinity.com/usagemeter"&gt;usage meter&lt;/a&gt;. Why Comcast would require me to log in to view my usage when I&amp;#8217;m accessing their site from the &lt;span class="caps"&gt;IP&lt;/span&gt; address &lt;em&gt;they&lt;/em&gt; gave me, on &lt;em&gt;their&lt;/em&gt; network, I have no idea&amp;#8230; unless it&amp;#8217;s to provide a disincentive for customers to be aware of their usage. But I wrote the script, and it seems to be working. For the time being, I&amp;#8217;m both pushing the results into Graphite so I can see usage over time, and sending myself a daily email so I can keep on top of&amp;nbsp;usage.&lt;/p&gt;
&lt;p&gt;Apparently Comcast used to have &lt;a href="http://usmapp-qa.comcast.net/"&gt;a desktop app&lt;/a&gt; to track usage but it&amp;#8217;s since been completely shut down, along with the &lt;span class="caps"&gt;API&lt;/span&gt; that backed it (which an enterprising fellow reverse-engineered in &lt;a href="https://github.com/WTFox/comcastUsage"&gt;this script&lt;/a&gt;). I can only assume this is another indication that, though the bandwidth cap was introduced citing &amp;#8220;network performance&amp;#8221;, they really don&amp;#8217;t want people lowering network load (and avoiding&amp;nbsp;fees).&lt;/p&gt;
&lt;p&gt;I don&amp;#8217;t remember anything about screen-scraping in the Xfinity terms of service - and if they&amp;#8217;re f-ing injecting elements into &lt;em&gt;my&lt;/em&gt; web traffic, I sure as hell hope they don&amp;#8217;t complain about me checking my own usage - but use this at your own risk. Also be aware that it&amp;#8217;s screen-scraping, so it may well break with a site redesign or element &lt;span class="caps"&gt;ID&lt;/span&gt;&amp;nbsp;changes.&lt;/p&gt;
&lt;p&gt;If anyone would find this useful, please see &lt;a href="https://github.com/jantman/xfinity-usage"&gt;https://github.com/jantman/xfinity-usage&lt;/a&gt;.&lt;/p&gt;</content><category term="comcast"></category><category term="xfinity"></category><category term="data"></category><category term="usage"></category><category term="bandwidth"></category><category term="cap"></category><category term="python"></category><category term="selenium"></category></entry><entry><title>Web Traffic for JasonAntman.com - Webalizer, Site Maps</title><link href="https://blog.jasonantman.com/2008/04/web-traffic-for-jasonantmancom-webalizer-site-maps/" rel="alternate"></link><published>2008-04-22T15:52:00-04:00</published><updated>2008-04-22T15:52:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2008-04-22:/2008/04/web-traffic-for-jasonantmancom-webalizer-site-maps/</id><summary type="html">&lt;p&gt;I&amp;#8217;ve been working on the design of a town council campaign site for a
friend - &lt;a href="http://www.mikennick08.com/"&gt;www.mikennick08.com&lt;/a&gt;. It&amp;#8217;s hosted
by an additional Apache vhost on my personal server (and running off of
port 10015 - ugh). I setup &lt;a href="http://www.mrunix.net/webalizer/"&gt;Webalizer&lt;/a&gt;
for him, so I figured I&amp;#8217;d give my …&lt;/p&gt;</summary><content type="html">&lt;p&gt;I&amp;#8217;ve been working on the design of a town council campaign site for a
friend - &lt;a href="http://www.mikennick08.com/"&gt;www.mikennick08.com&lt;/a&gt;. It&amp;#8217;s hosted
by an additional Apache vhost on my personal server (and running off of
port 10015 - ugh). I setup &lt;a href="http://www.mrunix.net/webalizer/"&gt;Webalizer&lt;/a&gt;
for him, so I figured I&amp;#8217;d give my own webalizer installation a check.
Wow - 30,215 hits this month alone. That reminded me of a problem with
my ignored hosts - 17,600 of those hits were Googlebot, and another
2,065 were Yandex (a Russian search&amp;nbsp;engine).&lt;/p&gt;
&lt;p&gt;Amazingly, though, it seems like Google is only indexing my blog. My
precious wiki seems out of whack, not to metion my &lt;span class="caps"&gt;CVS&lt;/span&gt;&amp;nbsp;repository.&lt;/p&gt;
&lt;p&gt;So, this reminded me of two long-overdue&amp;nbsp;tasks:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Get webalizer to properly ignore the common&amp;nbsp;bots.&lt;/li&gt;
&lt;li&gt;Get sitemaps of my entire&amp;nbsp;site.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, off to the&amp;nbsp;races!&lt;/p&gt;
&lt;p&gt;First, I added Googlebot, Yandex, and a few others to webalizer.conf
with IgnoreAgent directives. Then, after clearing out my entire output
directory - and waiting a &lt;span class="caps"&gt;LONG&lt;/span&gt; time for it to run - bingo! Real stats.
Down to about 8000 hits for the month, which seems more logical, even
including the \~2,000 hits from Google&amp;nbsp;feedfetcher.&lt;/p&gt;
&lt;p&gt;Next stop was sitemaps. It tooks some &lt;span class="caps"&gt;PHP&lt;/span&gt; magic to hack apart the
MediaWiki sitemaps, put in the correct URLs (it was showing an
internal-only hostname), and drop all that and my Blogger rss.xml in an
index file. It&amp;#8217;s now 2 &lt;span class="caps"&gt;AM&lt;/span&gt;, and it just crashed and burned - the &lt;span class="caps"&gt;PHP&lt;/span&gt;
script worked fine, but for some reason my entries in
sitemaps_index.xml - which pointed to sitemaps in a subdirectory - came
back with errors. Well, something to work on&amp;nbsp;tomorrow.&lt;/p&gt;
&lt;p&gt;This morning I checked my backups and noticed that nothing had run in 3
days. It turns out I just had one failed job holding everything up. And
I screwed up - I was home this weekend and forgot to swap tapes. It&amp;#8217;ll
be another 2 weeks before I can. But, I took the time to setup a backup
status box on my administrative portal (more on that later) and will
also be revising my apparently ineffectual Nagios check&amp;nbsp;script.&lt;/p&gt;
&lt;p&gt;On a few side notes: First, I&amp;#8217;m seriously thinking of dumping Verizon
FiOS. While I really like the service, their static &lt;span class="caps"&gt;IP&lt;/span&gt; (business)
variant is $100/month for 15 Mbps down / 2 Mbps up, whereas
Cablevision&amp;#8217;s Optimum Business with static &lt;span class="caps"&gt;IP&lt;/span&gt; is $55/month for 30 Mbps
down / 5 Mbps&amp;nbsp;up!&lt;/p&gt;
&lt;p&gt;Most of the previous projects have been put on hold for the time being
(mainly because of impending final exams at school) - the new Gigabit
Ethernet switch for backups, testing Zenoss and upgrading monitoring (to
a new product or Nagios 3),&amp;nbsp;etc.&lt;/p&gt;</content><category term="bacula"></category><category term="bandwidth"></category><category term="connectivity"></category><category term="ISP"></category><category term="Nagios"></category><category term="webalizer"></category></entry><entry><title>Blacklists, Network Performance, New Project, XKCD</title><link href="https://blog.jasonantman.com/2007/10/blacklists-network-performance-new-project-xkcd/" rel="alternate"></link><published>2007-10-10T14:03:00-04:00</published><updated>2007-10-10T14:03:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2007-10-10:/2007/10/blacklists-network-performance-new-project-xkcd/</id><summary type="html">&lt;p&gt;Part 2 of today&amp;#8217;s&amp;nbsp;thoughts&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Blacklists, Blocking, Reverse-Validation&lt;/strong&gt; - Yes, they have some uses.
I use Daemon Synchronization in
&lt;a href="http://www.jasonantman.com/wiki/index.php/DenyHosts"&gt;DenyHosts&lt;/a&gt; and
plug-ins like Pyzor in SpamAssassin. However, I&amp;#8217;ve also been the victim
of blacklists, and the new Internet order, many times. There&amp;#8217;s a
conspiracy between ISPs - simply put …&lt;/p&gt;</summary><content type="html">&lt;p&gt;Part 2 of today&amp;#8217;s&amp;nbsp;thoughts&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Blacklists, Blocking, Reverse-Validation&lt;/strong&gt; - Yes, they have some uses.
I use Daemon Synchronization in
&lt;a href="http://www.jasonantman.com/wiki/index.php/DenyHosts"&gt;DenyHosts&lt;/a&gt; and
plug-ins like Pyzor in SpamAssassin. However, I&amp;#8217;ve also been the victim
of blacklists, and the new Internet order, many times. There&amp;#8217;s a
conspiracy between ISPs - simply put, big ISPs want everyone else to use
big ISPs. I understand the logic behind reverse-validation. However, I
have a residential internet connection. I also run Linux. When I got
Verizon, I configured Postfix to deliver mail directly. Big mistake.
Most big email providers (&lt;span class="caps"&gt;AOL&lt;/span&gt;, &lt;span class="caps"&gt;MSN&lt;/span&gt;, probably Gmail too) will bounce back
e-mail that comes from a domain that doesn&amp;#8217;t reverse-validate. And since
Verizon owns my &lt;span class="caps"&gt;IP&lt;/span&gt;, despite the substantial sums of money they&amp;#8217;ve been
getting from me, my &lt;span class="caps"&gt;IP&lt;/span&gt; doesn&amp;#8217;t reverse-validate to my domain name. To
top it off, Verizon blocks the usual &lt;span class="caps"&gt;SMTP&lt;/span&gt; ports on residential
connections, so I can&amp;#8217;t have people send me e-mail either. Everything
needs to be relayed through Verizon. To add to the frustration, Verizon
blocks port 80 on my connection, so I&amp;#8217;m forced to serve my whole site on
an unused (and un-blocked) high-numbered port. And use
&lt;a href="http://www.dyndns.org"&gt;DynDNS.org&lt;/a&gt; to redirect to my dynamic &lt;span class="caps"&gt;IP&lt;/span&gt;. This
wouldn&amp;#8217;t be so much of an issue if I didn&amp;#8217;t know that some large
companies have firewalls configured to block &lt;span class="caps"&gt;HTTP&lt;/span&gt; requests *&lt;span class="caps"&gt;OUT&lt;/span&gt;* to
any non-default port. As a result, my own father can&amp;#8217;t view my web site
or blog from work. What ever happened to the little&amp;nbsp;guy?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Network Performance&lt;/strong&gt; - I know I have old computers and an old switch.
But there&amp;#8217;s something wrong when network file transfers crawl by at \~3
Mbps. I setup &lt;a href="http://sd.wareonearth.com/~phil/net/ttcp/"&gt;nttcp&lt;/a&gt; on two
of my machines to measure throughput, and was greeted with numbers in
the realm of 93-96 Mbps - what I&amp;#8217;d expect on a 100 Mpbs network.
However, a file transfer between these two machines barely scratched 8
Mpbs. Maybe GigE is the answer, but I&amp;#8217;ll be looking into the theory
behind this in the next few days - admittedly, I don&amp;#8217;t know much about
network performance, but I&amp;#8217;m willing to&amp;nbsp;learn&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;New Project&lt;/strong&gt; - I&amp;#8217;ve started planning on a new project,
&lt;a href="http://www.openepcr.org"&gt;openEPCR&lt;/a&gt;. My &lt;a href="http://www.php-ems-tools.com"&gt;&lt;span class="caps"&gt;PHP&lt;/span&gt; &lt;span class="caps"&gt;EMS&lt;/span&gt;
Tools&lt;/a&gt; package for &lt;span class="caps"&gt;EMS&lt;/span&gt; and fire agencies
seems to be generating a lot of downloads (yet little community
interest), and I&amp;#8217;m now seriously thinking about the lack of a free,
open-source Electronic Patient Care Report package for the pre-hospital
care industry. A lot of these organizations are volunteer and operating
on limited budgets. Stay tuned&amp;#8230; all I&amp;#8217;ll say is that what I&amp;#8217;ve planned
is something that you&amp;#8217;d expect from me - open-source,
platform-independent, and geared towards limited hardware resources.
I&amp;#8217;ll probably be looking towards Java as a development platform, though
the interest generated in &lt;a href="http://gears.google.com/"&gt;Google Gears&lt;/a&gt; may
also pay off. Of course, there&amp;#8217;s no way I can do such an ambitious
project myself, so I&amp;#8217;m looking for developers to help&amp;nbsp;out.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Comic&lt;/strong&gt; - pretty much the only non-serious content in my Google Reader
account is &lt;a href="http://xkcd.com"&gt;&lt;span class="caps"&gt;XKCD&lt;/span&gt;.com&lt;/a&gt;. It&amp;#8217;s a great comic with
wonderful technical and geek humor. Today&amp;#8217;s
&lt;a href="http://xkcd.com/327/"&gt;comic&lt;/a&gt; was so good that I just had to include&amp;nbsp;it&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://xkcd.com/327/"&gt;&lt;img alt="image" src="http://imgs.xkcd.com/comics/exploits_of_a_mom.png"&gt;&lt;/a&gt;&lt;/p&gt;</content><category term="bandwidth"></category><category term="blacklist"></category><category term="comic"></category><category term="epcr"></category><category term="firefox"></category><category term="java"></category><category term="openepcr"></category><category term="patient care report"></category><category term="pcr"></category><category term="php ems tools"></category><category term="reverse-validation"></category><category term="XKCD"></category></entry></feed>