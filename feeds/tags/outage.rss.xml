<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jason Antman's Blog</title><link>http://blog.jasonantman.com/</link><description></description><atom:link href="http://blog.jasonantman.com/feeds/tags/outage.rss.xml" rel="self"></atom:link><lastBuildDate>Tue, 20 Mar 2012 18:12:00 -0400</lastBuildDate><item><title>Leap Year Windows Azure Cloud Outage</title><link>http://blog.jasonantman.com/2012/03/leap-year-windows-azure-cloud-outage/</link><description>&lt;p&gt;I haven&amp;#8217;t talked about Microsoft in quite a while (mainly because I
don&amp;#8217;t follow mainstream tech news as much anymore), but I happened by a
very interesting &lt;a href="http://blogs.msdn.com/b/windowsazure/archive/2012/03/09/summary-of-windows-azure-service-disruption-on-feb-29th-2012.aspx"&gt;post on the Windows Azure
blog&lt;/a&gt;
the other day. It&amp;#8217;s a very detailed postmortem of the major outage of
the Windows Azure cloud service which occurred from 4:00 &lt;span class="caps"&gt;PM&lt;/span&gt; &lt;span class="caps"&gt;PST&lt;/span&gt; on
February 28&lt;sup&gt;th&lt;/sup&gt; through 2:15 &lt;span class="caps"&gt;AM&lt;/span&gt; on March 1&lt;sup&gt;st&lt;/sup&gt;. Before I get into any of
the details, I should say that it really is a nice, well-done post. And
the fact that they&amp;#8217;re willing to do such a detailed, public postmortem -
and admit the failures that they did - is a step in the right direction
for Microsoft (a company that I don&amp;#8217;t particularly care for, to put it&amp;nbsp;lightly).&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m going to glance over the majority of the post, though I highly
recommend that anyone interested in running web-scale services,
specifically highly available ones, read it. The general overview
(really just the points that are germane to my discussion) is as
follows: An agent running inside the guest &lt;span class="caps"&gt;VM&lt;/span&gt; instances (i.e. domU)
communicates with a counterpart on the host &lt;span class="caps"&gt;OS&lt;/span&gt; (i.e. dom0) over an
encrypted channel, authenticated by certificate. The certs are generated
and passed from the guest to the host when the guest instance is first
initialized, which means when an app is first deployed, scaled out, &lt;span class="caps"&gt;OS&lt;/span&gt;
updated, or when an app is reinitialized on a new host. This cert was
generated for a 1-year validity period, by adding 1 to the integer year
- hence, the generation process failed on February 29th of a leap year,
as the cert end date wasn&amp;#8217;t valid. When the cert generation failed, the
guest agent essentially stopped cold. The host agent waited for a 25
minute timeout, then re-initialized the guest and started over. After
three of these failures, the host assumes there&amp;#8217;s a hardware error
(since the guest would have reported a more specific error otherwise),
declares itself in an error state, and tries to move its current
workload over to another host. Which re-initializes the guests on that
host, thereby causing a chain-reaction of failures in this case. Skip
forward the 2-1/2 hours it took them to identify the problem, and
further 2-1/2 hours to get a fix ready. They fast-tracked their fix to 7
clusters that had already been in the process of a software update, but
ended up with those clusters in an inconsistent state with
incompatibilities between the guest and host networking subsystems,
bringing down previously-unaffected instances on these&amp;nbsp;clusters.&lt;/p&gt;
&lt;p&gt;This whole scenario offers a few important points on both the
development and operations&amp;nbsp;sides:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Inputs need error checking, and errors need to be raised.&lt;/strong&gt; So the
first problem here was the failed cert generation. I&amp;#8217;ll leave alone the
fact that, in my opinion, doing math on a the integer year of a date is
a high school or college programming mistake, and never should have been
made by someone doing platform coding for a major company (believe it or
not, 25% of years are leap years &amp;lt;/sarcasm&gt;). If whatever code was
generating the cert was smart enough to check the cert end date validity
and error out, that error should have been pushed up the stack to
somewhere where it could be handled - or, at least, sent to a central
log server that does error&amp;nbsp;trending.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Secure communications when provisioning need an insecure error path.&lt;/strong&gt;
This is somewhat connected to the previous point. If the normal process
of creating a new instance and communicating errors up the stack relies
on certs and authentication or encryption, there should be some method
of communicating errors with &lt;em&gt;that&lt;/em&gt; process either up the stack, or to a
separate event correlation/trending system. Errors with a
certificate-based system are not unusual, and even something as simple
as a vastly incorrect time set on the guests could have caused this same
problem. In environments where management/control communication between
levels of a system are encrypted or authenticated, there should be some
way for lower levels of the system to deliver a meaningful error message
&amp;#8220;somewhere&amp;#8221;. Even if this is just a syslog server or web service that
listens for errors and can escalate a warning when the numbers spike,
it&amp;#8217;s a useful alarm and debugging&amp;nbsp;tool.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Autonomous systems shouldn&amp;#8217;t lightly assume hardware failures.&lt;/strong&gt; It&amp;#8217;s
arrogance for a host system to assume that just because it can&amp;#8217;t
instantiate new guests, a hardware failure exists. This entire incident
is a perfect example that, at least if hardware error indicators are
properly monitored, it&amp;#8217;s more likely for a software problem to be
falsely identified as a hardware problem than the other way around. All
of my points are somewhat related, but I can think of many more reasons
why a new guest can&amp;#8217;t be instantiated that are software-related rather
than&amp;nbsp;hardware-related.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Autonomous control mechanisms need historical trending, and need to
call for help if this looks wrong.&lt;/strong&gt; These host systems tried to
instantiate new guests three times, waiting 25 minutes in between, and
then declared themselves bad and tried to migrate guests to other hosts.
From what I understand, Microsoft got it right in having a &amp;#8220;kill switch&amp;#8221;
that prevented further migration of guests. What they didn&amp;#8217;t have right
was reporting of autonomous actions (guest migration) to a central
location that performs trending. The 25 minute timeout with three
attempts is a great safety feature, but if the status of guest creation
actions was reported to a central server, it would have been much more
quickly apparent that 100% of guest creations in the past, say, 10
minutes, had failed - across all clusters. I know plenty of shops that
do little, if any, real-time analysis and historical comparisons of
their log data. But when systems are designed to perform self-healing
and autonomous actions, it&amp;#8217;s imperative that these actions are tracked
in near-real-time, compared to historical averages, and that deviation
from a baseline is identified and escalated to&amp;nbsp;humans.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Release procedures are more, not less, important when the sky is
falling.&lt;/strong&gt; The extended downtime of the last seven clusters was because
of an improperly &lt;span class="caps"&gt;QA&lt;/span&gt;&amp;#8217;ed update that was pushed out bypassing the normal
release and testing procedures. As a matter of fact, it was so poorly
&lt;span class="caps"&gt;QA&lt;/span&gt;&amp;#8217;ed that the update totally broke networking for the guest VMs, and
was still pushed out. I&amp;#8217;m sure this was more of a management/executive
decision than one made by the actual engineers, but organizations (even
management) need to understand that when the sky is falling, services
are down, and everybody is stressed, it&amp;#8217;s &lt;em&gt;more&lt;/em&gt; likely for mistakes and
oversights to happen, and this is when a proper, well-documented &lt;span class="caps"&gt;QA&lt;/span&gt; and
release procedure (including phased rollout) is &lt;em&gt;most&lt;/em&gt; important.
Failure to follow these procedures results in exactly what happened in
this case - making an already bad problem much&amp;nbsp;worse.&lt;/p&gt;
&lt;p&gt;Even &lt;em&gt;I&lt;/em&gt; can&amp;#8217;t blame Microsoft specifically for all this (though the
whole thing would have been avoided if they just represented timestamps
as integers like the rest of us&amp;#8230;), but it is a good opportunity for us
all to learn from a major incident at a &amp;#8220;pretty well known&amp;#8221;&amp;nbsp;company.&lt;/p&gt;
&lt;p&gt;release procedures are most important when things are already going&amp;nbsp;wrong&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Tue, 20 Mar 2012 18:12:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-03-20:2012/03/leap-year-windows-azure-cloud-outage/</guid><category>azure</category><category>microsoft</category><category>outage</category><category>release</category><category>testing</category><category>windows</category></item><item><title>Blackberry Oops</title><link>http://blog.jasonantman.com/2011/10/blackberry-oops/</link><description>&lt;p&gt;If you&amp;#8217;ve been following the tech news lately, you&amp;#8217;ve probably heard at
least a bit about the
&lt;a href="http://www.cnn.com/2011/10/12/tech/mobile/blackberry-outage/"&gt;massive&lt;/a&gt;
&lt;a href="http://abcnews.go.com/blogs/technology/2011/10/blackberry-outage-spreads-to-u-s/"&gt;blackberry&lt;/a&gt;
&lt;a href="http://www.nytimes.com/2011/10/14/technology/rim-struggles-to-overcome-blackberry-outages.html?_r=1"&gt;outage&lt;/a&gt;
over the past three days. While yes, it&amp;#8217;s the first truly grand failure
of &lt;span class="caps"&gt;RIM&lt;/span&gt;&amp;#8217;s infrastructure in their 12-year history, it&amp;#8217;s also a wonderful
case&amp;nbsp;study.&lt;/p&gt;
&lt;p&gt;Apparently the outage started Monday morning with &lt;span class="caps"&gt;RIM&lt;/span&gt; infrastructure in
Europe, the Middle East and Asia. However, by Wednesday, it had become a
global outage/slowdown of BlackBerry infrastructure (specifically the
parts that go through &lt;span class="caps"&gt;RIM&lt;/span&gt; - email, web browsing, and &lt;span class="caps"&gt;BBM&lt;/span&gt;; voice calls
and &lt;span class="caps"&gt;SMS&lt;/span&gt;/&lt;span class="caps"&gt;MMS&lt;/span&gt; were unaffected). With BlackBerry&amp;#8217;s market share falling,
Android&amp;#8217;s rapidly growing (and Android slowly becoming a viable
enterprise option), and the launch of the iPhone 4S just around the
corner, the timing of this couldn&amp;#8217;t be worse for&amp;nbsp;&lt;span class="caps"&gt;RIM&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;RIM&lt;/span&gt;&amp;#8217;s original
&lt;a href="http://www.rim.com/newsroom/service-update.shtml"&gt;statement&lt;/a&gt; about the
problem, at 21:30 on Tuesday October 11th,&amp;nbsp;was,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The messaging and browsing delays that some of you are still
experiencing were caused by a core switch failure within &lt;span class="caps"&gt;RIM&lt;/span&gt;’s
infrastructure. Although the system is designed to failover to a
back-up switch, the failover did not function as previously tested. As
a result, a large backlog of data was generated and we are now working
to clear that backlog and restore normal service as quickly as
possible. We sincerely apologise for the inconvenience caused to many
of you and we will continue to keep you&amp;nbsp;informed.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I haven&amp;#8217;t been able to find much more technical information than that -
a &lt;a href="http://www.computerworld.com/s/article/9220736/RIM_global_outage_caused_by_core_switch_failure_fix_under_way"&gt;&lt;span class="caps"&gt;CNET&lt;/span&gt; article from
Tuesday&lt;/a&gt;
goes into as much depth as anything I could find. I did find one mention
(misplaced the link) that the core switch in question uses technology
from &amp;#8220;multiple vendors&amp;#8221;. So what follows is part common sense (for me&amp;#8230;
why not for a multi-national corporation?) and part speculation. If
you&amp;#8217;re unfamiliar with &lt;span class="caps"&gt;RIM&lt;/span&gt;&amp;#8217;s architecture, the pertinent points are that
all Internet-bound traffic (browsing, email, and &lt;span class="caps"&gt;BBM&lt;/span&gt;) is piped through
&lt;span class="caps"&gt;RIM&lt;/span&gt;&amp;#8217;s data centers, where it&amp;#8217;s encypted and who-knows-what-else&amp;#8217;ed
(perhaps
&lt;a href="http://online.wsj.com/article/SB10001424052970204612504576608561811929654.html"&gt;monitored&lt;/a&gt;)
before going back out onto the &amp;#8216;net. In the Enterprise market, their big
claim is encryption/security, and monitoring/management/policy
enforcement on&amp;nbsp;handsets.&lt;/p&gt;
&lt;p&gt;First main point: &lt;span class="caps"&gt;RIM&lt;/span&gt; is a &lt;em&gt;big&lt;/em&gt; company. The thought that they rely on
an (apparently custom) core switch - a &lt;em&gt;single&lt;/em&gt; core switch for multiple
&lt;em&gt;continents&lt;/em&gt; - is amazing. It&amp;#8217;s even more amazing that they&amp;#8217;d let such a
large part of their infrastructure ride on an architecture with,
apparently, an untested failover mechanism. Of course I don&amp;#8217;t know all
the details, but I&amp;#8217;d hope that for a single piece of hardware which is
so critical, they&amp;#8217;d a) have a cold spare physically nearby so a
replacement wouldn&amp;#8217;t take a day or two, and b) if they can&amp;#8217;t do an
online failover test, at least have a full lab environment to test the
failover&amp;nbsp;in.&lt;/p&gt;
&lt;p&gt;Second main point: Their big claim through all of this is that they
didn&amp;#8217;t lose any data - email, &lt;span class="caps"&gt;BBM&lt;/span&gt;, etc. - it just got delayed. So if
there was a core switch failure in their data center serving &lt;span class="caps"&gt;EMEA&lt;/span&gt;, and
the next day global services slowed to a crawl, the only thing that
comes to mind to me is a waterfall; &lt;span class="caps"&gt;EMEA&lt;/span&gt; went down, and they started
rerouting traffic to their North America data center. The increased load
- probably a disaster recovery plan they never truly tested or even
planned - brought everything to a screeching halt, and caused them to
resort to simply caching messaging and pushing it out bit by bit as the
infrastructure could handle.&amp;nbsp;Oops.&lt;/p&gt;
&lt;p&gt;So what are my (admittedly poorly-informed) thoughts on&amp;nbsp;this?&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Scaling out works. Scaling up - especially with single points of
    failure, or N+1 redundancy - is dangerous. If &lt;span class="caps"&gt;RIM&lt;/span&gt; had scaled out and
    used regional data centers, with a close-to-commodity core and as
    much redundancy as possible, this wouldn&amp;#8217;t have happened. Sure,
    infrastructure costs money. But if that one &amp;#8220;core switch&amp;#8221; had been
    1,000 devices spread across multiple racks in multiple data centers,
    this never would have happened. And the devices would be
    comparatively cheap enough to probably keep spares on hand too. And
    regularly test their failover procedures. To all of the big
    businesses (apparently like &lt;span class="caps"&gt;RIM&lt;/span&gt;) who still think that big iron is
    the only way to do things right&amp;#8230; maybe it&amp;#8217;s time to take a hard
    look at that, and compare your architecture to that of the modern,
    new, hip giants like Google and Facebook. Grids and clusters. Nodes
    that can fail without anyone blinking. Scaling out might not fix
    every problem, but having half the world on a single core switch
    with N+1 redundancy probably isn&amp;#8217;t smart&amp;nbsp;either.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Disasters need to be planned for. Every possible contingency needs
    to be planned for. Plans need to be tested, regularly. If your giant
    core switch goes down and the failover doesn&amp;#8217;t &amp;#8220;function as
    previously tested&amp;#8221;, there&amp;#8217;s a serious problem both in your disaster
    planning, and in your validation and test procedure. If you have
    half of your customer base riding on a failover plan that isn&amp;#8217;t
    &lt;em&gt;regularly&lt;/em&gt; tested or otherwise validated, that&amp;#8217;s bad. While I can
    argue that the whole architecture - given this massive point of
    failure - could stand to be re-thought, the real issue here is with
    test/validation methodologies and procedures. For a company with 70
    million users, having a piece of infrastructure this critical fail,
    and the failover &amp;#8216;not work as tested&amp;#8217; is a very serious&amp;nbsp;issue.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Damage control is important. As I said, I can only imagine that the
    severe service degradation outside of &lt;span class="caps"&gt;EMEA&lt;/span&gt; was due to rerouting data
    from &lt;span class="caps"&gt;EMEA&lt;/span&gt; to the remaining functional data center(s). Such a
    solution should only be considered if it has been tested, or at
    least has engineering validation. If it was part of the disaster
    recovery plan, it obviously isn&amp;#8217;t actually a suitable solution, and
    served only to increase the scope of the outage. If it wasn&amp;#8217;t part
    of the disaster recovery plan, and was decided on-the-fly, someone
    really didn&amp;#8217;t do their research and engineering before putting the
    fix in place. A workable solution should have been planned ahead of
    time. And if one wasn&amp;#8217;t, it&amp;#8217;s very bad practice - this outage shows
    the results - to put in place a fix that hasn&amp;#8217;t been thought&amp;nbsp;out.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For a company whose business is so telecom-focused, this seems like a
glaringly bad design that shouldn&amp;#8217;t be acceptable in a&amp;nbsp;telecom.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Thu, 13 Oct 2011 22:20:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2011-10-13:2011/10/blackberry-oops/</guid><category>blackberry</category><category>outage</category></item></channel></rss>