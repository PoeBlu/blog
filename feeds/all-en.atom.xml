<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Jason Antman's Blog</title><link href="http://blog.jasonantman.com/" rel="alternate"></link><link href="http://blog.jasonantman.com/feeds/all-en.atom.xml" rel="self"></link><id>http://blog.jasonantman.com/</id><updated>2017-10-08T19:14:00-04:00</updated><entry><title>Pre-Authorized AWS Console URLs forÂ Notifications</title><link href="http://blog.jasonantman.com/2017/10/pre-authorized-aws-console-urls-for-notifications/" rel="alternate"></link><published>2017-10-08T19:14:00-04:00</published><updated>2017-10-08T19:14:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2017-10-08:/2017/10/pre-authorized-aws-console-urls-for-notifications/</id><summary type="html">&lt;p&gt;Pre-authorized &lt;span class="caps"&gt;AWS&lt;/span&gt; Console login URLs with limited permissions allow immediate investigation from&amp;nbsp;notifications.&lt;/p&gt;</summary><content type="html">&lt;h2 id="background"&gt;Background&lt;/h2&gt;
&lt;p&gt;Almost all of my work for the past two and a half years has revolved around Amazon Web Services,
but my personal &lt;span class="caps"&gt;AWS&lt;/span&gt; account (mostly a single tiny t2.micro instance that handles a tiny amount
of &lt;span class="caps"&gt;HTTP&lt;/span&gt; traffic and some cron jobs) has languished. Recently I&amp;#8217;ve undertaken a project to modernize
it, moving from an &lt;span class="caps"&gt;EC2&lt;/span&gt; instance using an &lt;span class="caps"&gt;AMI&lt;/span&gt; baked by &lt;a href="https://www.packer.io/"&gt;Packer&lt;/a&gt; and some
custom Ruby code to manage it, to a barebones instance acting as an
&lt;a href="https://aws.amazon.com/ecs/"&gt;Elastic Container Service (&lt;span class="caps"&gt;ECS&lt;/span&gt;)&lt;/a&gt; Docker host, and all of my applications
running in containers. This makes for much easier testing and deployment, and is a lot lower effort than
baking and testing a new &lt;span class="caps"&gt;AMI&lt;/span&gt; every time I want to change an nginx config file (yes, everything is&amp;nbsp;immutable).&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve got most of the basic work done and every resource in the account imported into
&lt;a href="https://www.terraform.io/"&gt;terraform&lt;/a&gt;, containers created and tested to replace what my old &lt;span class="caps"&gt;EC2&lt;/span&gt;
instance is doing, and terraform management of the &lt;span class="caps"&gt;ECS&lt;/span&gt; tasks and services too. So, I decided that
I&amp;#8217;d better setup some monitoring of all this before I forget about it. I try my best to keep my account
in the free tier for &lt;span class="caps"&gt;AWS&lt;/span&gt;; my bills have usually been about $15 &lt;span class="caps"&gt;USD&lt;/span&gt;/month in the past (mostly the t2.micro
instance and Route53) and I&amp;#8217;m expecting to go up to about $20/month with the new&amp;nbsp;infrastructure.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve gotten some basic monitoring in place - 7 CloudWatch Alarms for the important things, and a
Lambda function running every 30 minutes that does some more complicated and non-metric checks (and
sends to the same &lt;span class="caps"&gt;SNS&lt;/span&gt; topic as the alarms if it finds a problem). However, I realized how spoiled
I&amp;#8217;ve been at my day job, where a lot of our &lt;span class="caps"&gt;AWS&lt;/span&gt; monitoring infrastructure relies on
&lt;a href="https://www.datadoghq.com/"&gt;Datadog&lt;/a&gt; and &lt;a href="https://www.pagerduty.com/"&gt;PagerDuty&lt;/a&gt; (both
of which I love not only for their functionality but also for their APIs). While the new
&lt;a href="https://aws.amazon.com/blogs/aws/cloudwatch-dashboards-create-use-customized-metrics-views/"&gt;CloudWatch Dashboards&lt;/a&gt;
feature is pretty cool for a tiny infrastructure with no other monitoring tools
(and they can finally be managed via &lt;span class="caps"&gt;API&lt;/span&gt;), CloudWatch still had two big pain points for me
(aside from &lt;a href="https://aws.amazon.com/cloudwatch/pricing/"&gt;cost past the free tier&lt;/a&gt;):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;There&amp;#8217;s no option for re-notification from Alarms; if you set an &lt;span class="caps"&gt;SNS&lt;/span&gt; Topic target for a
  CloudWatch Metric Alarm, the notification is sent &lt;em&gt;once&lt;/em&gt; when the Alarm changes state.
  And that&amp;#8217;s&amp;nbsp;it.&lt;/li&gt;
&lt;li&gt;The notification messages are horribly&amp;nbsp;plain.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To solve the first problem, I just have my custom monitoring Lambda function also check
for any CloudWatch Alarms in a non-&lt;span class="caps"&gt;OK&lt;/span&gt; state for longer than 30 minutes (how often the
function runs) and re-notify for them. The second solution is a bit more&amp;nbsp;involved&amp;#8230;&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The&amp;nbsp;Problem&lt;/h2&gt;
&lt;p&gt;The Lambda function that I put in place to do some monitoring sends alerts to a &lt;span class="caps"&gt;SNS&lt;/span&gt; Topic
that delivers them to my phone via &lt;span class="caps"&gt;SMS&lt;/span&gt; and to my personal email account. While I&amp;#8217;ve made use
of the ability to &lt;a href="http://docs.aws.amazon.com/sns/latest/dg/PublishTopic.html#sns-message-formatting-by-protocol"&gt;send different messages per protocol&lt;/a&gt;
to send a short notification to &lt;span class="caps"&gt;SMS&lt;/span&gt; and a longer email, I still really miss the rich context
of notifications from real monitoring systems that include graph images and other useful
information. This becomes an even bigger inconvenience since I&amp;#8217;m rarely logged in to the
&lt;span class="caps"&gt;AWS&lt;/span&gt; Console for my personal account, and doing so involves a dance with several long passwords
and &lt;span class="caps"&gt;MFA&lt;/span&gt;&amp;nbsp;tokens.&lt;/p&gt;
&lt;p&gt;So, I wanted a way to be able to receive an &lt;span class="caps"&gt;SNS&lt;/span&gt; monitoring notification and actually &lt;em&gt;see&lt;/em&gt;
the metric graphs or events that generated it, rather than getting a plaintext (yeah, the
Simple &lt;em&gt;Notification&lt;/em&gt; Service is clearly designed for &lt;span class="caps"&gt;SMS&lt;/span&gt; and mobile push, and can&amp;#8217;t even
send &lt;span class="caps"&gt;HTML&lt;/span&gt; email) description of the triggered alarm. My first thought had been a Lambda
function triggered by the &lt;span class="caps"&gt;SNS&lt;/span&gt; topic, that would identify the alarm(s) in question, render
a graph of them, and then send that in a &lt;span class="caps"&gt;HTML&lt;/span&gt; email via &lt;span class="caps"&gt;SES&lt;/span&gt;. But that seemed like much more
work than I was interested in; all I &lt;em&gt;really&lt;/em&gt; needed to make this workable was a way to quickly
view alarms, metrics and events in&amp;nbsp;CloudWatch.&lt;/p&gt;
&lt;h2 id="solution"&gt;Solution&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Disclaimer: This is a bit of a kludge. It was designed for a tiny personal account with one human user, no monitoring other than CloudWatch, and for minimal&amp;nbsp;cost.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For integration with non-&lt;span class="caps"&gt;SAML&lt;/span&gt; identity providers (&amp;#8220;custom federation brokers&amp;#8221;), &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;IAM&lt;/span&gt;
provides a way to
&lt;a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html"&gt;create a &lt;span class="caps"&gt;URL&lt;/span&gt; that enables federated users to access the &lt;span class="caps"&gt;AWS&lt;/span&gt; Console&lt;/a&gt;.
In short, an &lt;span class="caps"&gt;IAM&lt;/span&gt; user with the required permissions can call &lt;a href="http://docs.aws.amazon.com/STS/latest/APIReference/API_AssumeRole.html"&gt;AssumeRole&lt;/a&gt;
to generate temporary credentials for a specified &lt;span class="caps"&gt;IAM&lt;/span&gt; Role, and then pass those credentials in
a &lt;span class="caps"&gt;HTTP&lt;/span&gt; request to &lt;code&gt;https://signin.aws.amazon.com/federation&lt;/code&gt; and get back a temporary &lt;code&gt;SigninToken&lt;/code&gt; granting
access to the &lt;span class="caps"&gt;AWS&lt;/span&gt; Console with the assumed role. This token can be used to construct a single &lt;span class="caps"&gt;URL&lt;/span&gt; that signs in
to the Console under the assumed role and brings the user to a specified destination &lt;span class="caps"&gt;URL&lt;/span&gt; in the &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;nbsp;Console.&lt;/p&gt;
&lt;p&gt;The one catch to this process (documented on the link above) is the user that makes the &lt;code&gt;AssumeRole&lt;/code&gt; &lt;span class="caps"&gt;API&lt;/span&gt; call must have
long-term credentials (i.e. a real &lt;span class="caps"&gt;IAM&lt;/span&gt; User). The call to the &lt;code&gt;/federation&lt;/code&gt; endpoint will fail if &lt;code&gt;AssumeRole&lt;/code&gt; was called
by another assumed role&amp;#8217;s temporary credentials, such as a Lambda function or Instance Profile. That tripped me up at first,
but I ended up figuring out a workable&amp;nbsp;solution.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a new &lt;span class="caps"&gt;IAM&lt;/span&gt; Role for read-only cloudwatch access and attach the &lt;span class="caps"&gt;AWS&lt;/span&gt;-managed CloudWatch Read Only
  policy to it (&lt;code&gt;arn:aws:iam::aws:policy/CloudWatchReadOnlyAccess&lt;/code&gt;). This is the role that our pre-authorized
  (federated) console login will&amp;nbsp;use.&lt;/li&gt;
&lt;li&gt;Create a new &lt;span class="caps"&gt;IAM&lt;/span&gt; User that we&amp;#8217;ll use to make the AssumeRole call from our Lambda function. This user should have
  a policy with only &lt;em&gt;one&lt;/em&gt; permission: calling &lt;code&gt;sts:AssumeRole&lt;/code&gt; on the &lt;span class="caps"&gt;IAM&lt;/span&gt; Role we created in the previous&amp;nbsp;step.&lt;/li&gt;
&lt;li&gt;Deploy our Lambda function, and pass the Access Key &lt;span class="caps"&gt;ID&lt;/span&gt; and Secret Access Key to it as environment variables.
  This is terrifyingly insecure (see note below), but little risk read-only credentials and an account that
  only has one other&amp;nbsp;user.&lt;/li&gt;
&lt;li&gt;Add code to our Lambda function to call AssumeRole for the cloudwatch read-only role, and then create the
  &lt;a href="http://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_providers_enable-console-custom-url.html"&gt;federated login &lt;span class="caps"&gt;URL&lt;/span&gt;&lt;/a&gt;
  for the Console. For this part, I found a really helpful
  &lt;a href="https://gist.github.com/weavenet/d21b288327bcc4947e690be13e19c79c"&gt;gist with the Python/boto3 implementation already done&lt;/a&gt;.
  For the &lt;code&gt;Destination&lt;/code&gt; parameter on the signin &lt;span class="caps"&gt;URL&lt;/span&gt;, I specified the full &lt;span class="caps"&gt;URL&lt;/span&gt; to the relevant
  CloudWatch Dashboard with my&amp;nbsp;metrics.&lt;/li&gt;
&lt;li&gt;Embed this &lt;span class="caps"&gt;URL&lt;/span&gt; in your &lt;span class="caps"&gt;SNS&lt;/span&gt; notification text. Most email clients should auto-link the &lt;span class="caps"&gt;URL&lt;/span&gt;, so you&amp;#8217;ll end up with an email
  notification that&amp;#8217;s still plaintext, but includes a clickable link for a read-only CloudWatch view with no
  additional authentication required. This provides a much quicker &amp;#8220;ok, what does this problem look like?&amp;#8221;&amp;nbsp;workflow.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;em&gt;Longer note on security:&lt;/em&gt; This isn&amp;#8217;t terribly secure. I wouldn&amp;#8217;t implement anything like this at my day job.
But I&amp;#8217;m the only user that has access to both my &lt;span class="caps"&gt;AWS&lt;/span&gt; account and my email. If someone gets access to my email,
the fact that they can also view my CloudWatch metrics is likely the least of my worries. Similarly, putting
actual &lt;span class="caps"&gt;IAM&lt;/span&gt; User credentials in Lambda environment variables is horribly, painfully, terrifyingly insecure. But
the credentials are read-only and only for CloudWatch, and in order to them, someone would need to have access
to one of &lt;em&gt;my&lt;/em&gt; Users in the account, all of which are much more privileged. So, I decided that it&amp;#8217;s an acceptably
small risk. I also wouldn&amp;#8217;t be handing out pre-signed URLs, even with a very limited read-only role, in a
multi-user context. But once again, for a single-user low-value account, it&amp;#8217;s a workable&amp;nbsp;solution.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Short note on cost:&lt;/em&gt; If I were setting up even a similarly minuscule infrastructure for any organization that
relied on it, I&amp;#8217;d certainly invest in real monitoring solutions. &lt;a href="https://www.datadoghq.com/pricing/"&gt;Datadog&amp;#8217;s pricing&lt;/a&gt;
isn&amp;#8217;t bad at all, with a $15 &lt;span class="caps"&gt;USD&lt;/span&gt; per month per host plan (their free plan has 1-day data retention, so it&amp;#8217;s
really just a demo) and &lt;a href="https://www.pagerduty.com/pricing/"&gt;PagerDuty starts at&lt;/a&gt; $9 &lt;span class="caps"&gt;USD&lt;/span&gt;/user/month. But the
combination of those two is more than my entire monthly infrastructure bill right now, so&amp;#8230; not really worth it for&amp;nbsp;me.&lt;/p&gt;
&lt;h2 id="next-steps"&gt;Next&amp;nbsp;Steps&lt;/h2&gt;
&lt;p&gt;If I get around to it, I&amp;#8217;d like to stop sending email and &lt;span class="caps"&gt;SNS&lt;/span&gt; notifications directly from CloudWatch alarms,
and instead pass them through a Lambda function first. This would provide a means to include the pre-authorized
Dashboard &lt;span class="caps"&gt;URL&lt;/span&gt; described above, as well as some additional context (such as the last N metrics for the alarm
and the alarm&amp;nbsp;history).&lt;/p&gt;
&lt;p&gt;Ideally, though this is quite a bit more work, I&amp;#8217;d figure out a simple way of rendering a graph of the
CloudWatch metric in question, and move email notifications from &lt;span class="caps"&gt;SNS&lt;/span&gt; to &lt;span class="caps"&gt;SES&lt;/span&gt;, sending &lt;span class="caps"&gt;HTML&lt;/span&gt; emails with
a bit more detail and some useful graphs. Another option would be to continue with &lt;span class="caps"&gt;SNS&lt;/span&gt;, but (assuming I
consider my email to be relatively secure and my notifications to be not-too-sensitive, both of which are
true) generate graphs and decently-useful &lt;span class="caps"&gt;HTML&lt;/span&gt; pages for each alert, upload them (at pseudo-random paths,
for some level of security from casual onlookers) to a public S3 bucket with website access enabled,
and include &lt;em&gt;that&lt;/em&gt; &lt;span class="caps"&gt;URL&lt;/span&gt; in the &lt;span class="caps"&gt;SNS&lt;/span&gt;&amp;nbsp;notification.&lt;/p&gt;</content><category term="aws"></category><category term="iam"></category><category term="monitoring"></category><category term="notifications"></category></entry><entry><title>Python script to check xfinity dataÂ usage</title><link href="http://blog.jasonantman.com/2017/04/python-script-to-check-xfinity-data-usage/" rel="alternate"></link><published>2017-04-17T16:11:00-04:00</published><updated>2017-04-17T16:11:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2017-04-17:/2017/04/python-script-to-check-xfinity-data-usage/</id><summary type="html">&lt;p&gt;Python/selenium script to check your Xfinity data&amp;nbsp;usage&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday I got one of those invasive, abusive, utterly awful (and idiotic) &lt;a href="https://www.techdirt.com/articles/20161123/10554936126/comcast-takes-heat-injecting-messages-into-internet-traffic.shtml"&gt;injected popups from Xfinity&lt;/a&gt; that I&amp;#8217;m at 75% of my monthly bandwidth allocation. Nevermind the fact that I have a bunch of automated scripts running on my computer and injected &lt;span class="caps"&gt;HTML&lt;/span&gt; might never be seen by a human, or that I work from home and every once in a while I&amp;#8217;ll find myself pulling and pushing multi-&lt;span class="caps"&gt;GB&lt;/span&gt; Docker images, which completely kills my &lt;span class="caps"&gt;1TB&lt;/span&gt; bandwidth limit. But it&amp;#8217;s only half way through the month and, frankly, I&amp;#8217;m pretty mystified how I could have used so much data this quickly. I went to Xfinity&amp;#8217;s site to check my usage meter - after rummaging around in my password manager to find my credentials - and realized that while it shows a graph of the past three months and a progress bar for the current month, it doesn&amp;#8217;t show me any detailed (i.e. daily or hourly) data that would help me figure out the&amp;nbsp;cause.&lt;/p&gt;
&lt;p&gt;So, I wrote a little &lt;a href="https://github.com/jantman/xfinity-usage"&gt;script&lt;/a&gt; using Python and Selenium to log in to their My Account site and screen-scrape the &lt;a href="http://www.xfinity.com/usagemeter"&gt;usage meter&lt;/a&gt;. Why Comcast would require me to log in to view my usage when I&amp;#8217;m accessing their site from the &lt;span class="caps"&gt;IP&lt;/span&gt; address &lt;em&gt;they&lt;/em&gt; gave me, on &lt;em&gt;their&lt;/em&gt; network, I have no idea&amp;#8230; unless it&amp;#8217;s to provide a disincentive for customers to be aware of their usage. But I wrote the script, and it seems to be working. For the time being, I&amp;#8217;m both pushing the results into Graphite so I can see usage over time, and sending myself a daily email so I can keep on top of&amp;nbsp;usage.&lt;/p&gt;
&lt;p&gt;Apparently Comcast used to have &lt;a href="http://usmapp-qa.comcast.net/"&gt;a desktop app&lt;/a&gt; to track usage but it&amp;#8217;s since been completely shut down, along with the &lt;span class="caps"&gt;API&lt;/span&gt; that backed it (which an enterprising fellow reverse-engineered in &lt;a href="https://github.com/WTFox/comcastUsage"&gt;this script&lt;/a&gt;). I can only assume this is another indication that, though the bandwidth cap was introduced citing &amp;#8220;network performance&amp;#8221;, they really don&amp;#8217;t want people lowering network load (and avoiding&amp;nbsp;fees).&lt;/p&gt;
&lt;p&gt;I don&amp;#8217;t remember anything about screen-scraping in the Xfinity terms of service - and if they&amp;#8217;re f-ing injecting elements into &lt;em&gt;my&lt;/em&gt; web traffic, I sure as hell hope they don&amp;#8217;t complain about me checking my own usage - but use this at your own risk. Also be aware that it&amp;#8217;s screen-scraping, so it may well break with a site redesign or element &lt;span class="caps"&gt;ID&lt;/span&gt;&amp;nbsp;changes.&lt;/p&gt;
&lt;p&gt;If anyone would find this useful, please see &lt;a href="https://github.com/jantman/xfinity-usage"&gt;https://github.com/jantman/xfinity-usage&lt;/a&gt;.&lt;/p&gt;</content><category term="comcast"></category><category term="xfinity"></category><category term="data"></category><category term="usage"></category><category term="bandwidth"></category><category term="cap"></category><category term="python"></category><category term="selenium"></category></entry><entry><title>VyOS on Alix 2C1 Single BoardÂ Computer</title><link href="http://blog.jasonantman.com/2017/03/vyos-on-alix-2c1-single-board-computer/" rel="alternate"></link><published>2017-03-04T12:27:00-05:00</published><updated>2017-03-04T12:27:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2017-03-04:/2017/03/vyos-on-alix-2c1-single-board-computer/</id><summary type="html">&lt;p&gt;How to install VyOS 1.1.7 on an Alix 2c1 or similar i386 single board&amp;nbsp;computer&lt;/p&gt;</summary><content type="html">&lt;p&gt;Back in 2011 I &lt;a href="/2011/09/vyatta-networkos-routerfirewall-on-alix-board-compact-flash/"&gt;wrote a post&lt;/a&gt; on installing
the formerly open-source &lt;a href="https://wiki.vyos.net/wiki/Vyatta"&gt;Vyatta&lt;/a&gt; router/firewall distribution on an Alix compact flash-based single board
computer. I&amp;#8217;d been using it for many years, since Vyatta Community was a completely F/&lt;span class="caps"&gt;OSS&lt;/span&gt; project. I stopped updating regularly sometime around
when Vyatta (now Vyatta Core, differentiated from their paid offering) began widening the gap between its F/&lt;span class="caps"&gt;OSS&lt;/span&gt; Core and paid versions.
It got much worse when &lt;a href="http://newsroom.brocade.com/press-releases/brocade-acquires-vyatta-a-pioneer-and-leader-in-s-nasdaq-brcd-0949599#.WLr6DkArJhE"&gt;Brocade acquired Vyatta&lt;/a&gt;
in 2012. Soon thereafter open source builds stopped, the forums were shut down, the source code was made much more difficult to find, and
eventually vyatta.org itself was shut down. I won&amp;#8217;t go into further detail as there&amp;#8217;s been a lot written about this debacle and forcible destruction of a community,
such as &lt;a href="http://dotbalm.org/brocade-missed-the-boat-with-vyatta/"&gt;Chris Wadge&amp;#8217;s post&lt;/a&gt; and
&lt;a href="https://libertysys.com.au/2013/08/the-tragedy-of-vyatta-cores-demise/"&gt;this one&lt;/a&gt;, but I will say that the above made it increasingly difficult to plan
an upgrade of my home&amp;nbsp;router.&lt;/p&gt;
&lt;p&gt;On the positive side, however, the &lt;a href="https://vyos.io/"&gt;VyOS&lt;/a&gt; F/&lt;span class="caps"&gt;OSS&lt;/span&gt; fork has emerged, and seems to have quite a vibrant community
at this point. A few weeks ago, I decided to finally take the time to upgrade to the latest VyOS 1.1.7 on my aged
&lt;a href="https://www.pcengines.ch/alix2c1.htm"&gt;Alix 2c1&lt;/a&gt; single board router (purchased in 2008; 3 &lt;span class="caps"&gt;LAN&lt;/span&gt;, 433 MHz &lt;span class="caps"&gt;AMG&lt;/span&gt; Geode &lt;span class="caps"&gt;LX700&lt;/span&gt;, &lt;span class="caps"&gt;128MB&lt;/span&gt; &lt;span class="caps"&gt;RAM&lt;/span&gt;).
VyOS is targeted at the cloud (virtualization) or &amp;#8220;real&amp;#8221; hardware, and doesn&amp;#8217;t seem to have anywhere near as many people
installing on dedicated SBCs as the former Vyatta community (probably because of the astonishing drop in the price of small, fanless
systems in recent years). I wasn&amp;#8217;t able to find much information about installing VyOS on such hardware, aside from
&lt;a href="https://forum.vyos.net/showthread.php?tid=6045"&gt;a few&lt;/a&gt; &lt;a href="https://forum.vyos.net/showthread.php?tid=26029"&gt;forum&lt;/a&gt;
&lt;a href="https://forum.vyos.net/showthread.php?tid=26881"&gt;threads&lt;/a&gt; and a &lt;a href="http://elderguerra.blogspot.com/2014/04/vyos-routerfirewall-on-alix-board.html"&gt;post on Elder Guerra&amp;#8217;s blog&lt;/a&gt;
that actually makes reference to and is based on my original post from&amp;nbsp;2011.&lt;/p&gt;
&lt;p&gt;So, for anyone who&amp;#8217;s interested, here&amp;#8217;s how I got VyOS 1.1.7 installed on my Alix&amp;nbsp;&lt;span class="caps"&gt;SBC&lt;/span&gt;:&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;A Linux machine with VirtualBox. For posterity, I did this on a recently updated Arch Linux machine, using VirtualBox 5.1.12 from Arch&amp;#8217;s repos (running via &lt;span class="caps"&gt;DKMS&lt;/span&gt; and with the Oracle extensions&amp;nbsp;installed).&lt;/li&gt;
&lt;li&gt;A Compact Flash card to perform the install on (I used &lt;a href="https://www.amazon.com/gp/product/B00PW1PH14/"&gt;this&lt;/a&gt; Wintec &amp;#8220;Industrial Grade&amp;#8221; &lt;span class="caps"&gt;SLC&lt;/span&gt; &lt;span class="caps"&gt;NAND&lt;/span&gt; &lt;span class="caps"&gt;4GB&lt;/span&gt; card from&amp;nbsp;amazon).&lt;/li&gt;
&lt;li&gt;A reader/writer for the card (my previous one was throwing errors, so I got &lt;a href="https://www.amazon.com/gp/product/B0056TYRMW/"&gt;this&lt;/a&gt; &lt;span class="caps"&gt;USB&lt;/span&gt; one from&amp;nbsp;Amazon).&lt;/li&gt;
&lt;li&gt;Ensure your user can read and write the raw disk devices. On Linux, this means your user must be in the &lt;code&gt;disk&lt;/code&gt; group. If it isn&amp;#8217;t, you&amp;#8217;ll need to log out and back in after making that&amp;nbsp;change.&lt;/li&gt;
&lt;li&gt;Assuming you&amp;#8217;re installing onto a headless board like the Alix, you&amp;#8217;ll need a null modem cable to connect to the serial console port, and whatever you need (&lt;span class="caps"&gt;USB&lt;/span&gt; to serial adapter) to plug that in to your&amp;nbsp;computer.&lt;/li&gt;
&lt;li&gt;A terminal emulator installed on your computer (I use &lt;a href="https://alioth.debian.org/projects/minicom"&gt;minicom&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="configuration-migration"&gt;Configuration&amp;nbsp;Migration&lt;/h2&gt;
&lt;p&gt;I just wanted to upgrade from my existing &lt;span class="caps"&gt;VC&lt;/span&gt; 6.3 installation, and use a new &lt;span class="caps"&gt;CF&lt;/span&gt; card. If you are doing a fresh install
and do not need to migrate the configuration, you can skip this&amp;nbsp;section.&lt;/p&gt;
&lt;p&gt;To migrate the configuration, I first set up a VyOS 1.1.7 VirtualBox &lt;span class="caps"&gt;VM&lt;/span&gt;, using the &lt;a href="https://github.com/higebu/vagrant-vyos"&gt;vagrant-vyos&lt;/a&gt;
plugin for &lt;a href="https://www.vagrantup.com/"&gt;Vagrant&lt;/a&gt; and the author&amp;#8217;s &lt;a href="https://atlas.hashicorp.com/higebu/boxes/vyos"&gt;vyos Vagrant box&lt;/a&gt;. I setup
three network interfaces on the &lt;span class="caps"&gt;VM&lt;/span&gt; to match the three on my Alix board, and put the &lt;code&gt;Vagrantfile&lt;/code&gt; in the same directory as my config
backups from the current&amp;nbsp;router.&lt;/p&gt;
&lt;p&gt;Once I had the &lt;span class="caps"&gt;VM&lt;/span&gt; up and running with &lt;span class="caps"&gt;SSH&lt;/span&gt; access, I ran &lt;code&gt;load /vagrant/config.boot&lt;/code&gt; to load the configuration backup, and let the
config migration tool do its work. This took a few iterations of modifying the old (&lt;span class="caps"&gt;VC&lt;/span&gt; 6.3) config until I got something that would
load cleanly into VyOS 1.1.7; note that per the &lt;a href="https://wiki.vyos.net/wiki/Migrating_from_Vyatta"&gt;Migrating from Vyatta&lt;/a&gt; documentation,
coming from &lt;span class="caps"&gt;VC&lt;/span&gt; 6.4 or earlier, there were some manual changes I had to make before the old configuration would load in VyOS.
Once that was done, I committed and saved the config, then rebooted the &lt;span class="caps"&gt;VM&lt;/span&gt; and confirmed that it
came up correctly configured. I run &lt;span class="caps"&gt;SSH&lt;/span&gt; on a non-default port, so before reloading the &lt;span class="caps"&gt;VM&lt;/span&gt; I needed to edit the &lt;code&gt;Vagrantfile&lt;/code&gt;
to add &lt;code&gt;config.ssh.guest_port = SSH_PORT_NUMBER&lt;/code&gt; and &lt;code&gt;config.vm.network "forwarded_port", guest: SSH_PORT_NUMBER, host: SSH_PORT_NUMBER, id: "ssh"&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once finished, I copied &lt;code&gt;/config/config.boot&lt;/code&gt; from the &lt;span class="caps"&gt;VM&lt;/span&gt; to my host &lt;span class="caps"&gt;OS&lt;/span&gt;. I removed the &lt;span class="caps"&gt;MAC&lt;/span&gt; addresses for the interfaces
and the &lt;code&gt;vagrant&lt;/code&gt; user, and then used that as the starting configuration for my new install on the Alix&amp;nbsp;board.&lt;/p&gt;
&lt;h2 id="installation"&gt;Installation&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Put your new &lt;span class="caps"&gt;CF&lt;/span&gt; card in the &lt;span class="caps"&gt;USB&lt;/span&gt; adapter and plug it in. Watch &lt;code&gt;dmesg&lt;/code&gt; to see what device name it&amp;#8217;s assigned. &lt;strong&gt;In this example, we&amp;#8217;ll call it /dev/sdX. Make &lt;span class="caps"&gt;SURE&lt;/span&gt; you correct that path in the below instructions to be the correct one for your &lt;span class="caps"&gt;CF&lt;/span&gt;&amp;nbsp;card.&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;Create a raw &lt;span class="caps"&gt;VMDK&lt;/span&gt; so VirtualBox can use the raw disk: &lt;code&gt;VBoxManage internalcommands createrawvmdk -filename /home/$USER/vyos_cf.vmdk -rawdisk /dev/sdX&lt;/code&gt; (note that the filename must be an absolute&amp;nbsp;path).&lt;/li&gt;
&lt;li&gt;Download the VyOS i586 &lt;span class="caps"&gt;ISO&lt;/span&gt; from &lt;a href="https://vyos.io/"&gt;vyos.net&lt;/a&gt;. Optionally verify the &lt;span class="caps"&gt;GPG&lt;/span&gt;&amp;nbsp;signature.&lt;/li&gt;
&lt;li&gt;Manually create a new VirtualBox&amp;nbsp;&lt;span class="caps"&gt;VM&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Select &amp;#8220;Linux&amp;#8221; and then &amp;#8220;Other Linux (32-bit)&amp;#8221; for the &lt;span class="caps"&gt;OS&lt;/span&gt;&amp;nbsp;type.&lt;/li&gt;
&lt;li&gt;Select the appropriate amount of &lt;span class="caps"&gt;RAM&lt;/span&gt; for your board (older Alix are 128 &lt;span class="caps"&gt;MB&lt;/span&gt; or 256&amp;nbsp;&lt;span class="caps"&gt;MB&lt;/span&gt;).&lt;/li&gt;
&lt;li&gt;Select &amp;#8220;Use an existing virtual hard disk file&amp;#8221; and select the raw &lt;span class="caps"&gt;VMDK&lt;/span&gt; you created in Step 2. Uncheck &amp;#8220;use host I/O&amp;nbsp;cache&amp;#8221;.&lt;/li&gt;
&lt;li&gt;Create the&amp;nbsp;&lt;span class="caps"&gt;VM&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Edit the &lt;span class="caps"&gt;VM&lt;/span&gt; settings to remove the floppy disk device, mount the &lt;span class="caps"&gt;ISO&lt;/span&gt; in the optical drive, disable audio, and disable all network&amp;nbsp;adapters.&lt;/li&gt;
&lt;li&gt;Boot the &lt;span class="caps"&gt;VM&lt;/span&gt;. Wait for the VyOS &lt;span class="caps"&gt;ISO&lt;/span&gt; to boot and log in using the information provided in the&amp;nbsp;banner.&lt;/li&gt;
&lt;li&gt;Run &lt;code&gt;install image&lt;/code&gt; and answer yes to the&amp;nbsp;prompt.&lt;/li&gt;
&lt;li&gt;Select Auto partitioning and select the &lt;span class="caps"&gt;CF&lt;/span&gt; card (it should be &lt;code&gt;sda&lt;/code&gt;, the only&amp;nbsp;option).&lt;/li&gt;
&lt;li&gt;Fill the whole device with the root&amp;nbsp;partition.&lt;/li&gt;
&lt;li&gt;Use the default name for the image and copy &lt;code&gt;/config/config.boot&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Set a password for the vyos Administrator&amp;nbsp;account.&lt;/li&gt;
&lt;li&gt;Setup grub on the one disk (&lt;code&gt;sda&lt;/code&gt;).&lt;/li&gt;
&lt;li&gt;You should now be returned to the&amp;nbsp;prompt.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;reboot&lt;/code&gt; and shut down the &lt;span class="caps"&gt;VM&lt;/span&gt; once it gets back to the &lt;span class="caps"&gt;BIOS&lt;/span&gt; or bootloader; installation is&amp;nbsp;complete.&lt;/li&gt;
&lt;li&gt;Delete the &lt;span class="caps"&gt;VM&lt;/span&gt; (don&amp;#8217;t delete files) and then remove the raw &lt;span class="caps"&gt;VMDK&lt;/span&gt; you created in Step&amp;nbsp;2.&lt;/li&gt;
&lt;li&gt;Mount the &lt;span class="caps"&gt;CF&lt;/span&gt; card partition on your host &lt;span class="caps"&gt;OS&lt;/span&gt; (it&amp;#8217;s an ext4 partition). For the purposes of this example, we&amp;#8217;ll assume we&amp;#8217;re mounting it at &lt;code&gt;/mnt/tmp&lt;/code&gt;: &lt;code&gt;mount /dev/sdX1 /mnt/tmp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;cd to the root of the partition: &lt;code&gt;cd /mnt/tmp&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Find and &lt;code&gt;cd&lt;/code&gt; to the &lt;code&gt;live-rw&lt;/code&gt; directory for your image; for VyOS 1.1.7 installed with the default image name of &amp;#8220;1.1.7&amp;#8221;, this is &lt;code&gt;boot/1.1.7/live-rw&lt;/code&gt; on the&amp;nbsp;partition.&lt;/li&gt;
&lt;li&gt;If you are migrating a configuration file (above section), copy your configuration file to &lt;code&gt;opt/vyatta/etc/config/config.new&lt;/code&gt; and chmod it&amp;nbsp;0755.&lt;/li&gt;
&lt;li&gt;Unmount the &lt;span class="caps"&gt;CF&lt;/span&gt; card and remove it from your&amp;nbsp;system.&lt;/li&gt;
&lt;li&gt;Find a &lt;span class="caps"&gt;DB9&lt;/span&gt; null modem cable and a &lt;span class="caps"&gt;USB&lt;/span&gt; to serial adapter. Plug the cable into the Alix board&amp;#8217;s serial port, and into your adapter, and plug it into the&amp;nbsp;system.&lt;/li&gt;
&lt;li&gt;Fire up your favorite terminal emulator (I use minicom) and connect at 9600&amp;nbsp;8N1.&lt;/li&gt;
&lt;li&gt;If you have the board running already (already being used for something), connect and make sure you get a prompt. It helps to know that the serial&amp;nbsp;works.&lt;/li&gt;
&lt;li&gt;If the board is running, power it down and unplug all connections before proceeding to the next&amp;nbsp;step.&lt;/li&gt;
&lt;li&gt;Open up the Alix enclosure, and swap in your new &lt;span class="caps"&gt;CF&lt;/span&gt;&amp;nbsp;card.&lt;/li&gt;
&lt;li&gt;Close the board, plug in serial and network&amp;nbsp;cables.&lt;/li&gt;
&lt;li&gt;Plug in the power cable, and watch your terminal emulator. If all went well, you&amp;#8217;ll get the board&amp;#8217;s &lt;span class="caps"&gt;BIOS&lt;/span&gt; and then the bootloader and kernel output. Eventually you should be dropped to a login&amp;nbsp;prompt.&lt;/li&gt;
&lt;li&gt;Log in as the vyos&amp;nbsp;user.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;configure&lt;/code&gt; to enter configuration mode. Then:&lt;ul&gt;
&lt;li&gt;If you are migrating a configuration as discussed above, &lt;code&gt;load /opt/vyatta/etc/config/config.new&lt;/code&gt;. It will take a while to load. Make any necessary changes, then run &lt;code&gt;commit&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;If you are starting from scratch, follow the &lt;a href="https://wiki.vyos.net/wiki/User_Guide"&gt;User Guide&lt;/a&gt; to setup the&amp;nbsp;system.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Once the commit is done (it will take a while), &lt;code&gt;save&lt;/code&gt;. The router should now be up and running with the desired&amp;nbsp;configuration.&lt;/li&gt;
&lt;li&gt;Reboot the router to ensure it comes up&amp;nbsp;correctly.&lt;/li&gt;
&lt;li&gt;Backup the running configuration somewhere&amp;nbsp;safe.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you&amp;#8217;re running the &lt;span class="caps"&gt;PC&lt;/span&gt; Engines Alix.2, update the serial settings on the board as described on &lt;a href="http://blog.jasonantman.com/2011/09/vyatta-networkos-routerfirewall-on-alix-board-compact-flash/"&gt;my blog&lt;/a&gt; or &lt;a href="http://elderguerra.blogspot.com/2014/04/vyos-routerfirewall-on-alix-board.html"&gt;this post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I hope this can be of use to&amp;nbsp;others.&lt;/p&gt;</content><category term="vyos"></category><category term="vyatta"></category><category term="alix"></category><category term="virtualbox"></category><category term="network"></category><category term="router"></category><category term="firewall"></category><category term="linux"></category><category term="sbc"></category></entry><entry><title>Yesterdayâs Widespread Internet Outage, forÂ non-geeks</title><link href="http://blog.jasonantman.com/2016/10/yesterdays-widespread-internet-outage-for-non-geeks/" rel="alternate"></link><published>2016-10-22T21:41:00-04:00</published><updated>2016-10-22T21:41:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2016-10-22:/2016/10/yesterdays-widespread-internet-outage-for-non-geeks/</id><summary type="html">&lt;p&gt;An explanation of yesterday&amp;#8217;s widespread &lt;span class="caps"&gt;DNS&lt;/span&gt; outage and why it&amp;nbsp;matters&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;See the end of this post for&amp;nbsp;updates.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Yesterday, Friday October 21st, 2016, Internet users in the &lt;span class="caps"&gt;US&lt;/span&gt; (initially the East Coast) experienced issues accessing many popular websites, including Twitter, Etsy, Spotify, &lt;span class="caps"&gt;CNN&lt;/span&gt;, Amazon and others. This also affected numerous companies and services used heavily in the technology industry for everything from storing and sharing source code to monitoring the health of services and alerting on-call technical&amp;nbsp;staff.&lt;/p&gt;
&lt;p&gt;The cause of the problems was an outage of Dyn, one of the leading providers of &lt;a href="https://en.wikipedia.org/wiki/Domain_Name_System"&gt;&lt;span class="caps"&gt;DNS&lt;/span&gt;&lt;/a&gt; services, the system which translates human-readable domain names (like &amp;#8220;blog.jasonantman.com&amp;#8221;) to the numeric &lt;span class="caps"&gt;IP&lt;/span&gt; addresses actually used by computers to communicate. Aside from the immediate take-away - that so many high-traffic websites are relying on a single &lt;span class="caps"&gt;DNS&lt;/span&gt; provider, and therefore a single point of failure - there are a number of important points about what&amp;nbsp;happened:&lt;/p&gt;
&lt;p&gt;1) This was not an accidental outage, it was an intentional attack by a malicious party; specifically a &lt;a href="https://en.wikipedia.org/wiki/Denial-of-service_attack"&gt;distributed denial of service (DDoS)&lt;/a&gt; attack, where an attacker takes control of tens or hundreds of thousands, or millions, of computers, and uses them to bombard the target with data. Assuming enough data is generated, as was the case yesterday, this will overwhelm the target, preventing it from fulfilling the requests of legitimate users. This was a very visible, intentional attack on critical Internet&amp;nbsp;infrastructure.&lt;/p&gt;
&lt;p&gt;2) Some of the leading experts on computer security, people who consult for Fortune-500 companies and have substantial access to non-public information, &lt;a href="https://www.schneier.com/blog/archives/2016/09/someone_is_lear.html"&gt;believe&lt;/a&gt; that this attack was one of a series carried out by an advanced, well-funded nation state attacker (likely either China or Russia, with evidence pointing to the former). Furthermore, evidence points to this attack not being the end-goal, but rather part of an escalating test to determine the point of failure of critical Internet infrastructure. The cyber equivalent of test-firing missiles and flying over enemy territory to map defenses, in preparation for an attack. Homeland Security and the &lt;span class="caps"&gt;FBI&lt;/span&gt; &lt;a href="http://www.reuters.com/article/us-usa-cyber-idUSKCN12L1ME"&gt;are investigating&lt;/a&gt;, but have not speculated publicly on the source of the&amp;nbsp;attack.&lt;/p&gt;
&lt;p&gt;3) The &lt;a href="https://krebsonsecurity.com/2016/10/hacked-cameras-dvrs-powered-todays-massive-internet-outage/"&gt;source&lt;/a&gt; &lt;a href="http://thehackernews.com/2016/10/iot-dyn-ddos-attack.html"&gt;of&lt;/a&gt; the flood of traffic that caused the outage was a &lt;a href="https://en.wikipedia.org/wiki/Botnet"&gt;botnet&lt;/a&gt;, a large network of tens of millions of computers that were hijacked by the attacker and used to send massive amounts of data to Dyn&amp;#8217;s systems. More specifically, the computers in this instance were &lt;a href="https://en.wikipedia.org/wiki/Internet_of_things"&gt;&amp;#8220;Internet of Things&amp;#8221; (IoT)&lt;/a&gt; Internet-connected devices, mainly &lt;span class="caps"&gt;IP&lt;/span&gt;-based video/surveillance cameras and video recorders; everything from warehouse and corporate surveillance cameras to &amp;#8220;nanny cams&amp;#8221; and video baby monitors. These devices pose a significant risk to the Internet, and therefore to our economic and physical infrastructure; they&amp;#8217;re often manufactured by companies that provide little to no support (the most common manufacturer provides none, selling their goods &amp;#8220;white box&amp;#8221; to be relabeled by distributors) and sold to users with little to no technical expertise. Unfortunately, there&amp;#8217;s also no legal requirement in the &lt;span class="caps"&gt;US&lt;/span&gt; for them to be secure, generally no procedure for software updates, and usually no way for users to enhance the security of the&amp;nbsp;devices.&lt;/p&gt;
&lt;p&gt;The first two pieces of information should be deeply disturbing to all of us; in an age when &amp;#8220;cyber warfare&amp;#8221; is constantly in the media, yesterday&amp;#8217;s even is the modern equivalent of a foreign power sending fighter planes over Washington, &lt;span class="caps"&gt;DC&lt;/span&gt;, to see how we react. There&amp;#8217;s no plausible explanation for this, short of testing our defenses for an imminent or possible attack. It&amp;#8217;s also important to note the implications of this beyond Twitter and Etsy; an attack of this magnitude launched against more than one - or all - of the leading &lt;span class="caps"&gt;DNS&lt;/span&gt; providers at once would cripple everything from banking, payments and credit card processing to healthcare and travel, and possibly large portions of our transportation, power and utility infrastructure. All areas of modern American life have become inextricably linked to the health of the Internet, whether we realize it or not (ever tried to eat at a restaurant when their &amp;#8220;computers were down,&amp;#8221; or fill your car with gas when there was a local Internet outage?) And while the original core of the network that became the Internet was designed to withstand a cold-war nuclear attack, the modern infrastructure relies increasingly on a small number of private companies. An attack of the scale possible from a nation-state has never happened before, and it&amp;#8217;s not likely that it would be handled&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;On a separate note, the use of IoT &amp;#8220;smart&amp;#8221; devices in this attack is particularly unsettling. While most people expect that their desktop and laptop computers and smartphones will receive regular software updates from a company that cares about their continued functionality, and expect that their devices won&amp;#8217;t be hijacked to attack critical infrastructure, the same is not true of the exploding field of &amp;#8220;smart&amp;#8221; devices. Robotic vacuum cleaners, Internet-based baby monitors, light bulbs that can be dimmed from your phone and networked thermostats are often sold as appliances at face-value, with very little ongoing support. Most people who buy these devices expect them to just work as advertised, and to be secure - they expect their baby monitor to &lt;a href="http://www.cbsnews.com/news/baby-monitors-connect-internet-vulnerable-hackers-cybersecurity/"&gt;not let hackers watch inside their homes&lt;/a&gt; and they expect their light bulbs to not attack critical Internet infrastructure on behalf of a third party. Perhaps the worst part is that, as many manufacturers of such devices are neither &amp;#8220;computer companies&amp;#8221; nor terribly interested in long-term customer relationships, every insecure device that&amp;#8217;s sold will likely be in operation for the next five to ten years, or more. We need both a legislative solution to this, as well as a consumer-focused solution; perhaps the modern equivalent of Underwriter&amp;#8217;s Laboratory for computerized devices. Without a solution, we may well end up in the world where, as &lt;a href="http://mjg59.dreamwidth.org/45098.html"&gt;suggested by some experts&lt;/a&gt;, it will be up to the Internet Service Provider of befuddled users to disconnect their service when a foreign power turns their toaster into a&amp;nbsp;weapon.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update Sunday October 23rd, 2016:&lt;/strong&gt; I didn&amp;#8217;t find it until now, but yesterday Dyn published an initial &lt;a href="http://hub.dyn.com/static/hub.dyn.com/dyn-blog/dyn-statement-on-10-21-2016-ddos-attack.html"&gt;statement on the attack&lt;/a&gt; on their Blog. The details confirm what I read earlier and said above, but only add one new bit of information beyond what I&amp;#8217;d read earlier: the botnet(s) acting as the source of the attack were vastly larger than originally stated; &amp;#8220;tens of millions of &lt;span class="caps"&gt;IP&lt;/span&gt; addresses&amp;#8221; rather than the approximately 500,000 originally suspected. I&amp;#8217;ll be keeping an eye out for a more detailed follow-up from&amp;nbsp;Dyn.&lt;/p&gt;</content><category term="Dyn"></category><category term="DNS"></category><category term="botnet"></category><category term="security"></category><category term="DDoS"></category></entry><entry><title>OpenSSH changing hostnames based onÂ location</title><link href="http://blog.jasonantman.com/2016/08/openssh-changing-hostnames-based-on-location/" rel="alternate"></link><published>2016-08-27T11:22:00-04:00</published><updated>2016-08-27T11:22:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2016-08-27:/2016/08/openssh-changing-hostnames-based-on-location/</id><summary type="html">&lt;p&gt;How to change &lt;span class="caps"&gt;SSH&lt;/span&gt; hostnames based on guessed&amp;nbsp;location&lt;/p&gt;</summary><content type="html">&lt;p&gt;Yesterday I was doing some work on my laptop, SSHed in to my desktop (&amp;#8220;phoenix&amp;#8221;). As
always happens when I&amp;#8217;m using my laptop from home, I kept getting connection errors&amp;#8230;
because my &lt;code&gt;~/.ssh/config&lt;/code&gt; on my laptop is setup with my dynamic &lt;span class="caps"&gt;DNS&lt;/span&gt; hostname and port
to reach my desktop, for any time I&amp;#8217;m out of the house. But those don&amp;#8217;t work while
on the home network, and I got really tired of having to &lt;code&gt;ssh 192.168.0.24&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;It turns out that, as long as your&amp;#8217;re using &lt;a href="http://www.openssh.com/txt/release-6.5"&gt;OpenSSH &amp;gt;= 6.5&lt;/a&gt;,
the &lt;code&gt;ssh_config (5)&lt;/code&gt; file (typically &lt;code&gt;~/.ssh/config&lt;/code&gt;) supports a &lt;code&gt;Match&lt;/code&gt; directive
that can execute system commands, and either match or not based on exit&amp;nbsp;code.&lt;/p&gt;
&lt;p&gt;I came up with relatively naive script that tries to determine whether or not I&amp;#8217;m on my
home network based on a combination of &lt;code&gt;resolv.conf&lt;/code&gt; settings, &lt;span class="caps"&gt;IP&lt;/span&gt; address and WiFi&amp;nbsp;&lt;span class="caps"&gt;SSID&lt;/span&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="c1"&gt;# test if I&amp;#39;m on my home network,&lt;/span&gt;
&lt;span class="c1"&gt;# for &lt;span class="caps"&gt;SSH&lt;/span&gt; matching. Somewhat naive.&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;span class="c1"&gt;# For use in ~/.ssh/config Match directive;&lt;/span&gt;
&lt;span class="c1"&gt;# exit 0 if I&amp;#39;m at home, exit 1 otherwise&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;span class="c1"&gt;# To debug, run script directly as `bash -x am_i_am_home.sh`&lt;/span&gt;
&lt;span class="c1"&gt;########&lt;/span&gt;

&lt;span class="c1"&gt;# check that I&amp;#39;ve got the right nameserver and search domain; exit otherwise&lt;/span&gt;
grep -q &lt;span class="s1"&gt;&amp;#39;jasonantman.com&amp;#39;&lt;/span&gt; /etc/resolv.conf &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
grep &lt;span class="s1"&gt;&amp;#39;^nameserver&amp;#39;&lt;/span&gt; /etc/resolv.conf &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s1"&gt;&amp;#39;192.168.0.1&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;

&lt;span class="c1"&gt;# check that I&amp;#39;ve got a 192.168.0. address; exit otherwise&lt;/span&gt;
ip addr &lt;span class="p"&gt;|&lt;/span&gt; grep -q &lt;span class="s1"&gt;&amp;#39;192.168.0.&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;

&lt;span class="c1"&gt;# check that I&amp;#39;m connected to one of my SSIDs; if so, exit 0 (match)&lt;/span&gt;
nmcli -t -f active,ssid dev wifi &lt;span class="p"&gt;|&lt;/span&gt; grep -q &lt;span class="s1"&gt;&amp;#39;^yes:ObiWAN&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;
nmcli -t -f active,ssid dev wifi &lt;span class="p"&gt;|&lt;/span&gt; grep -q &lt;span class="s1"&gt;&amp;#39;^yes:&lt;span class="caps"&gt;WAP1&lt;/span&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;0&lt;/span&gt;

&lt;span class="c1"&gt;# assume not; no match&lt;/span&gt;
&lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This can be used in my &lt;code&gt;~/.ssh/config&lt;/code&gt; to trigger an initial (internal network)
directive if it exits 0, and fall through to the external-network directive otherwise,
as shown below. The &lt;code&gt;originalhost phoenix&lt;/code&gt; portion of the &lt;code&gt;Match&lt;/code&gt; line ensures
that it&amp;#8217;s only executed if I &lt;code&gt;ssh phoenix&lt;/code&gt;, so it doesn&amp;#8217;t conflict with other
host&amp;nbsp;directives.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;# phoenix when at home
Match originalhost phoenix exec &amp;quot;/home/jantman/bin/am_i_at_home.sh&amp;quot;
     HostName phoenix
     Port 22

# fall-through - phoenix when abroad
Host phoenix
     HostName my_dynamic_hostname
     Port &amp;lt;something other than 22&amp;gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="openssh"></category><category term="ssh"></category></entry><entry><title>The Psychological Side of RemoteÂ Work</title><link href="http://blog.jasonantman.com/2016/08/the-psychological-side-of-remote-work/" rel="alternate"></link><published>2016-08-26T15:10:00-04:00</published><updated>2016-08-26T15:10:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2016-08-26:/2016/08/the-psychological-side-of-remote-work/</id><summary type="html">&lt;p&gt;Some thoughts on the psychological aspects of remote&amp;nbsp;work&lt;/p&gt;</summary><content type="html">&lt;p&gt;For most of my professional career, I&amp;#8217;ve worked from home a few days a week - and a few times, for
stretches of a few weeks - and that&amp;#8217;s been the norm on most of my teams, not counting full-time
remote workers. But for all the talk about remote work, much of it seems to focus on tools,
collaboration and productivity - and relatively little on the human side, and tips to help
balance work and life when the work happens in your home (or wherever else you&amp;nbsp;are).&lt;/p&gt;
&lt;p&gt;These certainly aren&amp;#8217;t for everyone, but here are some of the tricks I&amp;#8217;ve accumulated so
far that help me be happier, more productive and more sane when working from home, whether
it&amp;#8217;s a day or few a week or a few weeks straight. I know these aren&amp;#8217;t applicable to everyone,
and they also assume a culture that already values work-life balance, but might not spend
a lot of time talking about the little hacks to make remote work better. Not all of these
will work for every person, and there are plenty I&amp;#8217;m surely forgetting or not aware&amp;nbsp;of.&lt;/p&gt;
&lt;h2 id="1-dress-for-the-job"&gt;1. Dress for the&amp;nbsp;Job&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;ve made the &amp;#8220;I&amp;#8217;m working from home so I&amp;#8217;m not wearing pants&amp;#8221; joke plenty of times
myself. But it&amp;#8217;s never true. When I&amp;#8217;m working from home, the first thing I do is
go through the same morning routine I do when going in to the office - most importantly,
making coffee and getting dressed. Lately I&amp;#8217;ve taken to wearing &amp;#8220;nicer&amp;#8221; (read: having
buttons somewhere on them) shirts to the office, and &lt;span class="caps"&gt;OK&lt;/span&gt;, I&amp;#8217;ll wear a t-shirt when I&amp;#8217;m
home. But I still put on long pants and&amp;nbsp;shoes.&lt;/p&gt;
&lt;p&gt;For me, this is a big part of the psychological side of work/life separation. I have
&amp;#8220;work clothes&amp;#8221; and &amp;#8220;hanging around the house&amp;#8221; clothes. When I&amp;#8217;m going to be working,
I wear work clothes. When I&amp;#8217;m done for the day, a (psychologically) big part of my
end-of-the-work-day routine is changing from work clothes to &amp;#8220;hanging around&amp;#8221;&amp;nbsp;clothes.&lt;/p&gt;
&lt;p&gt;Whoever I heard this from - and I&amp;#8217;m sad to say I don&amp;#8217;t even remember who it was - probably
sounded crazy to me. But it really goes a long way to psychologically separate work time
from personal time - especially on the days when I finish up work and get right back to
the keyboard working on personal projects or&amp;nbsp;gaming.&lt;/p&gt;
&lt;h2 id="2-separate-profiles"&gt;2. Separate&amp;nbsp;Profiles&lt;/h2&gt;
&lt;p&gt;Keep separate profiles in your browser, one for work and one for personal.
&lt;a href="https://support.google.com/chrome/answer/2364824?hl=en"&gt;Chrome&lt;/a&gt; and
&lt;a href="https://www.chromium.org/user-experience/multi-profiles"&gt;Chromium&lt;/a&gt; have support
for separate users; Firefox recently added built-in
&lt;a href="https://support.mozilla.org/en-US/kb/profile-manager-create-and-remove-firefox-profiles"&gt;support for switching profiles&lt;/a&gt;,
which is quite nice for those of us who had been using &lt;a href="https://developer.mozilla.org/en-US/docs/Mozilla/Profile_Manager"&gt;Profile Manager&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;While I&amp;#8217;m sure there are some people who do this simply to keep history/bookmarks/etc. separate,
I do it as an incentive to focus on work when I&amp;#8217;m at work and focus on not work when I&amp;#8217;m not.
At the end of my work day I close my &amp;#8220;work&amp;#8221; browser session and open my &amp;#8220;personal&amp;#8221; one back
up. Thanks to session restore, all of my tabs are right where I left them&amp;#8230; and I don&amp;#8217;t have
a reason to leave distracting personal tabs open during the work day, or work tabs open
when I&amp;#8217;m on my own&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;(I know this doesn&amp;#8217;t work for everyone, especially if you&amp;#8217;re on-call and it&amp;#8217;s busy. In those
cases, I usually keep tabs in separate windows, one for work and one for personal; it&amp;#8217;s all
there when I need it, but out of the way when I&amp;nbsp;don&amp;#8217;t).&lt;/p&gt;
&lt;h2 id="3-take-breaks"&gt;3. Take&amp;nbsp;Breaks&lt;/h2&gt;
&lt;p&gt;When I&amp;#8217;m physically in the office, my day is usually broken up by moments away from my computer
work with colleagues - whether it&amp;#8217;s walking to a meeting or lunch, whatever conversations pop up
in the room, or even just whiteboarding a problem. Sure, there&amp;#8217;s plenty of conversation on HipChat
when we&amp;#8217;re remote, but I&amp;#8217;m not &lt;em&gt;getting away from the keyboard&lt;/em&gt; like I do in the&amp;nbsp;office.&lt;/p&gt;
&lt;p&gt;Take breaks. Take the dogs out, take five minutes to play with your kids or pets, go make coffee
or lunch, whatever. Just be sure to break up your day and not fall into the trap of sitting in
the same spot for however many hours staring at a monitor. Our bodies need &amp;#8220;get up and walk around&amp;#8221;
time, but our minds need a change of scenery too. Five minutes away from the same monitor is helpful.
Sure, if I&amp;#8217;m deep in the middle of something, I&amp;#8217;ll keep at it. But I usually find that stepping
away for a few minutes now and then at logical context switch points helps a lot. And if it&amp;#8217;s
a day of mostly writing code, it also provides a good time to think about what I&amp;#8217;m working on
without the burning urge to immediately start&amp;nbsp;implementing.&lt;/p&gt;
&lt;p&gt;Don&amp;#8217;t fall into the trap of feeling that work from home time demands that
you&amp;#8217;re glued to the keyboard every second without pause - nobody in the office is doing that.
And don&amp;#8217;t fall into the trap of feeling like you have to prove you&amp;#8217;re being productive while
remote by never taking a break - if you&amp;#8217;re like many people I know, you&amp;#8217;re probably already
more productive when you&amp;#8217;re in an environment that feels more comfortable (you can always
point people to Psychology Today&amp;#8217;s
&lt;a href="https://www.psychologytoday.com/blog/mind-the-manager/201605/remote-workers-are-happier-and-more-productive"&gt;Remote Workers Are Happier and More Productive&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Also, don&amp;#8217;t give up opportunities to change your surroundings. If you have a webinar or
online meeting, grab the laptop and take it from the couch (assuming the setting is acceptable
for the attendees). Need to read a lengthy article or documentation? Bring the laptop
outside. Anything to get a mental and visual change of&amp;nbsp;pace.&lt;/p&gt;
&lt;h2 id="4-lunch-is-holy"&gt;4. Lunch is&amp;nbsp;Holy&lt;/h2&gt;
&lt;p&gt;When I&amp;#8217;m in the office, if my team doesn&amp;#8217;t have lunch plans (which is the rule rather
than the exception), I tend to eat at my desk, and only take as much time as I need
to eat. Sometimes I&amp;#8217;m good to myself and read something, other times I just work while
I&amp;nbsp;eat.&lt;/p&gt;
&lt;p&gt;When I&amp;#8217;m at home, I take the 45 minutes we&amp;#8217;re given for lunch, every day. If my wife is
home, we watch something on &lt;span class="caps"&gt;TV&lt;/span&gt; and eat together. If she isn&amp;#8217;t, I catch up on an episode
of something on Netflix. But no matter what I do, I set aside the full &amp;#8220;official&amp;#8221;
45 minutes to decompress, step away from the keyboard, and breathe a bit. Sure, sometimes
my mind is still churning away on whatever I&amp;#8217;m working on, and I usually only break
for food at a logical stopping point. But it&amp;#8217;s still an important part of staying
focused through the day, and not hitting the end of the day feeling exhausted and&amp;nbsp;glassy-eyed.&lt;/p&gt;
&lt;h2 id="5-disconnect"&gt;5.&amp;nbsp;Disconnect&lt;/h2&gt;
&lt;p&gt;On-call or similar exceptions aside, when they day&amp;#8217;s over, the day&amp;#8217;s over. Disconnect.
I have HipChat and various other communication tools on my phone if I absolutely have
to be reached after hours. But aside from that, when my work day is over, I close my
work browser session and switch to personal, close any editors/windows/etc. I have open,
and ensure all of my changes are committed and pushed. As much as I occasionally rue
the fact that my current job has almost everything on our private network, it means
I can disconnect my &lt;span class="caps"&gt;VPN&lt;/span&gt; client and be unable to access anything work-related. I&amp;#8217;ll
admit that I usually leave my work-related &lt;code&gt;screen&lt;/code&gt; windows open overnight, just
for ease of picking back up, but I always close them over a weekend or&amp;nbsp;vacation.&lt;/p&gt;
&lt;p&gt;Unless you&amp;#8217;re being paid for a 168-hour work week, you deserve to have some time
away from work. I&amp;#8217;ll admit I&amp;#8217;ve had some jobs where on-call was&amp;#8230; quite busy&amp;#8230;
but it doesn&amp;#8217;t take that long to connect and start up what you need, when needed.
You owe it to yourself: when you&amp;#8217;re done, make the work stuff disappear from your
field of view. If you spend much time at your computer outside of work, you need
to keep the two separate&amp;#8230; or else &amp;#8220;work time&amp;#8221; becomes &amp;#8220;awake&amp;nbsp;time.&amp;#8221;&lt;/p&gt;
&lt;h2 id="6-end-of-day-routine"&gt;6. End-of-day&amp;nbsp;Routine&lt;/h2&gt;
&lt;p&gt;Develop a daily end-of-work routine or ritual. Do something to psychologically say
to yourself &amp;#8220;I&amp;#8217;m done with work for today, I&amp;#8217;m switching gears to my personal life.&amp;#8221;
If your office is also your home, you miss out on the decompression and context switch
provided by a commute (even a short one). Put something in its place. This is
especially important if your after-work time, even occasionally, involves going back
to the same chair, keyboard and monitor(s) - whatever the&amp;nbsp;reason.&lt;/p&gt;
&lt;p&gt;Change your clothes, catch up with family, spend some time with your pets, watch a &lt;span class="caps"&gt;TV&lt;/span&gt;
show or read a book, cook dinner, go for a walk - whatever. Just do something to signal
to yourself that the work day is over, and you can leave all of those thoughts, anxieties,
problems, ideas and tasks until tomorrow. As much as you may intellectually know that
you&amp;#8217;re &amp;#8220;done&amp;#8221; for the day (even if your pager might go off), do something to &lt;em&gt;show&lt;/em&gt;
yourself, and tell your subconscious, that you&amp;#8217;re &amp;#8220;going&amp;nbsp;home.&amp;#8221;&lt;/p&gt;
&lt;h2 id="7-get-outside"&gt;7. Get&amp;nbsp;Outside&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;m an introvert, and have occasionally suffered from mild depression. Neither of those
is unusual in our industry; the former is just much more accepted and discussed
than the latter&amp;nbsp;(unfortunately).&lt;/p&gt;
&lt;p&gt;If you can, &lt;strong&gt;get outside&lt;/strong&gt;. Even if it&amp;#8217;s just to run an errand, take a short walk,
or drive around the block, get out of the house. This is especially important if
you usually don&amp;#8217;t (but can), or if you&amp;#8217;re working long hours. At a previous job
we had an &amp;#8220;incident&amp;#8221; that resulted in the engineering team working round-the-clock
for the better part of two weeks; we ended up scheduling a 24x7 rotation, but until
that two-week-ish mark, we were all more-or-less working whenever we were awake.
It was awful for all of us - both our personal well-being and our productivity
and focus. Even just running to the store or to pick up dinner was an immense
relief from the wake-work-sleep-repeat&amp;nbsp;cycle.&lt;/p&gt;
&lt;p&gt;Even if you&amp;#8217;re not working insane hours (and I certainly hope you&amp;#8217;re not), be aware
of how much time you&amp;#8217;re spending at home indoors. If you&amp;#8217;re not terribly social by
nature, make sure you get outside, and do it on a regular basis. Even if you don&amp;#8217;t
&lt;em&gt;have&lt;/em&gt; to go somewhere, do it anyway for a change of scenery and a chance to clear
your&amp;nbsp;mind.&lt;/p&gt;
&lt;h2 id="8-talk-about-burnout-and-frustrations"&gt;8. Talk About Burnout and&amp;nbsp;Frustrations&lt;/h2&gt;
&lt;p&gt;This goes for everyone, but more so for people who are remote full-time or for
long stretches, on a team that&amp;#8217;s at least partially in-office. It&amp;#8217;s really easy
to feel disconnected from what&amp;#8217;s happening, and it&amp;#8217;s also really difficult -
especially if you do most of your communication via text - for your manager and
co-workers to know when you&amp;#8217;re feeling stressed or overworked. Talk about&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;Burnout - even to the point of suicide - in technology has been talked about
more recently, including John Willis&amp;#8217;
&lt;a href="http://itrevolution.com/karojisatsu/"&gt;KarÅjisatsu&lt;/a&gt;, Gene Kim&amp;#8217;s recent work,
and &lt;a href="https://victorops.com/blog/burnout-in-tech/"&gt;Jason Hand&lt;/a&gt;. It&amp;#8217;s certainly
something that our industry needs to be &lt;em&gt;much&lt;/em&gt; more aware of, and something we
all need to work to fix, both personally and with our colleagues. If you&amp;#8217;re a
manager of remotes, make sure you check in with them often - not just on how
their work is going, but on how they&amp;#8217;re doing emotionally, especially related
to work. Find out what&amp;#8217;s frustrating them, what&amp;#8217;s draining their energy and
how their work-life balance is. If you&amp;#8217;re not a manager, check in with your
coworkers about the same things, and make sure someone knows how you&amp;#8217;re doing.
It might feel awkward when you&amp;#8217;re having a great time or starting a new job,
but it&amp;#8217;s a habit that&amp;#8217;s much easier to get into when things are going well
than when you really need&amp;nbsp;it.&lt;/p&gt;</content><category term="wfh"></category><category term="remote"></category><category term="work"></category><category term="balance"></category><category term="work from home"></category><category term="culture"></category></entry><entry><title>Tooling for AWS - webhooks to SQS via API Gateway andÂ Lambda</title><link href="http://blog.jasonantman.com/2016/08/tooling-for-aws-webhooks-to-sqs-via-api-gateway-and-lambda/" rel="alternate"></link><published>2016-08-06T21:38:00-04:00</published><updated>2016-08-06T21:38:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2016-08-06:/2016/08/tooling-for-aws-webhooks-to-sqs-via-api-gateway-and-lambda/</id><summary type="html">&lt;p&gt;Project I created that uses Python and Terraform to setup an &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; Gateway instance to receive webhooks, and enqueue their content in &lt;span class="caps"&gt;SQS&lt;/span&gt; queues via&amp;nbsp;Lambda.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few weeks ago at work, I was party to two discussions about possible tooling needs, both very low-priority. One was the possible need to sync MarkDown documentation
from GitHub repositories to&amp;#8230; another thing that can hold docs. The other was relating to the new Version 2 Docker Registry, &lt;a href="https://github.com/docker/distribution"&gt;distribution&lt;/a&gt;.
We have some Jenkins jobs that dynamically populate dropdown fields for build parameters with Docker image names and tags, using the &lt;a href="https://wiki.jenkins-ci.org/display/JENKINS/Active+Choices+Plugin"&gt;Active Choices Plugin&lt;/a&gt;.
Right now we&amp;#8217;re directly querying the Docker Registry &lt;span class="caps"&gt;API&lt;/span&gt; from Groovy, every time the Build With Parameters page is loaded. With the original version 1 Docker Registry,
images were often missing from the results (eek!) but the performance was good. With the switch to the v2 Registry, it takes almost two minutes to load the page.
While brainstorming solutions, we decided that caching the list of images and tags in the Registry was the solution. For bonus points, it would also be nice to
be able to query based on image labels - something that&amp;#8217;s not exposed in the Registry &lt;span class="caps"&gt;API&lt;/span&gt; at all. Luckily, the Registry has an option to fire a webhook every time
a new image is&amp;nbsp;pushed.&lt;/p&gt;
&lt;p&gt;Both of these problems have solutions that involve webhooks, from GitHub and Docker Distribution, respectively. They also both involve doing time-consuming things in custom code with the
data in those hooks - transforming MarkDown to another markup and pushing the result to an on-premesis system in the case of GitHub, and &lt;code&gt;pull&lt;/code&gt;ing and inspecting Docker
images in the case of the Registry. As such, the &amp;#8220;typical&amp;#8221; webhook things like &lt;a href="https://zapier.com/"&gt;Zapier&lt;/a&gt; won&amp;#8217;t fit the bill. All I really needed was something to receive webhooks
and push the content of them into a queue. Ideally, it would also be something that would utilize existing services we have, namely&amp;nbsp;&lt;span class="caps"&gt;AWS&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;After working a bunch of nights and the good part of a weekend, I have a solution: my new &lt;a href="https://pypi.python.org/pypi/webhook2lambda2sqs"&gt;webhook2lambda2sqs&lt;/a&gt; Python&amp;nbsp;package.&lt;/p&gt;
&lt;p&gt;This implements what I think is the cheapest and lowest-overhead solution for anyone with an existing &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;nbsp;account:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Setup an &lt;a href="https://aws.amazon.com/api-gateway/"&gt;&lt;span class="caps"&gt;API&lt;/span&gt; Gateway&lt;/a&gt; that receives json &lt;span class="caps"&gt;POST&lt;/span&gt; and &lt;span class="caps"&gt;GET&lt;/span&gt;&amp;nbsp;requests.&lt;/li&gt;
&lt;li&gt;It passes them to a &lt;a href="https://aws.amazon.com/lambda/"&gt;Lambda Function&lt;/a&gt; which pushes the content to one or more &lt;a href="https://aws.amazon.com/sqs/"&gt;&lt;span class="caps"&gt;SQS&lt;/span&gt;&lt;/a&gt; queues, for consumption by an&amp;nbsp;application.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The tooling is written in Python, but leverages &lt;a href="https://www.terraform.io/"&gt;HashiCorp&amp;#8217;s Terraform&lt;/a&gt; to actually manage the &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;nbsp;resources.&lt;/p&gt;
&lt;p&gt;From a &lt;span class="caps"&gt;JSON&lt;/span&gt; configuration file as simple&amp;nbsp;as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;{
  &amp;quot;endpoints&amp;quot;: {
    &amp;quot;some_resource_name&amp;quot;: {
      &amp;quot;method&amp;quot;: &amp;quot;&lt;span class="caps"&gt;POST&lt;/span&gt;&amp;quot;,
      &amp;quot;queues&amp;quot;: [&amp;quot;myqueue&amp;quot;]
    },
  },
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and a single command (&lt;code&gt;webhook2lambda2sqs genapply&lt;/code&gt;), you&amp;#8217;ll have the complete system up and running, receiving &lt;span class="caps"&gt;HTTP&lt;/span&gt; &lt;span class="caps"&gt;POST&lt;/span&gt; requests
at an &lt;span class="caps"&gt;AWS&lt;/span&gt;-generated &lt;span class="caps"&gt;URL&lt;/span&gt; and pushing them into the &lt;code&gt;myqueue&lt;/code&gt; &lt;span class="caps"&gt;SQS&lt;/span&gt; queue. Best of all, going by my testing (this is based on the time
the Lambda function takes to run, which can vary quite a bit), the whole thing is &lt;strong&gt;free for the first 1 million requests per month&lt;/strong&gt;
if your account is still on the Free Tier, and otherwise is less than $4/month for the first million&amp;nbsp;requests.&lt;/p&gt;
&lt;p&gt;The configuration can handle setting up multiple distinct endpoint paths in the same &lt;span class="caps"&gt;API&lt;/span&gt; Gateway, each
sending the data to one or more &lt;span class="caps"&gt;SQS&lt;/span&gt; queues. It also has options for enabling logging (to CloudWatch Logs) both in the function
and on the &lt;span class="caps"&gt;API&lt;/span&gt; Gateway, pushing &lt;span class="caps"&gt;API&lt;/span&gt; Gateway metrics to CloudWatch, and configuring rate&amp;nbsp;limiting.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;webhook2lambda2sqs&lt;/code&gt; program generates the Python code for the lambda function and packages it correctly for Lambda, and
then generates a Terraform configuration to manage all required &lt;span class="caps"&gt;AWS&lt;/span&gt; resources. Separate commands are available that wrap Terraform
(mainly to deal with some issues with its &lt;span class="caps"&gt;API&lt;/span&gt; Gateway implementation) to run &lt;code&gt;plan&lt;/code&gt;, &lt;code&gt;apply&lt;/code&gt; and &lt;code&gt;destroy&lt;/code&gt;. There are
also helper commands to view the Lambda Function and &lt;span class="caps"&gt;API&lt;/span&gt; Gateway logs from CloudWatch, view messages in the queue(s) and
&lt;span class="caps"&gt;GET&lt;/span&gt; or &lt;span class="caps"&gt;POST&lt;/span&gt; a test message to one or all of the&amp;nbsp;endpoints.&lt;/p&gt;
&lt;p&gt;Full documentation is available at &lt;a href="http://webhook2lambda2sqs.readthedocs.io/en/latest/"&gt;http://webhook2lambda2sqs.readthedocs.io/en/latest/&lt;/a&gt;
and the package (Python 2.7, 3.3-3.5) can be downloaded &lt;a href="https://pypi.python.org/pypi/webhook2lambda2sqs"&gt;from PyPI&lt;/a&gt;.&lt;/p&gt;</content><category term="aws"></category><category term="webhook"></category><category term="lambda"></category><category term="github"></category><category term="api-gateway"></category><category term="sqs"></category><category term="queue"></category><category term="python"></category><category term="terraform"></category></entry><entry><title>A Short Comment on Gun Rights inÂ America</title><link href="http://blog.jasonantman.com/2016/06/a-short-comment-on-gun-rights-in-america/" rel="alternate"></link><published>2016-06-13T20:02:00-04:00</published><updated>2016-06-13T20:02:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2016-06-13:/2016/06/a-short-comment-on-gun-rights-in-america/</id><summary type="html">&lt;p&gt;A diversion from &amp;#8220;all technical&amp;#8221; content, on gun ownership in&amp;nbsp;America.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This blog has, for many years, been unabashedly &amp;#8220;all tech.&amp;#8221; I&amp;#8217;ve often had second thoughts about that, wondering if I should give the rest of my self - my political, moral and ethical views - more of a voice. But I&amp;#8217;ve always put that aside - partially because this &lt;em&gt;always&lt;/em&gt; comes up when I&amp;#8217;m looking for a new job and I don&amp;#8217;t really want to be remembered as that guy with the strongly disagreeing views, partially because I don&amp;#8217;t want people looking for technical content to be frustrated by my rants. And partially because it&amp;#8217;s just harder for me to convey political or social concepts in the way I intend than it is to convey technical ones. But after the events of the past 36 hours - and moreover, of the past decade-plus - I&amp;#8217;m going to deviate from that. &lt;strong&gt;Be advised&lt;/strong&gt; that if you have strong feelings on either side of the gun control debate, this probably isn&amp;#8217;t going to make you&amp;nbsp;happy.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m a gun owner. I have been since my eighteenth birthday, the day when I became legally able to exercise my second amendment rights in my then-home state of New Jersey. That was only a decade ago, but the impact of guns on America seems so shockingly different. I own a single .22-caliber target pistol; certainly dangerous as any firearm is, but essentially the bottom of the scale when it comes to lethality. It&amp;#8217;s designed to shoot paper, not much else. I&amp;#8217;ve been shooting off and on since I was in my early teens, mostly handguns and shotguns. Apparently unlike most people who are vocal gun owners, I&amp;#8217;ve never fired a shot at anything that is or was ever alive, unless you count paper. I&amp;#8217;ve never fired a shot outside of a shooting&amp;nbsp;range.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve also never carried a gun in public, nor have I ever had a serious desire to. Despite what gun-rights activists claim about self-defense and defense of others, I think that the way police in this country are both trained and act in the field is a wonderful example of why firearm carry in public often doesn&amp;#8217;t work: we&amp;#8217;re talking about people whose &lt;em&gt;profession&lt;/em&gt; involves firearms, who must train and qualify with their guns annually (or more often), and still often can&amp;#8217;t hit a target in a real-life situation. As far as I&amp;#8217;m concerned, if someone wants to carry a firearm to defend themselves or others, they damn well better be shooting &lt;em&gt;every day&lt;/em&gt;, doing live-fire training, and completely confident that they could identify and hit a target in real life. To my knowledge, only the most elite units of the military do this. I grew up in New Jersey, where the only people who carry guns are law enforcement and criminals, period (&lt;span class="caps"&gt;OK&lt;/span&gt;, there are some &lt;em&gt;extremely&lt;/em&gt; rare exceptions). I now live in Georgia, where anyone can walk into WalMart and walk out with a gun - and many people walk around the store with handguns on their belts, something that&amp;#8217;s still foreign to&amp;nbsp;me.&lt;/p&gt;
&lt;p&gt;On my eighteenth birthday, I went to the police department in my home town to begin the process of purchasing the target pistol that my father had offered as a gift. For those of you who aren&amp;#8217;t familiar with how gun (specifically handgun - it&amp;#8217;s significantly easier for shotguns and rifles) ownership works in the northeast, here&amp;#8217;s the gist of it (my memory may be a bit hazy, and some of it may have been explained&amp;nbsp;poorly):&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I went to the police department and met with the chief, whom I already knew personally through&amp;nbsp;family.&lt;/li&gt;
&lt;li&gt;I paid a fee of approximately $100 and filled out about six pages of paperwork, including a full history of my addresses and a lengthy&amp;nbsp;questionnaire.&lt;/li&gt;
&lt;li&gt;I was fingerprinted - all ten fingers, three sets of&amp;nbsp;prints.&lt;/li&gt;
&lt;li&gt;I filled out consent forms for federal and state background checks and consents for mental health records&amp;nbsp;searches.&lt;/li&gt;
&lt;li&gt;These were all sent out to the respective&amp;nbsp;agencies.&lt;/li&gt;
&lt;li&gt;About a month later, the chief called to tell me that he&amp;#8217;d received the replies from the federal and state governments, and they came back &lt;span class="caps"&gt;OK&lt;/span&gt; (which is to be expected, as I had no mental health history and no criminal&amp;nbsp;record).&lt;/li&gt;
&lt;li&gt;He then sent out form letters to the three non-related character references I had specified on my application, requesting them to assert under penalty of perjury whether they believed that I posed any danger to myself or others, if I were allowed to purchase a&amp;nbsp;firearm.&lt;/li&gt;
&lt;li&gt;The letters were all returned by my references. The chief then sent the completed paperwork back to the state government, which issued me a state firearms purchaser identification card, entitling me to purchase firearms and ammunition. I was also issued - as requested - a Permit to Purchase a Handgun, which entitled me to purchase one (1) handgun within 90 days of the date of&amp;nbsp;issue.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span class="caps"&gt;OK&lt;/span&gt;, so that&amp;#8217;s quite a process. All in all, it took two or three months to&amp;nbsp;complete.&lt;/p&gt;
&lt;p&gt;When I finally received my &lt;span class="caps"&gt;ID&lt;/span&gt; card and permit - which had to be picked up in person - I headed out to a local gun store to find the model I wanted. And I did. I turned the purchase permit over to the salesperson, who called the state government to verify its validity. We finalized the details of the transaction and I inspected the merchandise and filled out some last paperwork. The gun I&amp;#8217;d purchased was then marked with a &amp;#8220;sold&amp;#8221; sticker and had a copy of the invoice attached, and was locked in a safe for me to pick up in seven days, when the mandatory waiting period was over. When I returned a week later, one of the two spent cartridges from the gun (shipped with it by the manufacturer) was sent to the state police for future forensic&amp;nbsp;use.&lt;/p&gt;
&lt;p&gt;New Jersey is widely regarded as having some of the strictest gun control laws in the country. And none of that is enough. There was never a follow-up. There was no ongoing monitoring that I&amp;#8217;m aware of. I&amp;#8217;ve heard more stories than I can remember of people being convicted of crimes or involved in domestic violence situations, and then using their legally-purchased and registered firearms in future crimes. It makes no sense to me that we live in a world where Amazon knows what items I&amp;#8217;m interested in today, but the authorities can&amp;#8217;t figure out to take my gun away if I assault someone. There was also never a mandatory proficiency or safety test, either written or at a real firing&amp;nbsp;range.&lt;/p&gt;
&lt;p&gt;How is it that we live in a country where operation of a motor vehicle - a device which &lt;em&gt;can&lt;/em&gt; be fatal when operated improperly - requires a written examination, a practical demonstration of skill, and an ongoing demonstration of responsibility (traffic police), but the possession and operation of a firearm does not? It makes no sense. I don&amp;#8217;t want to minimize the effects of poor driving decisions, they certainly claim many lives, but there&amp;#8217;s no avoiding the fact that firearms were - and in many cases, are - &lt;em&gt;designed&lt;/em&gt; to&amp;nbsp;kill.&lt;/p&gt;
&lt;p&gt;I suppose that most of my feelings on this are based on two of my core beliefs, which run contrary to many gun-rights activists: (1) that my only use for a firearm is target shooting, and it doesn&amp;#8217;t really matter to me how quickly I can buy one, and (2) that our military has tanks and nukes, and the idea of &amp;#8220;defending my Freedom [against a government takeover]&amp;#8221; with any legally-purchased weapon is impossible, regardless of whether or not it&amp;#8217;s simply&amp;nbsp;paranoid.&lt;/p&gt;
&lt;p&gt;Whether openly or not, many gun rights activists base their positions on a romanticized fictional &amp;#8220;past America&amp;#8221;, where everyone was good, owned guns, defended themselves, and was self-sufficient. Even if that past world existed (in reality it was quite violent; the violence just isn&amp;#8217;t remembered in the Utopian narrative), it&amp;#8217;s certainly not today. The past America they speak of was a time when most gun owners had been so since childhood (it was merely a part of living), likely had many fewer violent influences, and could count on swift justice. Let&amp;#8217;s also remember the technology of this past age: I might be &lt;span class="caps"&gt;OK&lt;/span&gt; with gun rights activists going back to this mythical past, if they also could only have a six-shot revolver that takes a few minutes to&amp;nbsp;reload.&lt;/p&gt;
&lt;p&gt;But despite what gun rights activists try to tell us, the problem here is not hardcore criminals. It&amp;#8217;s people who may or may not have criminal or suspicious pasts, who decide to act on their violent ideas. It&amp;#8217;s people who can walk into a store and buy a weapon or magazine that holds twenty or fifty rounds of ammunition - something which has no legitimate use other than mass killing. It&amp;#8217;s people who can borrow or steal guns from family or friends, because those firearms weren&amp;#8217;t treated with the proper respect and safety by their legal owners. It&amp;#8217;s the belief by some that there&amp;#8217;s a legitimate reason to obtain a gun in less than a week, or a month, or months. It&amp;#8217;s the ironic dichotomy of the people who believe that Snowden was a traitor and mass surveillance is fine, but registering guns is an unconstitutional invasion of privacy. And it has to stop. I dare say, if every mass shooting in the past decade or two was carried out by a Muslim or an immigrant or anyone other than a white Christian, half of the roadblocks to a safe America would&amp;#8217;ve&amp;nbsp;disappeared.&lt;/p&gt;
&lt;p&gt;To everyone who takes a literal reading of the Constitution and says that the second amendment grants every American the right to posses and carry as many and whatever kind of firearms they want, I make three&amp;nbsp;comments:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;falsely shouting &amp;#8216;fire&amp;#8217; in a crowded theater.&amp;#8221; We have a Supreme Court for a reason: that the framers knew the Constitution would have to be interpreted differently with&amp;nbsp;time.&lt;/li&gt;
&lt;li&gt;If you stand against gun registration or regulation, please keep in mind that your precious second amendment begins with the phrase, &amp;#8220;A well regulated Militia.&amp;#8221; Even if you don&amp;#8217;t believe that this is equivalent to a modern-day standing army, it clearly does &lt;em&gt;not&lt;/em&gt; say, &amp;#8220;Everybody gets all the firearms they want,&amp;nbsp;period.&amp;#8221;&lt;/li&gt;
&lt;li&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Life, liberty and the pursuit of happiness.&amp;#8221; They weren&amp;#8217;t put in that order by&amp;nbsp;mistake.&lt;/li&gt;
&lt;/ol&gt;</content><category term="politics"></category><category term="guns"></category></entry><entry><title>Terraform Shortcomings - No Interpolated Default Values, No Functions, No Conditionals, Local StateÂ Storage</title><link href="http://blog.jasonantman.com/2016/04/terraform-shortcomings-no-interpolated-default-values-no-functions/" rel="alternate"></link><published>2016-04-19T07:40:00-04:00</published><updated>2016-04-19T07:40:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2016-04-19:/2016/04/terraform-shortcomings-no-interpolated-default-values-no-functions/</id><summary type="html">&lt;p&gt;some complaints about Terraform&amp;#8217;s lack of functions and variable default&amp;nbsp;values&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lately I&amp;#8217;ve been using HashiCorp&amp;#8217;s &lt;a href="https://www.terraform.io/"&gt;Terraform&lt;/a&gt; a lot to manage infrastructure. It certainly has some big things going for it; it supports a whole bunch of providers (including on-prem, non-cloud stuff like VMWare and Docker) as well as some database engines and &lt;span class="caps"&gt;DNS&lt;/span&gt; providers and can even manage GitHub teams, it can plan changes before committing them (which CloudFormation only &lt;a href="https://aws.amazon.com/blogs/aws/new-change-sets-for-aws-cloudformation/"&gt;very recently&lt;/a&gt; learned), and it can store the current state of your infrastructure in &lt;a href="https://www.consul.io/"&gt;Consul&lt;/a&gt;. Also a big step past CloudFormation, it has &lt;a href="https://www.terraform.io/docs/provisioners/index.html"&gt;provisioners&lt;/a&gt; including local execution, remote execution, file copying and Chef (strangely no built-in support for Puppet, but the remote-exec can do that) that can reach out to your newly-created instances and take actions on&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;Terraform also has a &lt;a href="https://www.terraform.io/docs/providers/template/index.html"&gt;template provider&lt;/a&gt; that&amp;#8217;s used any time you need a templated file, such as &lt;span class="caps"&gt;EC2&lt;/span&gt; instance user-data or dynamically generated scripts to place on hosts. Terraform uses a &lt;span class="caps"&gt;DSL&lt;/span&gt; for its &lt;a href="https://www.terraform.io/docs/configuration/index.html"&gt;configuration&lt;/a&gt;, either the &lt;span class="caps"&gt;JSON&lt;/span&gt;-like but slightly-more-human-readable &lt;a href="https://github.com/hashicorp/hcl"&gt;Hashicorp Configuration Language (&lt;span class="caps"&gt;HCL&lt;/span&gt;)&lt;/a&gt; or the same information conveyed in pure &lt;span class="caps"&gt;JSON&lt;/span&gt;. The configuration language supports variables (passed in at the command line or in a file) and is based on &lt;a href="https://www.terraform.io/docs/configuration/interpolation.html"&gt;string interpolation&lt;/a&gt; with a handful of functions defined. It&amp;#8217;s also worth noting that Terraform is written in Go; it has a &lt;a href="https://www.terraform.io/docs/plugins/index.html"&gt;plugin system&lt;/a&gt; but only for Providers and Provisioners; there&amp;#8217;s no way to add core functionality (I suppose I&amp;#8217;ve been spolied by Puppet having such good support for adding core functionality via Ruby, or HashiCorp&amp;#8217;s Vagrant having a config file that itself is&amp;nbsp;Ruby).&lt;/p&gt;
&lt;p&gt;Now that I&amp;#8217;ve been nice and said some great things about Terraform (and it really is; at least for the way my current job is managing infrastructure, I&amp;#8217;ve fallen in love with it, and it certainly does fix some shortcomings that I found in CloudFormation, specifically with pre-execution plans and ability to interact with resources), on to my complaints of the&amp;nbsp;day.&lt;/p&gt;
&lt;h2 id="local-state-storage"&gt;Local State&amp;nbsp;Storage&lt;/h2&gt;
&lt;p&gt;My first complaint is that by default, Terraform stores the state of your infrastrucutre in a file in your current working directory. It uses this to attempt to figure out the already-existing resources you&amp;#8217;ve created, and only make the required changes. The first time I used terraform, I completely destroyed one of our (luckily non-production) services; coworkers of mine have brought down production services because of&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;Let&amp;#8217;s say that we have a Terraform configuration which takes one variable, &lt;code&gt;environment&lt;/code&gt;. That variable determines the &lt;span class="caps"&gt;VPC&lt;/span&gt; and subnets we deploy into, our &lt;span class="caps"&gt;DNS&lt;/span&gt; names, and also gets passed to &lt;span class="caps"&gt;EC2&lt;/span&gt; instances via user-data. We build our infrastructure with &lt;code&gt;environment = "prod"&lt;/code&gt;, and everything works right - we now have a production cluster of our service. Then we want to test some changes, so we run again with &lt;code&gt;environment = "dev"&lt;/code&gt;. The naive - and logical - assumption would be that we get a second &amp;#8220;dev&amp;#8221; cluster of our service. Nope. Terraform finds the &lt;code&gt;terraform.tfstate&lt;/code&gt; file in our current directory, reads it, and takes it to be the current state of our infrastructure. It sees that we &lt;strong&gt;changed&lt;/strong&gt; &lt;code&gt;environment&lt;/code&gt; from &amp;#8220;prod&amp;#8221; to &amp;#8220;dev&amp;#8221;&amp;#8230; so it destroys our &lt;span class="caps"&gt;EC2&lt;/span&gt; instances and &lt;span class="caps"&gt;DNS&lt;/span&gt; record, and creates new ones for &amp;#8220;dev&amp;#8221; (applying the requested&amp;nbsp;changes).&lt;/p&gt;
&lt;p&gt;This teaches us two important&amp;nbsp;points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Always&lt;/strong&gt; run &lt;code&gt;terraform plan&lt;/code&gt;. Even if you think your changes are trivial, examine what Terraform will do before running &lt;code&gt;apply&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Always&lt;/strong&gt; run &lt;code&gt;terraform&lt;/code&gt; through a wrapper. We have a simple Rake task in an internal rubygem that ensures that Terraform will always store state in Consul, so it won&amp;#8217;t be locked to one person&amp;#8217;s local machine, and also removes any local state files before running so they won&amp;#8217;t pollute the run or result in changes intended for one isolated instance of our Terraform configuration from being applied to&amp;nbsp;another.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="functions"&gt;Functions&lt;/h2&gt;
&lt;p&gt;Terraform&amp;#8217;s &lt;a href="https://www.terraform.io/docs/configuration/interpolation.html"&gt;configuration interpolation&lt;/a&gt; has a bunch of built-in functions for working with variables. They&amp;#8217;re a subset of what you&amp;#8217;d expect in a language that is mainly based around strings, arrays and maps/hashes: split, join, concat, lookup (get a hash item by key), index (find the index of an item in a list), element (return the n&amp;#8217;th element of a list), format (sprintf-like), etc. However, there&amp;#8217;s no function to retrieve only unique elements from a list. This becomes a problem especially when dealing with multi-&lt;span class="caps"&gt;AZ&lt;/span&gt;/multi-subnet &lt;span class="caps"&gt;AWS&lt;/span&gt; resources, as some of them (e.g. managing a set number individual &lt;span class="caps"&gt;EC2&lt;/span&gt; instances outside of an &lt;span class="caps"&gt;ASG&lt;/span&gt;, such as when assigning static IPs) require a list of subnets matching the number of resources, and others (cross-&lt;span class="caps"&gt;AZ&lt;/span&gt; ELBs) require a list of unique&amp;nbsp;subnets.&lt;/p&gt;
&lt;p&gt;Terraform and its language have no way to add this functionality (&lt;em&gt;see note below&lt;/em&gt;); the only option that I&amp;#8217;ve found is to wrap Terraform in some sort of runner (I use &lt;a href="https://github.com/ruby/rake"&gt;Rake&lt;/a&gt; but you could use any scripting or Make-like language) that does whatever manipulation and calculation is needed, and passes in the necessary values distinct variable values (i.e. the full subnet list, and the unique subnet list, as separate variables). To make this even more difficult, though Terraform supports loading built-time variables from a &lt;span class="caps"&gt;JSON&lt;/span&gt; or &lt;span class="caps"&gt;HCL&lt;/span&gt; file instead of the command line, it only supports taking in variables as strings (even in &lt;span class="caps"&gt;JSON&lt;/span&gt;). So in our subnet example, our wrapper script needs to join the list of subnets into a string (i.e. &lt;span class="caps"&gt;CSV&lt;/span&gt;) and then whenever we use the variable in Terraform, we need to &lt;code&gt;split()&lt;/code&gt; it on our separator character (because Terraform doesn&amp;#8217;t support variable setting or&amp;nbsp;manipulation).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Terraform [has] no way to add this functionality&amp;#8221;&lt;/em&gt; - I&amp;#8217;m aware that I could fork Terraform, learn Go, and submit pull requests for all of the features I think would be useful; and if I had maybe half a dozen less unfinished projects, I&amp;#8217;d probably do that. However, this still means that HashiCorp would need to accept and merge my PRs and release a new version, or else I&amp;#8217;d need to build and distribute my forked version. Terraform supports &lt;a href="https://www.terraform.io/docs/plugins/index.html"&gt;plugins&lt;/a&gt;, but only for Providers and Provisioners, not language internals. What I&amp;#8217;d really like is a way to define plugin functions that could be distributed without having to rebuild all of&amp;nbsp;Terraform.&lt;/p&gt;
&lt;h2 id="no-interpolated-default-variable-values"&gt;No Interpolated Default Variable&amp;nbsp;Values&lt;/h2&gt;
&lt;p&gt;Terraform variables can have default values defined for them. However, these default values have no way of using other variables. This means that even for relatively common use cases - like a service that has a name and a &lt;span class="caps"&gt;DNS&lt;/span&gt; record, both of which can be overridden but with the &lt;span class="caps"&gt;DNS&lt;/span&gt; record defaulting to &amp;#8220;SERVICE_NAME.example.com&amp;#8221;, you can&amp;#8217;t do that. The only options that I&amp;#8217;ve been able to figure out are to either do it in your wrapper script (which means the Terraform configs can&amp;#8217;t be run without the wrapper) or use the &lt;code&gt;coalesce&lt;/code&gt; function to give your variable an empty default value, and then choose a second interpolated string if the variable is&amp;nbsp;empty.&lt;/p&gt;
&lt;h2 id="no-conditionals"&gt;No&amp;nbsp;Conditionals&lt;/h2&gt;
&lt;p&gt;Terraform&amp;#8217;s configuration language also lacks conditional statements such as &lt;code&gt;if&lt;/code&gt;. This poses a problem with all but the simplest applications, and is certainly likely to be an issue for anyone who wants to do the right thing and use the same tooling to deploy multiple environments. It seems that the only options are to either pass in the necessary information as variables from a wrapper script, or generate Terraform configurations with other tooling. The former works only if the desired result is a variable in your configuration; there&amp;#8217;s simply no way that I&amp;#8217;ve found to have a conditional around resource(s). The only obvious option for that is to take advantage of Terraform&amp;#8217;s ability to read configurations as &lt;span class="caps"&gt;JSON&lt;/span&gt;, and simply generate your entire terraform configuration with another&amp;nbsp;tool.&lt;/p&gt;</content><category term="terraform"></category><category term="hashicorp"></category><category term="AWS"></category><category term="go"></category></entry><entry><title>Raspberry Pi SecurityÂ System</title><link href="http://blog.jasonantman.com/2016/01/raspberry-pi-security-system/" rel="alternate"></link><published>2016-01-16T10:00:00-05:00</published><updated>2016-01-16T10:00:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2016-01-16:/2016/01/raspberry-pi-security-system/</id><summary type="html">&lt;p&gt;A Raspberry Pi and webcam security&amp;nbsp;system.&lt;/p&gt;</summary><content type="html">&lt;p&gt;It seems that crime is on the rise in the area where I live, and in my &amp;#8220;gated&amp;#8221; (when they actually close)
apartment complex. I&amp;#8217;m going out of town for a while to visit family, and was a bit wary of leaving my
apartment - and all of my posessions, and most importantly my cats, unattended for too long. I&amp;#8217;m having
some family in the area check on the cats every few days, but that doesn&amp;#8217;t do a lot for my peace of mind
in a complex that&amp;#8217;s had a few break-ins this&amp;nbsp;year.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve played around on previous trips with with &lt;a href="http://www.lavrsen.dk/foswiki/bin/view/Motion/WebHome"&gt;motion&lt;/a&gt;, a motion-activated video recording tool,
and a &lt;a href="http://www.amazon.com/Logitech-960-000585-HD-Webcam-C310/dp/B003LVZO8S/ref=sr_1_1?ie=UTF8&amp;amp;qid=1450663461&amp;amp;sr=8-1&amp;amp;keywords=logitech+c310"&gt;Logitech C310 webcam&lt;/a&gt;,
but with four cats, it&amp;#8217;s far from a tool to detect a human in my apartment. So, the weekend before my trip,
I decided to do some&amp;nbsp;tinkering.&lt;/p&gt;
&lt;p&gt;There&amp;#8217;s an &amp;#8220;alarm system&amp;#8221; control panel next to the entry to my apartment, but it appears to be a no-name system that probably
cost $20 and doesn&amp;#8217;t actually do anything other than sound a chime when the door opens. I turned it off the day I moved in,
and hadn&amp;#8217;t given it a second thought since. However, it occurred to me that the useless panel next to the washing machine
probably had magnetic contact switches for the doors. Sure enough, after a few minutes with a multimeter, I found that both
the entry door and the sliding balcony door have normally-closed magnetic contacts wired back to the panel. After thinking
over the options for a few minutes, I remembered that I had a &lt;a href="https://www.raspberrypi.org/"&gt;Raspberry Pi&lt;/a&gt; (the original)
sitting unused under my &lt;span class="caps"&gt;TV&lt;/span&gt;, and a &lt;a href="https://www.sparkfun.com/products/11772"&gt;PiFace I/O card&lt;/a&gt; that I&amp;#8217;d never&amp;nbsp;used.&lt;/p&gt;
&lt;p&gt;After about an hour of connecting some wires and playing around with the wonderfully-simple &lt;a href="http://piface.github.io/pifacedigitalio/"&gt;pifacedigitalio&lt;/a&gt;
Python package &lt;a href="https://pypi.python.org/pypi/pifacedigitalio/3.0.5"&gt;available on PyPi&lt;/a&gt;, I was able to successfully read
inputs for when either door was open. I figured that this would provide the perfect squelch for motion recording from the
webcam, as the cats aren&amp;#8217;t able to operate the deadbolt on my front door (I had to replace all of the interior door handles
with cat-proof&amp;nbsp;models).&lt;/p&gt;
&lt;p&gt;&lt;a href="/GFX/rpi_alarm_1_large.jpg"&gt;&lt;img alt="Photograph of RPi in alarm enclosure" src="/GFX/rpi_alarm_1_sm.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="/GFX/rpi_alarm_2_large.jpg"&gt;&lt;img alt="Photograph of alarm enclosure closed, showing wires to RPi" src="/GFX/rpi_alarm_2_sm.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The system that I&amp;#8217;ve come up with is rather rough around the edges&amp;#8230; to put it lightly. It&amp;#8217;s pretty obvious that it was written
in a few days, and at this point, it&amp;#8217;s not really intended to be used by anyone who doesn&amp;#8217;t have a good understanding of the
components (and Python). But I&amp;#8217;m hoping that someone else might find it interesting, or perhaps improve on it. It&amp;#8217;s not terribly
robust, but it seems to be working acceptably well for my&amp;nbsp;needs.&lt;/p&gt;
&lt;h2 id="components"&gt;Components&lt;/h2&gt;
&lt;p&gt;The system is split into a number of components, with some of them running on the Raspberry Pi and some on my desktop&amp;nbsp;computer.&lt;/p&gt;
&lt;p&gt;The Pi is running my &lt;a href="https://github.com/jantman/piface-webhooks"&gt;piface-webhooks&lt;/a&gt; project (everything needed to set it up on
&lt;a href="https://www.raspbian.org/"&gt;Raspbian&lt;/a&gt; or &lt;a href="https://osmc.tv/"&gt;&lt;span class="caps"&gt;OSMC&lt;/span&gt;&lt;/a&gt; is available in the repo), which is made up of two Python&amp;nbsp;services:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;piface-listener&lt;/strong&gt; Is the code that actually polls the PiFace inputs. When the state of an input changes, it writes out
a file (under &lt;code&gt;/var/spool/piface-webhooks&lt;/code&gt; by default) with the input number, state, and&amp;nbsp;timestamp.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;piface-worker&lt;/strong&gt; Polls this directory for files; when one is found, it takes some action and then removes the file. The
current actions are sending an &lt;span class="caps"&gt;HTTP&lt;/span&gt; webhook, sending a message via &lt;a href="https://pushover.net/"&gt;Pushover&lt;/a&gt;, and sending an email
via Gmail. I currently use all of these, mainly for redudnancy. The webhook feature is used to &lt;span class="caps"&gt;POST&lt;/span&gt; data to &lt;code&gt;motion_piface_handler.py&lt;/code&gt;,
a Flask app running on my&amp;nbsp;desktop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My desktop computer is the heart of the system, handling the webcam and most of the &amp;#8220;alarm&amp;#8221;&amp;nbsp;logic:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.lavrsen.dk/foswiki/bin/view/Motion/WebHome"&gt;motion&lt;/a&gt; monitors the webcam feed for motion above a certain number of
pixels. When motion is detected, it saves both &lt;span class="caps"&gt;JPEG&lt;/span&gt; images and &lt;span class="caps"&gt;AVI&lt;/span&gt; files to disk, logs the event in a MySQL database, and
executes a Python script. It also saves a snapshot from the webcam every 30&amp;nbsp;seconds.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/jantman/misc-scripts/blob/master/s3sync_inotify.py"&gt;s3sync_inotify.py&lt;/a&gt; is a quick Python script I wrote that
uses Linux inotify to monitor &lt;code&gt;motion&lt;/code&gt;&amp;#8216;s output directory for new files (only when they&amp;#8217;ve been closed, and are finished being
written) and syncs them to an S3 bucket set up for static website hosting. It also generates an &lt;code&gt;index.html&lt;/code&gt; file for the bucket,
with links to all uploaded files. At startup, any files that aren&amp;#8217;t yet synced are uploaded, so it &lt;em&gt;should&lt;/em&gt; handle crashes relatively&amp;nbsp;well.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;handle_motion.py&lt;/strong&gt; is the command executed by &lt;code&gt;motion&lt;/code&gt; when an event is detected; it POSTs data to &lt;code&gt;motion_piface_handler.py&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;motion_piface_handler.py&lt;/strong&gt; is the heart of the system, explained&amp;nbsp;below.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="motion_piface_handlerpy-pulling-it-all-together"&gt;motion_piface_handler.py - Pulling it all&amp;nbsp;together&lt;/h2&gt;
&lt;p&gt;Webhooks from both the Raspberry Pi door sensor and &lt;code&gt;motion&lt;/code&gt;&amp;#8216;s command execution go to a Python &lt;a href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt; app
running on my desktop. This app accepts the incoming data, and also connects to the MySQL database used by Motion. When a &lt;span class="caps"&gt;POST&lt;/span&gt; from
&lt;code&gt;piface-worker&lt;/code&gt; comes in showing that a door has been opened, it adds a record to the MySQL database with information on the input
pin (which door) and state (open/closed), and&amp;nbsp;timestamp.&lt;/p&gt;
&lt;p&gt;When a &lt;span class="caps"&gt;POST&lt;/span&gt; comes in from &lt;code&gt;handle_motion.py&lt;/code&gt;, the command executed by &lt;code&gt;motion&lt;/code&gt; when a file is written, the app checks to see if
a door has been opened in the past few minutes. If not, the event is ignored (and logged, of course). However, if a door has been
opened, the real fun starts. First, the database is queried for the last time a notification was sent out. If one has been sent
in the past few minutes, the current event is ignored (and a rate-limiting message is logged). If it hasn&amp;#8217;t sent out a message
recently, the database is queried for the last door event (which door, and if it was opened or closed) as well as the last &lt;span class="caps"&gt;AVI&lt;/span&gt;
and last five JPEGs saved by &lt;code&gt;motion&lt;/code&gt;. This information is all formatted into a message and sent to my GMail account, and a
shortened version (with just the door event information, and that motion was detected) is sent to my phone via Pushover, with
the highest priority and a custom notification&amp;nbsp;sound.&lt;/p&gt;
&lt;p&gt;So far - at least as far as taking my dogs out is concerned - it appears to be working relatively well. There&amp;#8217;s a bit of
latency in the S3 uploads, especially when AVIs are written, so the files linked in the notification emails may not be
uploaded before the message goes out. That&amp;#8217;s a bit annying, but something that I think I can live&amp;nbsp;with.&lt;/p&gt;
&lt;p&gt;The use of disk queueing probably isn&amp;#8217;t the best, especially with the Pi&amp;#8217;s &lt;span class="caps"&gt;SD&lt;/span&gt; card, but I wanted something that was simple
and didn&amp;#8217;t introduce any additional service dependencies. Each of the components runs as a systemd service, configured to
always restart, so it should tolerate internal failures relatively well. The Python code has a &lt;em&gt;lot&lt;/em&gt; of bare excepts;
I&amp;#8217;m not sure this was the right way to approach it, but my initial theory was that in the event of an error, I&amp;#8217;m more
concerned about keeping the system running than getting an individual message through. The point of the system is to let
me know if my home - and more importantly, my four-legged children - are in danger. I figured that I&amp;#8217;d rather get a delayed
notification than none at&amp;nbsp;all.&lt;/p&gt;
&lt;h2 id="results"&gt;Results&lt;/h2&gt;
&lt;p&gt;After two weeks away, the system worked quite well. It triggered correctly, and quickly, when my family came to check on the cats.
On average, it took about 3-5 seconds for me to receive the PushOver and GMail notifications for a door open event, and about 30 seconds
for an alarm (motion after door state change)&amp;nbsp;event.&lt;/p&gt;
&lt;p&gt;However, I did have a few&amp;nbsp;issues:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Late one night, I got a door open alert when I hadn&amp;#8217;t been expecting anyone. After about half an hour of panic checking the webcam feed
and watching the logs remotely, I determined that it was a false positive. All was well, there wasn&amp;#8217;t any sign of anyone in the apartment,
the cats were all wandering (or lounging) around as normal, and the door never registered as closed. A day or two later, the door registered
as closing. I&amp;#8217;m not sure if this was an issue with the door sensor triggering because of wind or vibration, or an issue with the PiFace itself
having internal issues reading an input over such a long time, or something with induced current in the long unshielded sensor wire in the wall
(and possibly compounded by my naive debounce&amp;nbsp;logic).&lt;/li&gt;
&lt;li&gt;Having &lt;code&gt;motion&lt;/code&gt; store everything in one directory, and then &lt;code&gt;s3sync_inotify.py&lt;/code&gt; sync that to S3 and create an &lt;code&gt;index.html&lt;/code&gt; file was a
bad idea. &lt;code&gt;motion&lt;/code&gt; was triggered quite often by the cats; after about a week away, I had ~&lt;span class="caps"&gt;10GB&lt;/span&gt; of photos and videos in the S3 bucket, and the
&lt;code&gt;index.html&lt;/code&gt; file was over &lt;span class="caps"&gt;7MB&lt;/span&gt;. Not only did the index page take a painfully long amount of time to load, but generation of it introduced enough
latency in the upload process that &lt;code&gt;s3sync_inotify.py&lt;/code&gt; ended up missing a large number of&amp;nbsp;files.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="next-steps"&gt;Next&amp;nbsp;Steps&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;m not sure if I&amp;#8217;ll do much more work on this - we don&amp;#8217;t travel often - but if I do, the next things that I want to tackle&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Queueing of outgoing messages, so that network outages won&amp;#8217;t result in completely-lost&amp;nbsp;communication.&lt;/li&gt;
&lt;li&gt;Some sort of heartbeat - ideally to an off-premesis system, such as my &lt;span class="caps"&gt;EC2&lt;/span&gt; instance - from every process involved, to
confirm that all of the components (a) are running correctly, and (b) have&amp;nbsp;connectivity.&lt;/li&gt;
&lt;li&gt;Modify the &lt;code&gt;motion&lt;/code&gt; output directory structure and &lt;code&gt;s3sync_inotify.py&lt;/code&gt; to write into per-day (or per-hour) directories
and write &lt;code&gt;index.html&lt;/code&gt; files for each of&amp;nbsp;them.&lt;/li&gt;
&lt;li&gt;See if there&amp;#8217;s a straightforward way to use systemd&amp;#8217;s &lt;a href="http://www.freedesktop.org/software/systemd/man/sd_notify.html"&gt;sd_notify&lt;/a&gt;
from Python, to build a watchdog into the processes and have systemd restart them if they&amp;nbsp;hang.&lt;/li&gt;
&lt;li&gt;Packaging this all together into one or more real repositories, so maybe it can be used by&amp;nbsp;others.&lt;/li&gt;
&lt;li&gt;Cleaning up &lt;code&gt;handle_motion.py&lt;/code&gt; and &lt;code&gt;motion_piface_handler.py&lt;/code&gt; and releasing them along with everything&amp;nbsp;else.&lt;/li&gt;
&lt;/ul&gt;</content><category term="rpi"></category><category term="pi"></category><category term="raspberrypi"></category><category term="security"></category><category term="alarm"></category><category term="motion"></category><category term="camera"></category></entry><entry><title>A Rant on Post-Secondary TechÂ Education</title><link href="http://blog.jasonantman.com/2015/10/a-rant-on-post-secondary-tech-education/" rel="alternate"></link><published>2015-10-05T16:46:00-04:00</published><updated>2015-10-05T16:46:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-10-05:/2015/10/a-rant-on-post-secondary-tech-education/</id><summary type="html">&lt;p&gt;A rant on the state of post-secondary technology&amp;nbsp;education.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;em&gt;(Disclaimer: First, I know there&amp;#8217;s a wide range of curricula in the &lt;span class="caps"&gt;CS&lt;/span&gt;/tech education world,
and some schools are better than others; there are certainly programs that prepare students
much better than what I experienced. Second, these views are somewhat biased to my own
experience in the web, agile and DevOps worlds; for people who want to write Java for banks
their whole career, I&amp;#8217;m sure most schools prepare them&amp;nbsp;well.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s been a decade since I&amp;#8217;ve been in the University system, but I&amp;#8217;ve seen precious little to
indicate that much has changed since then. I graduated with a degree in &lt;a href="https://comminfo.rutgers.edu/component/cur,547/option,com_courses/sch,04/task,listing/"&gt;Information Technology
and Informatics (&lt;span class="caps"&gt;ITI&lt;/span&gt;)&lt;/a&gt;;
I switched majors partly because of my deep hatred for calculus, and partly
because I was already working for Rutgers University as a student systems programmer, and it
was painfully obvious how little the &lt;span class="caps"&gt;CS&lt;/span&gt; program would do to prepare me for a career. My time
in &lt;span class="caps"&gt;CS&lt;/span&gt; was spent writing pitifully small Java applications in teams as large as &lt;strong&gt;three&lt;/strong&gt; (any
team work was highly unusual) and playing with linked lists. I switched to &lt;span class="caps"&gt;ITI&lt;/span&gt; so I could take
classes on &lt;span class="caps"&gt;IT&lt;/span&gt; management and policy, information security and &amp;#8220;web application development&amp;#8221; (with
databases and &lt;span class="caps"&gt;JSON&lt;/span&gt;!) Since then, I&amp;#8217;ve had two family members enter &lt;span class="caps"&gt;CS&lt;/span&gt; or &lt;span class="caps"&gt;ECE&lt;/span&gt; programs, and I
tried to give them the best advice I could. I happened to be thinking about it, and figured
I&amp;#8217;d write down some of my&amp;nbsp;thoughts.&lt;/p&gt;
&lt;p&gt;First and foremost, I understand that technology changes very quickly - a lot quicker than
college syllabi. However, my alma mater&amp;#8217;s &lt;a href="https://www.cs.rutgers.edu/undergraduate/courses/"&gt;undergrad course list&lt;/a&gt;
does not appear to have changed &lt;em&gt;at all&lt;/em&gt; since I took some of those classes a decade ago.
Don&amp;#8217;t get me wrong, it&amp;#8217;s very good that we&amp;#8217;re teaching classes in Operating Systems Design
and Compilers. But current curricula seem to focus almost exclusively on fundamentals
and low-level details. When I was in college, almost all the classes were taught in Java,
and that already seemed dated and ignoring the &amp;#8220;Web 2.0&amp;#8221; world; these days, it relegates
most graduates to jobs that I&amp;#8217;d consider boring, in the Enterprise sector. Rutgers&amp;#8217; undergrad
&lt;span class="caps"&gt;CS&lt;/span&gt; catalog doesn&amp;#8217;t list &amp;#8220;Internet Technology&amp;#8221; until &lt;span class="caps"&gt;CS&lt;/span&gt; 352, and I&amp;#8217;d be surprised if any
networking (aside from maybe &amp;#8220;use raw sockets to do something&amp;#8221;) is covered before that class,
which is tentatively designed for third-year students. Very few of the courses that I see
listed (and many that I looked at haven&amp;#8217;t changed their description, or even their instructor,
in ten years) do much to prepare students for the actual tech industry. Even worse, most
of them are the same classes that I found boring, and caused me to switch&amp;nbsp;majors.&lt;/p&gt;
&lt;p&gt;Obviously, I can understand the argument that tech moves too fast for class materials
to keep pace. But introducing the Internet as a third-year topic, and Distributed
Systems as a senior-level class? Writing &lt;em&gt;everything&lt;/em&gt; in Java? This seemed silly
to me a decade ago; now, it seems downright wrong for a department that claims to
prepare students for careers in technology. Once again, I agree that a strong grasp
of fundamentals is overwhelmingly important. However, a balance needs to be struck
between this and (1) providing students with useful, current skills, and (2) keeping
students interested. In a world where more and more (and in many areas, such as my own
subculture, almost all) software runs in the browser, how can education ignore&amp;nbsp;this?&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll take a small moment to dovetail this to a heated topic in tech at the moment:
hiring practices and diversity. The tech education at most universities does little
to prepare students for actual jobs outside of, quite literally, &amp;#8220;entry-level Java
programmer&amp;#8221;. The people who will get good, interesting jobs in tech (yes, that&amp;#8217;s quite
opinionated; I&amp;#8217;m defining that as jobs with web or DevOps shops) are the ones who
either have ample free time and pre-existing interest to hack on their own projects,
and/or are insomniacs and can handle going to class, working, and still writing
a lot of code and experimenting on their own. I suppose this ends up being biased
towards stereotypical white males, even if only because it seems socially acceptable
for us to ignore having a social life in favor of finishing those last lines of&amp;nbsp;code.&lt;/p&gt;
&lt;p&gt;But I digress. There are few other industries that I can think of - and certainly
no professions - where someone leaves the University system with the highest
degree commonly attained in their field, yet has virtually zero real world
hands-on experience. I&amp;#8217;ll leave out the doctor or lawyer analogies, but some
of our closest parallels - engineers, architects, etc. - graduate and are able
to start practicing their profession. Sure, there&amp;#8217;s organization- or domain-specific
on the job training, but for the most part, they can do what they were hired to do.
On the other hand, tech-focused programs are turning out graduates many of whom
have never seen the tools (or even languages) that we use. They don&amp;#8217;t have
any real experience working in teams larger than three or four (at the best),
and have no concept of what goes into developing real software, working in
a large (or distributed) team, or what happens to software after someone
grades it (which in my experience, was usually as simple as &amp;#8220;does this program
produce the right output when it&amp;#8217;s run at the command&amp;nbsp;line).&lt;/p&gt;
&lt;p&gt;So, that&amp;#8217;s my rant. What do I think should be covered in tech/&lt;span class="caps"&gt;CS&lt;/span&gt; curricula that isn&amp;#8217;t?
Here are a&amp;nbsp;few:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt; - I have painful memories of being marked as failing assignments because
I used a tab when the instructors expected some number of spaces in the output of my
program. In most cases, my instructors used a test harness that exec&amp;#8217;ed our Java applications
and inspected the string output. I still have no idea why they didn&amp;#8217;t use JUnit. But that&amp;#8217;s
beyond the point; we&amp;#8217;re turning out programmers who don&amp;#8217;t know what unit or integration tests
are. I still don&amp;#8217;t understand how I got through a four-year degree, partially in &lt;span class="caps"&gt;CS&lt;/span&gt; and
partially in &lt;span class="caps"&gt;IT&lt;/span&gt;, without writing a single test for any program I wrote (aside from a few
courses where we wrote &amp;#8220;test harnesses&amp;#8221;, but never used a proper testing&amp;nbsp;framework).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Software Distribution&lt;/strong&gt; - Here&amp;#8217;s another no-brainer. Why - especially when working in
Java - would students email completed assignments to professors, or upload them to online
courseware, when so many artifact repositories exist? If people can&amp;#8217;t use your software it&amp;#8217;s
pretty pointless. Distribution should be a part of at least some assignments, whether it&amp;#8217;s
an open source model or just uploading an artifact to an internal maven&amp;nbsp;repository.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Maintenance&lt;/strong&gt; - Ok, sure, this is the part that none of us really like. But it&amp;#8217;s also
an inherent part of what we do, and the odds are most entry-level programmers will first
find themselves fixing bugs or adding features to someone else&amp;#8217;s application. Being able
to read and understand existing code is perhaps the most important thing we do, whether it&amp;#8217;s
to fix it or just to learn from it. Nobody I&amp;#8217;ve talked to about education as a programmer
has ever encountered an assignment of &amp;#8220;here&amp;#8217;s an application, here&amp;#8217;s a bug report, find and
fix it&amp;#8221; beyond the most trivial contrived examples. The process of fixing a bug in or adding
a feature to an existing codebase is probably one of the most important lessons in learning
to write code - even if it&amp;#8217;s partially a lesson in what not to&amp;nbsp;do.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distributed Projects&lt;/strong&gt; - &lt;em&gt;The&lt;/em&gt; &amp;#8220;big&amp;#8221; project in my time as a student was pairing on a Java
&lt;span class="caps"&gt;GUI&lt;/span&gt;/backend program. Perhaps I didn&amp;#8217;t have the best experience, as my partner neglected to write
any code. However, few people are going to enter the workforce and be the sole person touching
a given codebase. I think there should be much more emphasis on working as part of a realistically-sized
team. Sure, the tooling might not be the same, but at least graduates should have some experience
in collaborating with others, and more importantly, in working on code where they don&amp;#8217;t necessarily
understand all of it. If we&amp;#8217;re going to teach Java, we should at least be teaching it realistically
and throwing in some black-box classes or having students code to each others&amp;#8217; (not-yet-complete)&amp;nbsp;APIs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Web-First&lt;/strong&gt; - With the plummeting cost of cloud computing and containers, and the massive
compute farms available at most universities, there&amp;#8217;s really no excuse for completely ignoring
the Internet. Sure, it doesn&amp;#8217;t play much of a role in the low-level basics, but for the classic
&amp;#8220;hello world&amp;#8221;, calculator app, tic-tack-toe, etc. it&amp;#8217;s not really that much overhead to do
them in &lt;a href="http://spring.io/guides/gs/rest-service/"&gt;Spring&lt;/a&gt; or &lt;a href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt;
or &lt;a href="http://guides.rubyonrails.org/getting_started.html"&gt;Rails&lt;/a&gt; and also give students exposure
to a modern, web-centric framework that someone might actually use to write a simple application.
Most programmers are probably going to touch the web at some point. I&amp;#8217;d argue that it&amp;#8217;s also a
lot more applicable for people who aren&amp;#8217;t going into a distinctly programming&amp;nbsp;role.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Operable Software&lt;/strong&gt; - Going with the web-first paradigm, we have the virtualization or
container technology to give students a shell and a running web server. Why not show them
how to use it? I once failed a major assignment because the graders used a script that combined
&lt;span class="caps"&gt;STDOUT&lt;/span&gt; and &lt;span class="caps"&gt;STDERR&lt;/span&gt; and evaluated it. They asked me what all these weird lines in my output
were; I told them it was log4j. They asked what that was. Even the classes I took that
included working in teams or something else somewhat realistic, completely ignored the
operations side of software, as far as never discussing logging. If we want the quality
of software that we (as an industry) turn out to increase, one of the best things we can
do is introduce programmers to logging, testing, debugging and the operations side as early
as possible, even if in a quick, superficial way. If students had to actually &lt;em&gt;run&lt;/em&gt; their
app, and let it serve actual requests, they&amp;#8217;d have a lot clearer picture of what software
actually does after the build a &lt;span class="caps"&gt;JAR&lt;/span&gt;. Related to this, an introduction to the concepts
of security and stability would be quite&amp;nbsp;useful.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you&amp;#8217;re currently going to school for something programming-related, here are a few
things that I&amp;#8217;d recommend doing to get ahead of the&amp;nbsp;pack:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn some languages. Look through job ads for the type of entry-level/graduate work you
hope to get when you graduate, and see what they&amp;#8217;re asking for. If your school is still the
way mine appears to be, in a &lt;span class="caps"&gt;CS&lt;/span&gt; program you&amp;#8217;ll probably be exposed to Java and C. Learn some
Ruby or Python, or something else that&amp;#8217;s in use on the web. It certainly won&amp;#8217;t hurt you, and
it&amp;#8217;ll also be less intimidating to learn a new language once you already know a few, preferably
that are rather&amp;nbsp;different.&lt;/li&gt;
&lt;li&gt;If you want to work in the web world and are a Windows person, learn Linux. Despite what
some people tell you, it&amp;#8217;s the rule not the&amp;nbsp;exception.&lt;/li&gt;
&lt;li&gt;Read. I understand that not everyone has extensive time to experiment on their own, and I know
a lot of people who didn&amp;#8217;t, especially in college. Read. A lot. It seems that most tech programs
require a lot less work than other engineering disciplines; I remember how envious my &lt;span class="caps"&gt;ECE&lt;/span&gt; and Mech-E
friends were at the &amp;#8220;low&amp;#8221; amount of work we &lt;span class="caps"&gt;CS&lt;/span&gt;/&lt;span class="caps"&gt;IT&lt;/span&gt; majors had to do. Read everything you can, especially
about the industry you want to work in (if you have an idea of what it is). Find out what tools they&amp;#8217;re
using from job ads or company tech blogs, and find out about them. Even if you can&amp;#8217;t use them
yourself, at least knowing a bit about them will help a&amp;nbsp;lot.&lt;/li&gt;
&lt;li&gt;If you can, find an open source project or two to work on. This one comes with a bit of a warning;
the open source world can be quite abrasive, and sometimes downright hurtful. Unfortunately, technology
as a whole seems to attract a lot of very loud, angry, bad people. So do some research; try to find
a project that you&amp;#8217;re interested in and that you have some relevant experience for. But most importantly,
find a project that&amp;#8217;s clearly open to mentoring new contributors; they&amp;#8217;re unfortunately few and
far between, but it will really pay&amp;nbsp;off.&lt;/li&gt;
&lt;/ul&gt;</content><category term="college"></category><category term="computer science"></category><category term="CS"></category><category term="technology"></category><category term="education"></category></entry><entry><title>Puppetlabs Beaker SUTs with GUI /Â Non-Headless</title><link href="http://blog.jasonantman.com/2015/09/puppetlabs-beaker-suts-with-gui--non-headless/" rel="alternate"></link><published>2015-09-19T11:09:00-04:00</published><updated>2015-09-19T11:09:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-09-19:/2015/09/puppetlabs-beaker-suts-with-gui--non-headless/</id><summary type="html">&lt;p&gt;How to enable the &lt;span class="caps"&gt;GUI&lt;/span&gt; / disable headless mode on a puppetlabs Beaker&amp;nbsp;&lt;span class="caps"&gt;SUT&lt;/span&gt;.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://github.com/puppetlabs/beaker/"&gt;Beaker&lt;/a&gt; is a puppetlabs tool for automating acceptance testing
of puppet modules; in most common use cases, it uses a &lt;a href="https://www.vagrantup.com/"&gt;Vagrant&lt;/a&gt;/
&lt;a href="https://www.virtualbox.org/"&gt;virtualbox&lt;/a&gt; &lt;span class="caps"&gt;VM&lt;/span&gt; to run the&amp;nbsp;tests.&lt;/p&gt;
&lt;p&gt;This week, I was writing tests for a &lt;a href="https://github.com/jantman/puppet-archlinux-workstation"&gt;module&lt;/a&gt;
that configures my desktop and laptop, including installing and setting up Xorg and &lt;span class="caps"&gt;KDE&lt;/span&gt; and the
&lt;a href="https://github.com/sddm"&gt;&lt;span class="caps"&gt;SDDM&lt;/span&gt;&lt;/a&gt; display manager. I wanted to be able to test that they not only
got installed, but actually ran without dieing - which required a graphincal environment (ideally,
I&amp;#8217;d visually confirm this as&amp;nbsp;well).&lt;/p&gt;
&lt;p&gt;To do this in Vagrant, you&amp;#8217;d just add a &lt;code&gt;gui = true&lt;/code&gt; option to the
&lt;a href="https://docs.vagrantup.com/v2/virtualbox/configuration.html"&gt;virtualbox provider&lt;/a&gt; in your&amp;nbsp;Vagrantfile.&lt;/p&gt;
&lt;p&gt;It isn&amp;#8217;t documented anywhere, but I &lt;a href="https://github.com/jantman/puppet-archlinux-workstation/commit/6ca19a24853681c468eba38735c8d2d7f54cd616"&gt;found&lt;/a&gt;
that Beaker has support for this as well; all you need to do is add &lt;code&gt;vb_gui: true&lt;/code&gt; in your node definition&amp;nbsp;&lt;span class="caps"&gt;YAML&lt;/span&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gd"&gt;--- before.yaml 2015-09-19 11:20:47.772523116 -0400&lt;/span&gt;
&lt;span class="gi"&gt;+++ after.yaml  2015-09-19 11:20:20.768867546 -0400&lt;/span&gt;
&lt;span class="gu"&gt;@@ -1,11 +1,12 @@&lt;/span&gt;
 &lt;span class="caps"&gt;HOSTS&lt;/span&gt;:
   arch-x64:
     roles:
       - master
     platform: archlinux-2015.09.01-amd64
     box: jantman/packer-arch-workstation
     hypervisor: vagrant
&lt;span class="gi"&gt;+    vb_gui: true&lt;/span&gt;

 &lt;span class="caps"&gt;CONFIG&lt;/span&gt;:
   log_level: verbose
   type: foss
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once that&amp;#8217;s done, the VirtualBox &lt;span class="caps"&gt;VM&lt;/span&gt; will run with a graphical display enabled. This is probably only useful on a local
machine or if you&amp;#8217;re running on a remote host have you have access to and have &lt;a href="https://www.virtualbox.org/manual/ch07.html"&gt;vrdp&lt;/a&gt;
enabled, but in some edge cases like my module, it&amp;#8217;s&amp;nbsp;useful.&lt;/p&gt;</content><category term="puppet"></category><category term="beaker"></category><category term="sut"></category><category term="rspec"></category><category term="virtualbox"></category><category term="headless"></category><category term="GUI"></category></entry><entry><title>AwsLimitChecker - Check Your AWS Usage Against ServiceÂ Limits</title><link href="http://blog.jasonantman.com/2015/07/awslimitchecker-check-your-aws-usage-against-service-limits/" rel="alternate"></link><published>2015-07-25T08:35:00-04:00</published><updated>2015-07-25T08:35:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-07-25:/2015/07/awslimitchecker-check-your-aws-usage-against-service-limits/</id><summary type="html">&lt;p&gt;Initial release of AwsLimitChecker, a tool to check your &lt;span class="caps"&gt;AWS&lt;/span&gt; usage against service limits and Trusted&amp;nbsp;Advisor.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Over the past year or so, at my day job, we&amp;#8217;ve been leveraging &lt;span class="caps"&gt;AWS&lt;/span&gt; more and more, specifically
&lt;a href="https://aws.amazon.com/cloudformation/"&gt;CloudFormation&lt;/a&gt; to manage complete application stacks. One
side-effect of this is that we went through a few periods where we were constantly hitting various
&lt;span class="caps"&gt;AWS&lt;/span&gt; Service Limits - subnet groups, ElastiCache clusters, security groups, and a whole slew of others.
In pretty much all these cases, we weren&amp;#8217;t &lt;em&gt;really&lt;/em&gt; aware of the limits; we (the Tooling and
Automation team) had succeeded in our goal of handing our internal customers the tools to spin up
complete application environments, per-developer, on-demand. And it was wonderful until we hit some
magic number of CloudFormation stacks, at which point almost every day for a week or two we had to
open a new &lt;span class="caps"&gt;AWS&lt;/span&gt; Support ticket to have a different limit increased, and deal with completely broken
deploys until that was done (or send out a frantic &amp;#8220;someone please delete a dev stack&amp;#8221;&amp;nbsp;email).&lt;/p&gt;
&lt;p&gt;Early last month we decided that we had to do something about this. As much as I tried, I couldn&amp;#8217;t
find an existing solution that would monitor our limits and alert us when we approached them; there
were some open source scripts that would do so for a handful of limits (generally just &lt;span class="caps"&gt;EC2&lt;/span&gt; usage),
and the proprietary solutions that I was able to find didn&amp;#8217;t seem much better; none of them stated
that they handle &lt;span class="caps"&gt;VPC&lt;/span&gt; or ElastiCache limits, which had been problematic for us. &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217;s own
&lt;a href="https://aws.amazon.com/premiumsupport/trustedadvisor/"&gt;Trusted Advisor&lt;/a&gt; has a Service Limits
check available to Business- and Enterprise-level support accounts, but it only monitors 17 of the
94 Service Limits that we identified as relevant to us, and it sends out &lt;em&gt;weekly&lt;/em&gt; alerts. So,
I decided to write something to solve the problem. My co-workers and I have been trying to get
corporate legal approval to release our work publicly under an &lt;span class="caps"&gt;OSI&lt;/span&gt;-approved license for years,
to no avail. I asked my team if they&amp;#8217;d support waiting a while for this work, so I could do it
entirely in my own time, publicly, under an open source license. Happily, they&amp;nbsp;agreed.&lt;/p&gt;
&lt;p&gt;Today I&amp;#8217;m making the first release of &lt;a href="https://github.com/jantman/awslimitchecker"&gt;awslimitchecker&lt;/a&gt;,
an &lt;span class="caps"&gt;AGPL&lt;/span&gt; 3.0-licensed Python tool to calculate your &lt;span class="caps"&gt;AWS&lt;/span&gt; resource usage for various services bound by
&lt;a href="http://awslimitchecker.readthedocs.org/en/latest/limits.html#current-checks"&gt;service limits&lt;/a&gt;, and tell you which ones exceed a given threshold (actually, warning and critical
thresholds). Effective limits are hard-coded to the &lt;a href="http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html"&gt;published defaults&lt;/a&gt;,
but can be overridden in cases where you&amp;#8217;ve received limit increases, and will be automatically updated
from Trusted Advisor data for all limits that it monitors (if your account includes the full &lt;span class="caps"&gt;TA&lt;/span&gt; checks).
awslimitchecker provides warning and critical thresholds that can be set globally as a percentage of the
limit (defaults are 80% and 99%, respectively) or overridden on a per-limit basis, as either a percentage
or a fixed integer usage&amp;nbsp;value.&lt;/p&gt;
&lt;p&gt;awslimitchecker is available &lt;a href="https://pypi.python.org/pypi/awslimitchecker/0.1.0"&gt;from pypi&lt;/a&gt;.
It is compatible and tested with Python versions 2.6 through 3.4, though the library it uses to communicate
with &lt;span class="caps"&gt;AWS&lt;/span&gt;, &lt;a href="http://boto.readthedocs.org/en/latest/"&gt;boto&lt;/a&gt;, still has a few &lt;span class="caps"&gt;AWS&lt;/span&gt; services which are not python3-compatible.
awslimitchecker includes both a Python module with a &lt;a href="http://awslimitchecker.readthedocs.org/en/latest/awslimitchecker.checker.html"&gt;documented &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt; for those who
don&amp;#8217;t mind working with Python, and a command line script for those who&amp;nbsp;do.&lt;/p&gt;
&lt;p&gt;The project is still very young, and only being used by one organization, but it&amp;#8217;s proven
stable for us, and I&amp;#8217;m more than happy to accept questions, comments, criticisms,
&lt;a href="https://github.com/jantman/awslimitchecker/issues"&gt;issues/feature requests&lt;/a&gt; and Pull&amp;nbsp;Requests.&lt;/p&gt;</content><category term="aws"></category><category term="ec2"></category><category term="limits"></category><category term="python"></category><category term="awslimitchecker"></category><category term="cloud"></category></entry><entry><title>Visualization of when Iâm working on personal vs workÂ projects</title><link href="http://blog.jasonantman.com/2015/06/visualization-of-when-im-working-on-personal-vs-work-projects/" rel="alternate"></link><published>2015-06-05T21:20:00-04:00</published><updated>2015-06-05T21:20:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-06-05:/2015/06/visualization-of-when-im-working-on-personal-vs-work-projects/</id><summary type="html">&lt;p&gt;A fun python script to visualize the time of day and day of week of your commits to personal vs work&amp;nbsp;repositories.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was thinking the other day - as I was pushing out some final code reviews for work at &lt;span class="caps"&gt;11PM&lt;/span&gt; before taking a day off -
about how much work I do outside of &amp;#8220;work hours&amp;#8221;. And the answer is, I don&amp;#8217;t really know, especially when it comes to
projects that I really enjoy and find interesting. So, I decided to have some fun with &lt;a href="https://github.com/gitpython-developers/GitPython"&gt;GitPython&lt;/a&gt;
and find&amp;nbsp;out.&lt;/p&gt;
&lt;p&gt;The result of this was &lt;a href="https://github.com/jantman/misc-scripts/blob/master/whendoiwork.py"&gt;whendoiwork.py&lt;/a&gt;. It&amp;#8217;s a pretty simple script,
and also makes some pretty big assumptions, but I found the results interesting. Given some local directories which contain git clones
of my work repositories, and some which contain clones of my personal repos, it iterates over all* of the commits in them by me
(going by the git author name) in the last N days (default 365); it counts commits to personal repositories as +1 and to work
repositories as -1, and adds them to buckets per hour of day, per day of week. It then uses &lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt;
to build a heatmap, with the maximum commits per hour for work repos in blue and the maximum per hour for personal in&amp;nbsp;red.&lt;/p&gt;
&lt;p&gt;I can&amp;#8217;t vouch that it&amp;#8217;s 100% accurate, but the results were interesting to me; while it seems like I tend to do a fair amount
of work in the evenings, compared to work on personal projects, all of my work for my employer is well contained in my normal
7-3 work&amp;nbsp;day.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s an example of the output of this script, for my own work, run&amp;nbsp;with:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;./whendoiwork.py -v -a /home/jantman/&lt;span class="caps"&gt;GIT&lt;/span&gt; -b /home/jantman/work/git -b /home/jantman/work/git/ops -d 365 -t &amp;#39;&lt;span class="caps"&gt;US&lt;/span&gt;/Eastern&amp;#39; --repoAlabel personal --repoBlabel work
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that this iterates over every commit in all of the git repos it finds, possibly multiple times. On my own
system (9G of git repos with a few hundred thousand commits), this took about 2&amp;nbsp;minutes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="heatmap of days of week and hours of day when I commit to work vs personal repos" src="https://raw.githubusercontent.com/jantman/misc-scripts/master/whendoiwork.png"&gt;&lt;/p&gt;
&lt;p&gt;If you find any bugs/issues with it, please pass them along by &lt;a href="https://github.com/jantman/misc-scripts/issues"&gt;opening an issue&lt;/a&gt;.&lt;/p&gt;</content><category term="git"></category><category term="commits"></category><category term="python"></category><category term="graphs"></category><category term="visualization"></category></entry><entry><title>Local S3 Server to Acceptance Test Netflix Ice Installation InÂ Isolation</title><link href="http://blog.jasonantman.com/2015/05/local-s3-server-to-acceptance-test-netflix-ice-installation-in-isolation/" rel="alternate"></link><published>2015-05-05T06:45:00-04:00</published><updated>2015-05-05T06:45:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-05-05:/2015/05/local-s3-server-to-acceptance-test-netflix-ice-installation-in-isolation/</id><summary type="html">&lt;p&gt;How I wrote isolated acceptance tests for Netflix Ice Puppet installation using a locally-backed S3 &lt;span class="caps"&gt;API&lt;/span&gt;&amp;nbsp;server.&lt;/p&gt;</summary><content type="html">&lt;p&gt;At work, we recently started using &lt;a href="http://netflix.github.io/"&gt;Netflix &lt;span class="caps"&gt;OSS&lt;/span&gt;&lt;/a&gt;&amp;#8216;s &lt;a href="https://github.com/Netflix/ice"&gt;Ice&lt;/a&gt; &lt;span class="caps"&gt;AWS&lt;/span&gt; cost analysis tool.
It provides a Java daemon to read and parse &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217; &lt;a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html"&gt;detailed billing reports&lt;/a&gt;
and a web interface to the data (&lt;a href="https://github.com/Netflix/ice/blob/master/README.md#screenshots"&gt;screenshots&lt;/a&gt;). The single biggest feature for us
is the ability to do cost breakdowns (by hour/day/week/month) based on Cost Allocation tags in the detailed billing reports. We tag every billable &lt;span class="caps"&gt;AWS&lt;/span&gt;
resource with the Application Name, Service Class (environment; dev/test/prod) and Responsible Party. Ice lets us configure &amp;#8220;Application Groups&amp;#8221;
based on applications as seen from a business/budgetary standpoint and allow up-to-the-hour data on that available to anyone who needs&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;We spun up the development install of Ice for a few weeks to give it a spin, but once people started complaining that my screen session died and took
Ice with it, it was clear we needed a real, permanent installation. While there is &lt;a href="https://github.com/mdsol/ice_cookbook"&gt;chef&lt;/a&gt; and &lt;a href="https://github.com/Answers4AWS/netflixoss-ansible"&gt;ansible&lt;/a&gt;
code to install and configure Ice, we&amp;#8217;re a Puppet shop, and there wasn&amp;#8217;t anything available that I could find for Puppet. So, I set about writing a
module to install and configure Ice, running in Tomcat behind an Nginx proxy. Like any good modern module, I wanted not only &lt;a href="http://rspec-puppet.com/"&gt;rspec-puppet&lt;/a&gt;
unit tests but also &lt;a href="https://github.com/puppetlabs/beaker"&gt;beaker&lt;/a&gt; acceptance tests. For those unfamiliar, Beaker is an acceptance testing framework for Puppet
that&amp;#8217;s similar to Test Kitchen; it spins up Vagrant machines, runs some code in them, and then uses &lt;a href="http://serverspec.org/"&gt;serverspec&lt;/a&gt; to make assertions about
the state of the system (file contents, running processes, command output, etc.) (side note: if you used Beaker prior to the
&lt;a href="https://github.com/puppetlabs/beaker/blob/master/HISTORY.md#beaker2.0.0"&gt;2.0 release&lt;/a&gt; in December 2014, you should really try it again; they&amp;#8217;ve made some great&amp;nbsp;improvements).&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The&amp;nbsp;Problem&lt;/h2&gt;
&lt;p&gt;This posed a bit of a challenge, as Ice (in addition to being pretty poorly documented) is really designed to run in &lt;span class="caps"&gt;AWS&lt;/span&gt;. Firstly, the very reason we started running Ice was
to get a handle on our fast-growing &lt;span class="caps"&gt;AWS&lt;/span&gt; spend; as a result, we&amp;#8217;re trying hard not to use &lt;span class="caps"&gt;AWS&lt;/span&gt; for small-scale projects that could use existing resources. Second, while our
company very unfortunately doesn&amp;#8217;t have an open source policy and isn&amp;#8217;t releasing anything (hopefully this may be changing soon), we try hard to write generic, forge-quality&amp;nbsp;modules.&lt;/p&gt;
&lt;p&gt;As a result, I wanted to use the default Vagrant/VirtualBox provider for Beaker. To make matters worse, in keeping with the spirit of a community module, I didn&amp;#8217;t
want the acceptance tests to require anything specific to my company, such as an S3 bucket preseeded with our billing data. Ice both reads the detailed billing reports
(one of its three inputs; &lt;span class="caps"&gt;EC2&lt;/span&gt; pricing data and your accounts&amp;#8217; reservation pricing/capacity being the others) and writes state from and to S3. So, this was a bit difficult.
As we don&amp;#8217;t plan on upgrading Ice terribly often, and we wanted to install from the &lt;a href="https://netflixoss.ci.cloudbees.com/job/ice-master/"&gt;cloudbees master builds&lt;/a&gt;, we wanted
acceptance testing of not just the provisioning tooling, but also some basic smoke tests for the application&amp;nbsp;itself.&lt;/p&gt;
&lt;h2 id="the-solution"&gt;The&amp;nbsp;Solution&lt;/h2&gt;
&lt;p&gt;I managed to come up with a working, albeit somewhat Rube Goldberg, method of getting isolated acceptance tests to work. What follows is the gist of how I got Ice
working in complete isolation. The majority of this happens in &lt;code&gt;spec/acceptance/0prerequisite_spec.rb&lt;/code&gt; which runs first and both does the prerequisite setup
and validates that everything is setup right and working for the tests. The following solution is based on the amazingly helpful &lt;a href="https://github.com/jubos/fake-s3"&gt;fakes3&lt;/a&gt;
Ruby gem, the &lt;a href="http://www.apsis.ch/pound/"&gt;Pound&lt;/a&gt; reverse proxy, and some &lt;span class="caps"&gt;SSL&lt;/span&gt; certificate trickery. While my code was specific to Beaker, this should be generic
enough to use with any system acceptance testing&amp;nbsp;tool.&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;First, we obtain or create some files that we&amp;#8217;ll need on the test&amp;nbsp;instance:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Grab a relatively recent Detailed Billing With Resources and Tags zipped &lt;span class="caps"&gt;CSV&lt;/span&gt; report from an &lt;span class="caps"&gt;AWS&lt;/span&gt; account of yours (the filename is in the format
    &lt;code&gt;&amp;lt;ACCOUNT NUMBER&amp;gt;-aws-billing-detailed-line-items-with-resources-and-tags-&amp;lt;YYYY&amp;gt;-&amp;lt;MM&amp;gt;.csv&lt;/code&gt;). Manually trim it down to a sufficient sample of data;
    I took a few hours&amp;#8217; worth of data from one day and trimmed it down to just that referencing a few randomly chosen &lt;span class="caps"&gt;RDS&lt;/span&gt; instances, ELBs, on-demand &lt;span class="caps"&gt;EC2&lt;/span&gt;
    instances and reserved &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. I then anonymized the account number, resource IDs, tag values, and anything else identifying. Ice needs billing
    data in order to do anything, so this will serve as our test&amp;nbsp;data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When Ice runs, it attempts to retrieve reserved instance pricing. It appears (I&amp;#8217;ve lost the mailing list or GitHub issue reference) that it&amp;#8217;s typical for
    the first Ice run on an empty S3 work directory to die because these files are missing. As a result, grab the &lt;code&gt;reservation_prices.oneyear.*&lt;/code&gt; files from
    the S3 work bucket of a running/working Ice installation. This will prevent a time-consuming shutdown of Ice on the first&amp;nbsp;run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generate a self-signed &lt;span class="caps"&gt;SSL&lt;/span&gt; key and certificate for &lt;code&gt;fakebucket.s3.amazonaws.com&lt;/code&gt;. Package them together in a &lt;span class="caps"&gt;PEM&lt;/span&gt; file suitable for use in web servers.
    (Note that most modern S3 &lt;span class="caps"&gt;API&lt;/span&gt; clients accept a full &lt;span class="caps"&gt;URL&lt;/span&gt; to a bucket, as there are now third parties that implement the S3 &lt;span class="caps"&gt;API&lt;/span&gt;. Ice does not; it connects
    to https://&lt;span class="caps"&gt;BUCKETNAME&lt;/span&gt;.s3amazonaws.com. As a result, this &lt;span class="caps"&gt;SSL&lt;/span&gt; foolery is&amp;nbsp;required.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;a href="https://rubygems.org/gems/fakes3"&gt;fakes3&lt;/a&gt; rubygem; this provides an s3-compliant &lt;span class="caps"&gt;API&lt;/span&gt; backed by local filesystem storage.
    Configure it to run during your tests (I set it up as a systemd service, but there are certainly other ways to do this). Note that
    while fakes3 stores the uploaded data on the local filesystem, it maintains a mapping of known objects in memory; as such, the process
    always starts completely empty, regardless of what&amp;#8217;s in the backing directory on the filesystem. fakes3 allows all &lt;span class="caps"&gt;IAM&lt;/span&gt; credentials,
    so fake ones are fine. It also automatically creates buckets the first time they&amp;#8217;re&amp;nbsp;accessed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;a href="http://www.apsis.ch/pound/"&gt;pound&lt;/a&gt; reverse proxy and configure it to listen on port 443 with the &lt;span class="caps"&gt;PEM&lt;/span&gt; file you generated
    earlier, and proxy to fakes3 (which listens by default on port 10000). The &lt;code&gt;ListenHTTPS&lt;/code&gt;section of &lt;code&gt;pound.cfg&lt;/code&gt; will need the
    &lt;code&gt;xHTTP 1&lt;/code&gt; directive in order to enable &lt;span class="caps"&gt;HTTP&lt;/span&gt; verbs other than&amp;nbsp;&lt;span class="caps"&gt;GET&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setup a local hosts file entry pointing &lt;code&gt;fakebucket.s3.amazonaws.com&lt;/code&gt; at &lt;code&gt;127.0.0.1&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After fakes3 starts, upload your sample billing data file and your reserved instance pricing files to the appropriate paths under a
    bucket called &amp;#8220;fakebucket&amp;#8221;. You can use a tool such as &lt;a href="http://s3tools.org/s3cmd"&gt;s3cmd&lt;/a&gt; to manipulate its contents, and other
    supported tools are listed in &lt;a href="https://github.com/jubos/fake-s3/wiki/Supported-Clients"&gt;the documentation&lt;/a&gt;. This step also serves
    to validate your Pound configuration, which should pass &lt;span class="caps"&gt;HTTPS&lt;/span&gt; port 443 traffic through to fakes3 and allow you to store and
    retrieve&amp;nbsp;objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Figure out the path to the trusted keystore for the version of Java that you&amp;#8217;re running Ice under. On CentOS 7 with OpenJDK 1.7.0,
    this was (after a lot of symlinks) &lt;code&gt;/usr/lib/jvm/jre/lib/security/cacerts&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Import your self-signed certificate into the Java keystore as a trusted certificate. This will allow &lt;span class="caps"&gt;SSL&lt;/span&gt; verification to succeed even
    with a self-signed&amp;nbsp;certificate:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;/bin/keytool -importcert -alias fakebucket -file fakebucket.s3.amazonaws.com.crt -keystore /usr/lib/jvm/jre/lib/security/cacerts -storepass changeit -trustcacerts -noprompt
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure &lt;code&gt;ice.properties&lt;/code&gt; for the above. The important and unintuitive parts that I found&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Going by the above examples, your billing and work S3 bucket names should both be&amp;nbsp;&amp;#8220;fakebucket&amp;#8221;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unless you want to mock out bigger parts of the &lt;span class="caps"&gt;AWS&lt;/span&gt; metadata service, run Ice with
   &lt;code&gt;-Dice.s3AccessKeyId=NotAValidAccessKeyId -Dice.s3SecretKey=NotAValidAwsSecretKeyXxxxxxxxxxxxxxxxxxx&lt;/code&gt;
   in the &lt;code&gt;JAVA_OPTS&lt;/code&gt;. If Ice can&amp;#8217;t retrieve an instance&amp;#8217;s &lt;span class="caps"&gt;IAM&lt;/span&gt; role from the metadata service
   (http://169.254.169.254/latest/meta-data/iam/security-credentials/) and doesn&amp;#8217;t have the
   access and secret keys defined, it won&amp;#8217;t run. Also note that while the documentation is &lt;strong&gt;very&lt;/strong&gt;
   unclear on this, a number of &lt;a href="https://github.com/Netflix/ice/issues/49#issuecomment-23497701"&gt;github issues&lt;/a&gt;
   clarify that these need to be passed in as Java runtime options; they can&amp;#8217;t be put in the properties&amp;nbsp;file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Disable the Reservation Capacity Poller (&lt;code&gt;ice.reservationCapacityPoller=false&lt;/code&gt;). This service
   needs to connect to the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;, and will cause Ice to die if it&amp;nbsp;can&amp;#8217;t.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For testing purposes, it&amp;#8217;s a lot simpler and less error-prone (as well as being a lot faster) to
   test the processor and reader separately - at least in serial instead of simultaneously in the same&amp;nbsp;instance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once all this is done, running the Ice Processor should retrieve the billing file, process it, and write the processed data to the
fakes3 bucket. Running the Reader should display the data properly. So far I&amp;#8217;ve been unable to find any features (other than the
Reservation Capacity Poller, noted above) that don&amp;#8217;t work with this&amp;nbsp;setup.&lt;/p&gt;
&lt;p&gt;Whether it&amp;#8217;s related to Ice itself or ideas for acceptance testing isolated applications, I hope this can be of use to&amp;nbsp;someone&amp;#8230;&lt;/p&gt;</content><category term="netflix"></category><category term="ice"></category><category term="puppet"></category><category term="beaker"></category><category term="acceptance testing"></category><category term="aws"></category><category term="s3"></category><category term="fakes3"></category><category term="testing"></category></entry><entry><title>Jira to TrelloÂ Script</title><link href="http://blog.jasonantman.com/2015/04/jira-to-trello-script/" rel="alternate"></link><published>2015-04-10T05:58:00-04:00</published><updated>2015-04-10T05:58:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-04-10:/2015/04/jira-to-trello-script/</id><summary type="html">&lt;p&gt;A script to pull time tracking and dependency information for Jira tickets onto Trello&amp;nbsp;cards.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few weeks ago, I &lt;a href="/2015/03/my-new-found-love-of-trello-and-a-helpful-greasemonkey-script/"&gt;posted about&lt;/a&gt; how I&amp;#8217;ve
started using &lt;a href="https://trello.com/"&gt;Trello&lt;/a&gt; to keep track of my work and keep things flowing smoothly. It&amp;#8217;s been
absolutely wonderful, and I feel more productive and less stressed, and like I have a better idea of what&amp;#8217;s coming
up. The one thing that Trello is missing is time tracking. I don&amp;#8217;t need anything fancy, all I really wanted was to
be able to show a time estimate on my cards. I know I could&amp;#8217;ve just put the estimate right in the title of the cards,
but that seemed like a waste of&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;At the moment we&amp;#8217;re using Jira at work, so I wrote &lt;a href="https://github.com/jantman/misc-scripts/blob/master/jira2trello.py"&gt;jira2trello.py&lt;/a&gt;.
It&amp;#8217;s a pretty simple Python script that uses the &lt;a href="https://pypi.python.org/pypi/trello"&gt;trello&lt;/a&gt; and
&lt;a href="https://pypi.python.org/pypi/jira"&gt;jira&lt;/a&gt; packages from pypi to iterate over all cards on a specified Trello
board, and for each card that matches a configurable regular expression for ticket keys (i.e.
&lt;code&gt;.*((project1|project2|project3)-\d+):.*&lt;/code&gt;), the script&amp;nbsp;will:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Determine if the Jira issue is a subtask, and if so, prefix its title with the issue key of the parent issue,
using the format of &lt;code&gt;PARENT-xxx -&amp;gt; CHILD-xxx&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Look up the &amp;#8220;Original Estimate&amp;#8221; time tracking field in Jira, and if present, prepend it to the title of
the&amp;nbsp;card.&lt;/li&gt;
&lt;li&gt;Regenerate the title of the card, using the current issue summary from&amp;nbsp;Jira.&lt;/li&gt;
&lt;li&gt;Move the card to a specified &amp;#8220;Done&amp;#8221; list if it&amp;#8217;s closed in&amp;nbsp;Jira.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are a few assumptions in the script about how the titles of cards are formed, namely that they follow the
&lt;code&gt;ISSUE-xxx: Summary Here&lt;/code&gt; format used by my &lt;a href="https://github.com/jantman/userscripts#trellocontextmenu"&gt;TrelloContextMenu&lt;/a&gt;
Firefox userscript. But I hope that this might be of use to someone else as well. Please feel free to open issues
or submit pull requests for any improvements that would be helpful, including any assumptions I&amp;#8217;ve made that aren&amp;#8217;t
valid in your environment. The source can be found on GitHub: &lt;a href="https://github.com/jantman/misc-scripts/blob/master/jira2trello.py"&gt;jira2trello.py&lt;/a&gt;.&lt;/p&gt;</content><category term="jira"></category><category term="trello"></category><category term="python"></category><category term="ticket"></category><category term="kanban"></category></entry><entry><title>My New-found Love of Trello and a Helpful GreaseMonkeyÂ Script</title><link href="http://blog.jasonantman.com/2015/03/my-new-found-love-of-trello-and-a-helpful-greasemonkey-script/" rel="alternate"></link><published>2015-03-22T19:36:00-04:00</published><updated>2015-03-22T19:36:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-03-22:/2015/03/my-new-found-love-of-trello-and-a-helpful-greasemonkey-script/</id><summary type="html">&lt;p&gt;How Trello has made my past week so much less stressful, and a helpful GreaseMonkey script to help add new&amp;nbsp;cards.&lt;/p&gt;</summary><content type="html">&lt;p&gt;When I started at my current job two and a half years ago, we were just really getting into &lt;a href="http://en.wikipedia.org/wiki/Kanban_%28development%29"&gt;Kanban&lt;/a&gt;.
We used a web-based Kanban board (&lt;a href="https://github.com/cmheisel/kardboard"&gt;Kardboard&lt;/a&gt;, written by our director of development at the time), and each team had
their daily stand-up status meetings in the middle of the office in front of a projector screen, with their board filling the entire wall (and remotes on Skype). It took me a while
to get used to it, having always worked in very ticket-and-emergency-driven ops roles. But once it clicked, it was like an epiphany. Suddenly I could see all
of the (non-breakfix or unforeseen) work headed to our team, its priority and due dates, and what everyone (including myself) was working on at a given moment.
Even though work-in-progress (&lt;span class="caps"&gt;WIP&lt;/span&gt;) limits never made it to the ops team, it reduced my stress level amazingly. Instead of dealing with a massive queue of tickets
assigned to me - some of which were such low priority I&amp;#8217;d probably never get around to them - suddenly all I had to concern myself with was my &lt;span class="caps"&gt;WIP&lt;/span&gt; and what was
next up for&amp;nbsp;me.&lt;/p&gt;
&lt;p&gt;Over the past year we&amp;#8217;ve had a major (almost complete) changeover of management, and many of the old ways have disappeared. Some might even say that, as a technical
company, we&amp;#8217;ve regressed quite a bit. Either way, we haven&amp;#8217;t been using Kanban for over six months. Our development teams are moving to &lt;a href="http://en.wikipedia.org/wiki/Scrum_%28software_development%29"&gt;Scrum&lt;/a&gt;,
and our more ops-y teams (I&amp;#8217;m now on our Automation and Tooling team, straddling the awkward line between the two) are trying to figure out what&amp;#8217;s right for us.
And I haven&amp;#8217;t been this stressed since I started work here; my team is both busier than ever and understaffed by almost 50%. We stopped using the Kanban board in our
stand-ups, our new manager stopped referring to it, so (even with mostly-working synchronization with Jira) it stopped being useful, and I stopped using it. Without thinking,
I went back to my old &amp;#8220;page showing &lt;em&gt;everything&lt;/em&gt; assigned to me&amp;#8221; view in our ticketing system, and grew increasingly frustrated by managing my &lt;span class="caps"&gt;WIP&lt;/span&gt; and deciding what
needed to be worked&amp;nbsp;next.&lt;/p&gt;
&lt;p&gt;So, after discussing this with the rest of my team, last week two of us came to the same conclusion, independently, on the same day: use &lt;a href="https://trello.com/"&gt;Trello&lt;/a&gt; to
run our own personal Kanban boards. I&amp;#8217;ve been doing so for about a week now, and all I&amp;#8217;m horribly embarrassed that I didn&amp;#8217;t think of this sooner. It&amp;#8217;s absolutely wonderful -
I can keep managing my own work in a Kanban-like form (albeit without formal &lt;span class="caps"&gt;WIP&lt;/span&gt; limits) without needing management endorsement. Sure, it only works for things that I know
are coming and takes some manual curation time, but so far, it&amp;#8217;s been amazingly refreshing and calming. The best part is being able to (once again) visually see
both my &lt;span class="caps"&gt;WIP&lt;/span&gt;, and my recently-complete&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;Someone else on my team mentioned that they use Trello for their personal tasks; I created two more boards, one for my personal development work, and another for my general
around-the-house tasks and to-do&amp;#8217;s (my wife connected with her inner manager once I shared my board with her and she figured out that she could re-order by backlog&amp;#8230;).
It&amp;#8217;s 8 &lt;span class="caps"&gt;PM&lt;/span&gt; on Sunday night, and I can confidently say that I&amp;#8217;ve had one of the most productive weekends in ages. And one of the most relaxing. Instead of spending
lots of time trying to figure out what I have to do this weekend and what the priorities are, I just used the same Kanban method that I loved from work. And it
paid&amp;nbsp;off.&lt;/p&gt;
&lt;h2 id="the-greasemonkey-script"&gt;The GreaseMonkey&amp;nbsp;Script&lt;/h2&gt;
&lt;p&gt;The one thing that initially bothered me about Trello was the time it took to add cards. At work every task I have is in either our ticketing system or
a GitHub Issue. Our previous official tool, &lt;a href="https://github.com/cmheisel/kardboard"&gt;Kardboard&lt;/a&gt;, synchronized with Jira so everything was always
up-to-date and on the right board. At first I was adding cards manually, but I figured there had to be a better way. A quick Google search turned up
a &lt;a href="https://github.com/danlec/Trello-Bookmarklet"&gt;Bookmarklet&lt;/a&gt; by &lt;a href="https://github.com/danlec"&gt;Daniel LeCheminant&lt;/a&gt; of Trello, to add a Trello card for the current
page. It does some really cool stuff, like parsing Jira and GitHub issues and setting the card title nicely for them, as well as some other ticketing
systems. I also found a &lt;a href="https://gist.github.com/aggieben/5811685"&gt;GreaseMonkey script&lt;/a&gt; from &lt;a href="https://github.com/aggieben"&gt;Benjamin Collins&lt;/a&gt; of StackExchange
that adds a link to create Trello cards from StackExchange meta&amp;nbsp;posts.&lt;/p&gt;
&lt;p&gt;So, I took things a bit further and whipped up a GreaseMonkey script, &lt;a href="https://github.com/jantman/userscripts#trellocontextmenu"&gt;TrelloContextMenu&lt;/a&gt;. It
uses Daniel&amp;#8217;s card naming code (plus fixing the GitHub format a bit and adding support for &lt;a href="https://www.reviewboard.org/"&gt;ReviewBoard&lt;/a&gt; code reviews) and
the GreaseMonkey/Trello logic from Benjamin&amp;#8217;s script. Once installed and authenticated with Trello, the script retrieves a list of all of your boards
and cards, and adds an &amp;#8220;Add to Trello&amp;#8221; right-click context menu in Firefox, allowing you to add the current page to any list on any of your&amp;nbsp;boards.&lt;/p&gt;
&lt;p&gt;&lt;a href="/GFX/TrelloContextMenu_large.png"&gt;&lt;img alt="screenshot of TrelloContextMenu context menu popup in Firefox" src="/GFX/TrelloContextMenu_sm.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The script is &lt;a href="https://github.com/jantman/userscripts#trellocontextmenu"&gt;available on GitHub&lt;/a&gt; (the link in the &lt;span class="caps"&gt;README&lt;/span&gt; will go to an installable raw
version of the script), and uses GreaseMonkey&amp;#8217;s versioning capabilities. At the moment I&amp;#8217;ve only tested it with GreaseMonkey in Firefox, and I don&amp;#8217;t
expect it to work elsewhere as it uses a few GreaseMonkey-specific features, such as &lt;code&gt;GM_xmlhttpRequest&lt;/code&gt; and GreaseMoneky&amp;#8217;s browser-wide SQLite
persistent storage (to store your boards and lists, and authentication credentials, until you manually refresh). I&amp;#8217;d be happy to accept pull requests
from anyone who can get it working in other&amp;nbsp;browsers.&lt;/p&gt;</content><category term="Trello"></category><category term="kanban"></category><category term="tickets"></category><category term="organization"></category><category term="work"></category></entry><entry><title>Some Additional ServerspecÂ Types</title><link href="http://blog.jasonantman.com/2015/03/some-additional-serverspec-types/" rel="alternate"></link><published>2015-03-14T11:58:00-04:00</published><updated>2015-03-14T11:58:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-03-14:/2015/03/some-additional-serverspec-types/</id><summary type="html">&lt;p&gt;Some additional types that I wrote for&amp;nbsp;Serverspec&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="http://serverspec.org/"&gt;Serverspec&lt;/a&gt; is an rspec-based framework for testing live machines,
and making assertions about things like the output of commands, installed packages, running
services, file content, etc. However, it has a relatively limited and basic set of
&lt;a href="http://serverspec.org/resource_types.html"&gt;Resource Types&lt;/a&gt; that it can test&amp;nbsp;for.&lt;/p&gt;
&lt;p&gt;Before Serverspec completely disabled their GitHub issue tracker (they now seem to have no
issue tracker at all), I&amp;#8217;d suggested some improvements for more advanced resource types,
such as one that can perform an &lt;span class="caps"&gt;HTTP&lt;/span&gt; &lt;span class="caps"&gt;GET&lt;/span&gt; against an application and check the status code
and/or output. I was told in no uncertain terms that this is a task for application integration
testing, and that it&amp;#8217;s &amp;#8220;not what Serverspec is&amp;nbsp;for.&amp;#8221;&lt;/p&gt;
&lt;p&gt;I humbly disagree. I&amp;#8217;ve begun migrating my &lt;a href="https://www.linode.com/"&gt;Linode&lt;/a&gt; to an &lt;span class="caps"&gt;EC2&lt;/span&gt; machine,
using some technology that I&amp;#8217;ve been using at my day job; specifically, Puppet to configure the
machine and &lt;a href="https://packer.io/"&gt;Packer&lt;/a&gt; to build an &lt;span class="caps"&gt;AMI&lt;/span&gt;. Instead of using &lt;a href="http://aws.amazon.com/cloudformation/"&gt;Cloudformation&lt;/a&gt;
to spin up an entire stack, I just use a Rakefile to spin up a new &lt;span class="caps"&gt;EC2&lt;/span&gt; instance, test it, and
swap an &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html"&gt;Elastic &lt;span class="caps"&gt;IP&lt;/span&gt;&lt;/a&gt;
if all the tests pass. Of course, this requires that I have relatively complete automated testing
of the &lt;span class="caps"&gt;EC2&lt;/span&gt; instance. Stock Serverspec can handle 95% of what I want to test, but there are a few
other, more complex, things that it can&amp;#8217;t. So, I wrote some code to fix&amp;nbsp;that.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll admit right off the bat that this code doesn&amp;#8217;t really work the way Serverspec is intended to,
but it works and it&amp;#8217;s relatively simple. This largely breaks the abstraction of serverspec using
&lt;a href="https://github.com/serverspec/specinfra"&gt;specinfra&lt;/a&gt; under the hood, but I&amp;#8217;m not sure if that&amp;#8217;s even
a concern (since specinfra seems to be all about testing a running machine via some local command
execution mechanism, and two of the types that I wrote use network &lt;span class="caps"&gt;IO&lt;/span&gt;&amp;nbsp;instead).&lt;/p&gt;
&lt;p&gt;For the time being, I&amp;#8217;ve written three additional &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#Types"&gt;types&lt;/a&gt;
that solve some specific use cases for&amp;nbsp;me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#bitlbee"&gt;bitlbee&lt;/a&gt;
type that connects to a &lt;a href="http://www.bitlbee.org/"&gt;Bitlbee&lt;/a&gt; &lt;span class="caps"&gt;IRC&lt;/span&gt; gateway, authenticates,
and checks the running bitlbee version. It has matchers to check whether or not the connection and
authentication was successful, whether or not it timed out, and the bitlbee version. Parameters for
the type include login nick and password, bitlbee port, and whether or not to connect with&amp;nbsp;&lt;span class="caps"&gt;SSL&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;A &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#http_get"&gt;http_get&lt;/a&gt;
type which connects to the system under test (with a specified port) and issues a
&lt;span class="caps"&gt;HTTP&lt;/span&gt; &lt;span class="caps"&gt;GET&lt;/span&gt; request for a specified path, with a specified &lt;code&gt;Host&lt;/code&gt; header and a timeout (default
10 seconds). Matchers are provided for the response content body (string), response headers
(hash), &lt;span class="caps"&gt;HTTP&lt;/span&gt; status code, and whether or not the request timed out (which also sets a status of&amp;nbsp;0).&lt;/li&gt;
&lt;li&gt;A &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#virtualenv"&gt;virtualenv&lt;/a&gt; type for testing
python &lt;a href="https://virtualenv.pypa.io/en/latest/"&gt;virtualenv&lt;/a&gt;s. It takes the absolute path to the venv
on the filesystem, and uses serverspec&amp;#8217;s built-in file and command execution features to ensure that
the path &amp;#8220;looks like&amp;#8221; a virtualenv, and has matchers for the pip and python versions used in the venv
as well as the &lt;code&gt;pip freeze&lt;/code&gt; output as a hash of requirements and their&amp;nbsp;versions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hopefully this will be of use to someone else as well. As I continue using serverspec, I plan on
adding to the&amp;nbsp;types.&lt;/p&gt;
&lt;p&gt;The code for serverspec-extended-types is on &lt;a href="https://github.com/jantman/serverspec-extended-types/tree/master"&gt;GitHub&lt;/a&gt;
(pull requests and issues welcome) and it&amp;#8217;s packaged and hosted as a &lt;a href="https://rubygems.org/gems/serverspec-extended-types"&gt;ruby gem&lt;/a&gt;.
&lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/0.0.2#Installation"&gt;Installation&lt;/a&gt; and usage is as simple
as adding it to your Gemfile and &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/0.0.2#Usage"&gt;spec_helper&lt;/a&gt;
and then using the types and matchers in your&amp;nbsp;specs.&lt;/p&gt;</content><category term="serverspec"></category><category term="specinfra"></category><category term="testing"></category><category term="beaker"></category><category term="ruby"></category><category term="rspec"></category><category term="gem"></category></entry><entry><title>RSpec Matcher For Hash ItemÂ Value</title><link href="http://blog.jasonantman.com/2015/02/rspec-matcher-for-hash-item-value/" rel="alternate"></link><published>2015-02-21T10:33:00-05:00</published><updated>2015-02-21T10:33:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2015-02-21:/2015/02/rspec-matcher-for-hash-item-value/</id><summary type="html">&lt;p&gt;An RSpec matcher for hash item value&amp;nbsp;regex&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Well, this is embarassing. &lt;em&gt;After&lt;/em&gt; I posted this, I received a
&lt;a href="http://blog.jasonantman.com/2015/02/rspec-matcher-for-hash-item-value/#comment-1868422853"&gt;comment&lt;/a&gt;
within a few hours from &lt;a href="https://twitter.com/myronmarston"&gt;@myronmarston&lt;/a&gt;. I&amp;#8217;d originally
written this matcher for RSpec2, and then had to convert my project to use
RSpec3. I just blindly converted this matcher over. Myron pointed out that with
RSpec3&amp;#8217;s &lt;a href="http://rspec.info/blog/2014/01/new-in-rspec-3-composable-matchers/"&gt;composable matchers&lt;/a&gt;,
the functionality of this gem is built-in. It can be done as simply&amp;nbsp;as:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;its&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:headers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="n"&gt;should&lt;/span&gt; &lt;span class="kp"&gt;include&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;server&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="sr"&gt;/nginx\/1\./&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;As such, I&amp;#8217;ve yanked them gem and am leaving the code and blog post here just for posterity.&lt;/strong&gt;
This should probably not be&amp;nbsp;used.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve been working on a project to move my &lt;a href="http://linode.com"&gt;Linode&lt;/a&gt; &lt;span class="caps"&gt;VM&lt;/span&gt; to an
Amazon &lt;span class="caps"&gt;EC2&lt;/span&gt; instance; the entire instance is a &amp;#8220;baked&amp;#8221; &lt;span class="caps"&gt;AMI&lt;/span&gt; built by Puppet. Since
I&amp;#8217;d like to be able to rebuild this quickly, I&amp;#8217;m using &lt;a href="http://serverspec.org/"&gt;ServerSpec&lt;/a&gt;
(which I have some non-technical issues with, but that&amp;#8217;s a long story) to run full
integration tests of the whole system - check that packages are installed, services
are running, and even make live &lt;span class="caps"&gt;HTTP&lt;/span&gt; requests agsinst&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;One part of this was making live &lt;span class="caps"&gt;HTTP&lt;/span&gt; requests (from inside ServerSpec / &lt;a href="http://rspec.info/"&gt;rspec&lt;/a&gt;)
and checking &lt;span class="caps"&gt;HTTP&lt;/span&gt; response headers. Unfortunately, RSpec doesn&amp;#8217;t have a nice, clean way to make
assertions about a hash&amp;nbsp;item.&lt;/p&gt;
&lt;p&gt;So, I wrote a little Ruby Gem to do this, &lt;a href="https://github.com/jantman/rspec-matcher-hash-item"&gt;rspec-matcher-hash-item&lt;/a&gt;. At the moment it just
has one matcher, &lt;code&gt;have_hash_item_matching&lt;/code&gt;. This operates on a hash, and takes two arguments,
a key and a regex for the value. It allows me to do simple but useful things&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  &lt;span class="n"&gt;describe&lt;/span&gt; &lt;span class="n"&gt;http_get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;testapp1.jasonantman.com&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/testapp1234&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="n"&gt;its&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:headers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="n"&gt;should&lt;/span&gt; &lt;span class="n"&gt;have_hash_item_matching&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;server&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="sr"&gt;/nginx\/1\./&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(The &lt;code&gt;http_get&lt;/code&gt; serverspec matcher is coming in a future gem and blog&amp;nbsp;post)&lt;/p&gt;
&lt;p&gt;Among other things, it prints diffs on&amp;nbsp;failure:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;  2) privatepuppet::ec2::vhosts::testapp1 Http_get &amp;quot;&amp;quot; headers should include key &amp;#39;server&amp;#39; matching /badvalue/
     On host `54.149.198.147&amp;#39;
     Failure/Error: its(:headers) { should have_hash_item_matching(&amp;#39;server&amp;#39;, /badvalue/) }
       expected that hash[server] would match /badvalue/
       Diff:
       @@ -1,2 +1,6 @@
       -[&amp;quot;server&amp;quot;, /badvalue/]
       +&amp;quot;connection&amp;quot; =&amp;gt; &amp;quot;close&amp;quot;,
       +&amp;quot;content-type&amp;quot; =&amp;gt; &amp;quot;text/plain&amp;quot;,
       +&amp;quot;date&amp;quot; =&amp;gt; &amp;quot;Sat, 21 Feb 2015 16:07:42 &lt;span class="caps"&gt;GMT&lt;/span&gt;&amp;quot;,
       +&amp;quot;server&amp;quot; =&amp;gt; &amp;quot;nginx/1.6.2&amp;quot;,
       +&amp;quot;transfer-encoding&amp;quot; =&amp;gt; &amp;quot;chunked&amp;quot;,
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Using the gem is as simple as including it in your &lt;code&gt;Gemfile&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;gem &amp;quot;rspec-matcher-hash-item&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And adding a line to your &lt;code&gt;spec_helper.rb&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;require &amp;#39;rspec_matcher_hash_item&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that the gem is written for&amp;nbsp;RSpec3.&lt;/p&gt;
&lt;p&gt;This is available at &lt;a href="https://rubygems.org/gems/rspec-matcher-hash-item"&gt;rubygems.org&lt;/a&gt; or from
&lt;a href="https://github.com/jantman/rspec-matcher-hash-item"&gt;GitHub&lt;/a&gt;. See GitHub for the&amp;nbsp;documentation.&lt;/p&gt;</content><category term="ruby"></category><category term="rspec"></category><category term="spec"></category><category term="testing"></category></entry><entry><title>AWS CloudFormation and RDSÂ Snapshots</title><link href="http://blog.jasonantman.com/2014/12/aws-cloudformation-and-rds-snapshots/" rel="alternate"></link><published>2014-12-15T09:29:00-05:00</published><updated>2014-12-15T09:29:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-12-15:/2014/12/aws-cloudformation-and-rds-snapshots/</id><summary type="html">&lt;p&gt;Some tips, tricks and non-intuitive information about working with &lt;span class="caps"&gt;AWS&lt;/span&gt; CloudFormation and &lt;span class="caps"&gt;RDS&lt;/span&gt;&amp;nbsp;snapshots.&lt;/p&gt;</summary><content type="html">&lt;p&gt;For the past few weeks, I&amp;#8217;ve been working on spinning up a WordPress stack on Amazon &lt;span class="caps"&gt;AWS&lt;/span&gt;. It&amp;#8217;s intended to be a production application,
so it uses Multi-&lt;span class="caps"&gt;AZ&lt;/span&gt; and a few other tricks to try to achieve relatively high fault tolerance (nothing insane, still in one region). It uses
&lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217;s &lt;a href="https://aws.amazon.com/rds/"&gt;&lt;span class="caps"&gt;RDS&lt;/span&gt;&lt;/a&gt; hosted MySQL service for the database, and the stacks are created with &lt;a href="https://aws.amazon.com/cloudformation/"&gt;CloudFormation&lt;/a&gt;.
Using CloudFormation has been an utterly wonderful experience and being able to spin up an entire stack - multiple autoscaling web server
instances, a database, memcache, etc. with the click of a button in ~20 minutes - is as close to operations nirvana as I&amp;#8217;ve ever&amp;nbsp;gotten.&lt;/p&gt;
&lt;p&gt;One of the last steps for me was to work on database backups and restoration; both restoring the production application&amp;#8217;s database to a
previous snapshot, and restoring a production database snapshot to a test or development stack. This took a few days of testing, and I
wasn&amp;#8217;t able to find much complete information on the nuances of it; there are also some pieces that are not intuitive and (&lt;span class="caps"&gt;IMO&lt;/span&gt;) not
documented well enough in the &lt;span class="caps"&gt;AWS&lt;/span&gt; docs. In short, it&amp;#8217;s horribly easy to blow away your entire database. So, I&amp;#8217;m going to attempt to document
some of what I learned, in the hope that it will benefit&amp;nbsp;others.&lt;/p&gt;
&lt;p&gt;At the bottom of this post I&amp;#8217;ve included some snippets from my CloudFormation template, which I make reference to. It&amp;#8217;s probably worth looking
through that, as I make reference to some of the names used in it. Also, to make sense of this, you should be familiar with the nomenclature used by CloudFormation,
such as the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html"&gt;template anatomy&lt;/a&gt; and the difference between
parameters and properties, and resources and&amp;nbsp;instances.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I&amp;#8217;m writing this in mid-December 2014. I&amp;#8217;ll make every effort to keep this updated as I continue working with &lt;span class="caps"&gt;AWS&lt;/span&gt;, but it&amp;#8217;s possible
that some of the problems described herein will be fixed by &lt;span class="caps"&gt;AWS&lt;/span&gt; in the&amp;nbsp;future.&lt;/p&gt;
&lt;h2 id="deletionpolicy-snapshot"&gt;DeletionPolicy&amp;nbsp;Snapshot&lt;/h2&gt;
&lt;p&gt;CloudFormation resources support a &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html"&gt;DeletionPolicy&lt;/a&gt;
attribute that says what to do to a resource when deleted. For &lt;span class="caps"&gt;RDS&lt;/span&gt; instances, &amp;#8220;Snapshot&amp;#8221; is an option, which takes a manual snapshot when the resource
is deleted (manual snapshots, unlike the automated daily ones, live on even after the instance is deleted). Be warned, this only takes effect when you
delete the &lt;strong&gt;entire stack&lt;/strong&gt;. If you make a change to one of the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html"&gt;DBInstance properties&lt;/a&gt;
that requires a &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks.html#update-replacement"&gt;resource replacement&lt;/a&gt; to
take effect, the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance will be replaced with a new one, and all of the data and automatic snapshots from the old one will be deleted.
That last part deserves repeating: automatic snapshots (the daily ones created by &lt;span class="caps"&gt;RDS&lt;/span&gt;) are tied to the instance; if the instance is replaced
by CloudFormation, you lose all automatic (backup) snapshots with&amp;nbsp;it.&lt;/p&gt;
&lt;h2 id="stack-policy-to-prevent-updates"&gt;Stack Policy to Prevent&amp;nbsp;Updates&lt;/h2&gt;
&lt;p&gt;To prevent &lt;span class="caps"&gt;RDS&lt;/span&gt; data loss from accidentally changing a property of the instance, it&amp;#8217;s wise to add a
&lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html"&gt;stack policy to prevent updates to &lt;span class="caps"&gt;RDS&lt;/span&gt; resources&lt;/a&gt;.
This will prevent CloudFormation from making any changes to the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance at all. Once the stack policy
is in place, in order to make changes to the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance you would either need to set a temporary stack policy
to allow the update (see the &amp;#8220;Updating Protected Resources&amp;#8221; section of the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html"&gt;stack policy documentation&lt;/a&gt;)
or simply delete and re-create the stack (the recommended method, if it&amp;#8217;s feasible for&amp;nbsp;you).&lt;/p&gt;
&lt;p&gt;Setting a proper stack policy should prevent many of the pitfalls I describe below; however, for completeness,
I&amp;#8217;ve described how &lt;span class="caps"&gt;RDS&lt;/span&gt; resources behave currently without a stack policy protecting them. The
&lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html"&gt;&lt;span class="caps"&gt;AWS&lt;/span&gt;::&lt;span class="caps"&gt;RDS&lt;/span&gt;::DBInstance resource documentation&lt;/a&gt;
describes which properties can be updated in-place (&amp;#8220;Update requires: No interruption&amp;#8221; or &amp;#8220;some interruptions&amp;#8221;)
and which trigger complete replacement of the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance (&amp;#8220;Update requires:&amp;nbsp;replacement&amp;#8221;).&lt;/p&gt;
&lt;p&gt;When you try to update a protected resource through the &lt;code&gt;aws&lt;/code&gt; &lt;span class="caps"&gt;CLI&lt;/span&gt; tools, the update will appear to have worked, but the event
log on the stack will show the update denied and the update will be rolled&amp;nbsp;back.&lt;/p&gt;
&lt;h2 id="restoring-snapshots-and-dbname"&gt;Restoring Snapshots and&amp;nbsp;DBName&lt;/h2&gt;
&lt;p&gt;The DBSnapshotIdentifier property on a MySQL &lt;span class="caps"&gt;RDS&lt;/span&gt; instance specifies a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot to restore into the instance. The DBName
property will create a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with a single blank database of that name. This bears repeating again; if the DBName
property ever changes, your &lt;span class="caps"&gt;RDS&lt;/span&gt; instance will be replaced with one with a new, blank database of that name.
When creating a MySQL &lt;span class="caps"&gt;RDS&lt;/span&gt; instance, you can specify either the &lt;code&gt;DBName&lt;/code&gt; or &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; property, but not both;
if you attempt to specify both, you&amp;#8217;ll get an error, &amp;#8220;DBName must be null when Restoring for this&amp;nbsp;Engine.&amp;#8221;&lt;/p&gt;
&lt;p&gt;If you want to restore a snapshot to a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance, you&amp;#8217;ll need to ensure that &lt;code&gt;DBName&lt;/code&gt; is null (either not specified at all, or the special &lt;code&gt;AWS::NoValue&lt;/code&gt;
&lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html"&gt;pseudo parameter&lt;/a&gt;). In order
to do this automatically (and since NoValue/null can&amp;#8217;t be passed in as a template parameter), in the template snippet below I&amp;#8217;ve defined a
&lt;code&gt;UseDbSnapshot&lt;/code&gt; &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html"&gt;condition&lt;/a&gt;
that evaluates to true if the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter is not empty. In my &lt;code&gt;RDS::DBInstance&lt;/code&gt; resource,
I conditionally set (using &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-conditions.html#d0e42982"&gt;&lt;code&gt;Fn::If&lt;/code&gt;&lt;/a&gt;)
the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; and &lt;code&gt;DBName&lt;/code&gt; properties depending on the value of &lt;code&gt;UseDbSnapshot&lt;/code&gt;. The end result is that if the
&lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter is not empty, it is passed in as the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; property of the resource and
the &lt;code&gt;DBName&lt;/code&gt; property is set to &lt;code&gt;AWS::NoValue&lt;/code&gt;. Otherwise, the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; property is set to &lt;code&gt;AWS::NoValue&lt;/code&gt;
and the &lt;code&gt;DBName&lt;/code&gt; parameter is passed in to the corresponding property on the resource (indicating to create a new blank database
of that&amp;nbsp;name).&lt;/p&gt;
&lt;p&gt;To explain this a bit more, CloudFormation seems to have no introspection into &lt;span class="caps"&gt;RDS&lt;/span&gt; instances. The &lt;code&gt;DBName&lt;/code&gt; parameter
exists only in CloudFormation itself, and is only evaluated as a diff from the previous template; if it changes,
CloudFormation spins up a completely new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with a single blank database of that name. Whether or not
the value of &lt;code&gt;DBName&lt;/code&gt; matches the database currently in the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance (say, restored from a snapshot)
is not known by CloudFormation. In short, if you create an &lt;span class="caps"&gt;RDS&lt;/span&gt; instance from a snapshot of a &amp;#8220;foo&amp;#8221; database
and then change the template to have a &lt;code&gt;DBName&lt;/code&gt; of &amp;#8220;foo&amp;#8221;, CloudFormation will spin up a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance
with an empty &amp;#8220;foo&amp;#8221;&amp;nbsp;database.&lt;/p&gt;
&lt;h2 id="restoring-to-a-new-stack"&gt;Restoring to a New&amp;nbsp;Stack&lt;/h2&gt;
&lt;p&gt;When restoring to a new stack (stack creation), specify the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; and make sure &lt;code&gt;DBName&lt;/code&gt; is set
to &lt;code&gt;AWS::NoValue&lt;/code&gt; per the previous paragraph (condition in the template). Note that for the life of the stack, you
must continue specifying these parameters (or the &amp;#8220;use previous value&amp;#8221; option for them). Using my example template
below, if you restored into a new stack using the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter and then later updated the stack
and omitted that parameter (which, because of the condition, would set it to &lt;code&gt;NoValue&lt;/code&gt; and set the &lt;code&gt;DBName&lt;/code&gt; parameter
to its default value) the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance would be replaced with a new one with a blank&amp;nbsp;database.&lt;/p&gt;
&lt;p&gt;Because of this, stack updates should always use the previous value for the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter; this can
be done through the &lt;span class="caps"&gt;AWS&lt;/span&gt; Console, or using the &lt;code&gt;aws&lt;/code&gt; command line tools and a parameter like: &lt;code&gt;--parameters ParameterKey=DBSnapshotIdentifier,UsePreviousValue=true&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="restoring-to-an-existing-stack"&gt;Restoring to an Existing&amp;nbsp;Stack&lt;/h2&gt;
&lt;p&gt;Restoring a snapshot to an existing stack is a bit more nuanced. You can&amp;#8217;t restore a snapshot to an existing &lt;span class="caps"&gt;RDS&lt;/span&gt; instance,
you can only restore to a new instance. If you do this through the &lt;span class="caps"&gt;AWS&lt;/span&gt; Console, you&amp;#8217;ll end up with an &lt;span class="caps"&gt;RDS&lt;/span&gt; instance disconnected
from your CloudFormation stack. So the way to do this is more or less the same as restoring to a new stack - specify
the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter for your template, and it will create a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with the snapshot. The same
rules about using previous values for the parameters hold true. If you used a stack policy to prevent updates to the &lt;span class="caps"&gt;RDS&lt;/span&gt;
instance, you&amp;#8217;ll need to override that with a temporary policy when doing the&amp;nbsp;restore.&lt;/p&gt;
&lt;p&gt;There are a few caveats to keep in mind with this procedure. The first, obviously, is that there may be some application downtime
when the existing database is replaced with the new (restored) one, and any writes will obviously be lost. Also, this only
works on &lt;span class="caps"&gt;RDS&lt;/span&gt; instances that were created with DBName or a &lt;strong&gt;different&lt;/strong&gt; snapshot. In order to restore the same snapshot to
an &lt;span class="caps"&gt;RDS&lt;/span&gt; resource a second time, you need to first update with the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter removed and have the &lt;span class="caps"&gt;RDS&lt;/span&gt;
instance re-created with an empty database, and then update again with the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; in order to do the restore.
This is because CloudFormation doesn&amp;#8217;t reconcile the current state of instances to determine which actions to take, it only diffs
the updated template against the existing one. If the existing template and the updated one have the same value for the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance&amp;#8217;s
properties (specifically &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt;), CloudFormation determines there are no changes, and does&amp;nbsp;nothing.&lt;/p&gt;
&lt;h2 id="launchconfig-metadata-issues"&gt;LaunchConfig Metadata&amp;nbsp;Issues&lt;/h2&gt;
&lt;p&gt;The &lt;span class="caps"&gt;EC2&lt;/span&gt; instances I&amp;#8217;m using for this project are &amp;#8220;baked&amp;#8221; AMIs (built with &lt;a href="https://packer.io/"&gt;packer.io&lt;/a&gt;) in an Auto-Scaling Group (&lt;span class="caps"&gt;ASG&lt;/span&gt;).
They use a &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-launchconfig.html"&gt;LaunchConfig&lt;/a&gt; to write
out a file on disk with the database connection information for the application. In addition, my &lt;span class="caps"&gt;ASG&lt;/span&gt; has an &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html"&gt;UpdatePolicy&lt;/a&gt;
designed to perform rolling updates (termination and replacement) of &lt;span class="caps"&gt;EC2&lt;/span&gt; instances when their properties&amp;nbsp;change.&lt;/p&gt;
&lt;p&gt;In my testing, I noticed a number of times where updates to the &lt;span class="caps"&gt;RDS&lt;/span&gt; resource that triggered creation of a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance - such as restoring from
a snapshot in an existing stack, or changing the DBName - properly triggered an update of the LaunchConfig, but failed to trigger
the rolling update of the &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. This left the application in a state where one or more (sometimes all) of the &lt;span class="caps"&gt;EC2&lt;/span&gt;
instances couldn&amp;#8217;t connect to the database, because the file written out by the LaunchConfig still contained the old &lt;span class="caps"&gt;DB&lt;/span&gt; connection
information. For non-production stacks where the entire stack can be deleted and recreated instead of updating the &lt;span class="caps"&gt;RDS&lt;/span&gt; resource,
this shouldn&amp;#8217;t be an issue. Otherwise, if changes are made that replace the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance, I&amp;#8217;d recommend watching for the
LaunchConfig update completion, and manually terminating instances (or increasing the size of the &lt;span class="caps"&gt;ASG&lt;/span&gt; to add instances)
to ensure that the running &lt;span class="caps"&gt;EC2&lt;/span&gt; instances have the updated&amp;nbsp;LaunchConfig.&lt;/p&gt;
&lt;p&gt;Another option would be to use the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html"&gt;cfn-hup daemon&lt;/a&gt; to
listen for stack updates that cause changes in resource metadata, and perform the required actions without needing the rolling update
to replace the&amp;nbsp;instances.&lt;/p&gt;
&lt;h2 id="how-to-do-things-using-the-template-below"&gt;How to Do Things Using the Template&amp;nbsp;Below&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;m currently using the &lt;code&gt;aws&lt;/code&gt; command line tools to perform stack creation and updates,
wrapped in a Rakefile (I plan on changing this to use &lt;a href="https://github.com/boto/boto"&gt;boto&lt;/a&gt;
inside a &lt;a href="http://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; job). What follows is a quick high-level guide
on how to accomplish various &lt;span class="caps"&gt;RDS&lt;/span&gt;-related tasks, using the template snippet&amp;nbsp;below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Build a new stack using a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot and a stack policy to prevent updates&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ cat /tmp/stack_policy.json
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;Statement&amp;quot;&lt;/span&gt; : &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;Effect&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Deny&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Update:*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Principal&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Resource&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;LogicalResourceId/DBInstance&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;,
    &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;Effect&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Allow&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Update:*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Principal&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Resource&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
$ aws cloudformation create-stack --stack-name mystack --stack-policy-body file:///tmp/stack_policy.json --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;my-snapshot-identifier&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Temporarily override stack policy to allow updates&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a file with the following contents (we&amp;#8217;ll assume it&amp;#8217;s at &lt;code&gt;/home/myuser/allow_all_updates.json&lt;/code&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Statement&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Effect&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Allow&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Update:*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Principal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Resource&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the following &lt;code&gt;aws&lt;/code&gt; commands, append &lt;code&gt;--stack-policy-during-update-body file:///home/myuser/allow_all_updates.json&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Update a stack (built using a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot), without losing data&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,UsePreviousValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Load a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot into an existing stack&lt;/strong&gt; (that isn&amp;#8217;t already using this&amp;nbsp;snapshot):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;my-snapshot-identifier&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Load a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot into an existing stack again&lt;/strong&gt; (i.e. restore from the same snapshot a second time; this one is a&amp;nbsp;kludge):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;$ &lt;span class="c1"&gt;# re-create the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with a blank &lt;span class="caps"&gt;DB&lt;/span&gt; (DBName)&lt;/span&gt;
$ aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
$ &lt;span class="c1"&gt;# then load the snapshot again&lt;/span&gt;
$ aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;my-snapshot-identifier&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="cloudformation-template-snippet"&gt;CloudFormation Template&amp;nbsp;Snippet&lt;/h2&gt;
&lt;p&gt;This is by no means complete, but just includes the parameters, conditions, and resources which I make reference&amp;nbsp;to.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Parameters&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Default&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;wordpress&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Description&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;The WordPress database name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;String&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;MinLength&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;MaxLength&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;64&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;AllowedPattern&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;[a-zA-Z][a-zA-Z0-9]*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;ConstraintDescription&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;must begin with a letter and contain only alphanumeric characters.&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Description&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; The &lt;span class="caps"&gt;RDS&lt;/span&gt; MySQL snapshot name to restore to the new &lt;span class="caps"&gt;DB&lt;/span&gt; instance.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;String&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Default&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;quot;Conditions&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;UseDbSnapshot&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Fn::Not&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;Fn::Equals&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
          &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;]&lt;/span&gt;
      &lt;span class="p"&gt;}]&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;quot;Resources&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;DBInstance&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::&lt;span class="caps"&gt;RDS&lt;/span&gt;::DBInstance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Properties&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt;            &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Fn::If&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;UseDbSnapshot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::NoValue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;Engine&amp;quot;&lt;/span&gt;            &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;MySQL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;MasterUsername&amp;quot;&lt;/span&gt;    &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBUsername&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBInstanceClass&amp;quot;&lt;/span&gt;   &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBClass&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBSecurityGroups&amp;quot;&lt;/span&gt;  &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSecurityGroup&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}],&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBSubnetGroupName&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSubnetGroup&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;AllocatedStorage&amp;quot;&lt;/span&gt;  &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBAllocatedStorage&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;MasterUserPassword&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBPassword&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Fn::If&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;UseDbSnapshot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::NoValue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;MultiAZ&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;DeletionPolicy&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Snapshot&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;WebServerGroup&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::AutoScaling::AutoScalingGroup&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Properties&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;LaunchConfigurationName&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;LaunchConfig&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;UpdatePolicy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;AutoScalingRollingUpdate&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;MinInstancesInService&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;MaxBatchSize&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;WaitOnResourceSignals&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;PauseTime&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;PT10M&lt;/span&gt;&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;AutoScalingScheduledAction&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;IgnoreUnmodifiedGroupSizeProperties&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;CreationPolicy&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;ResourceSignal&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Timeout&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;PT10M&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Count&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;2&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;LaunchConfig&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::AutoScaling::LaunchConfiguration&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Metadata&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::CloudFormation::Init&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;config&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;files&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="nt"&gt;&amp;quot;/opt/wordpress/cloudformation_db.php&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;quot;content&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Fn::Join&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;&amp;lt;?php\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_NAME&amp;#39;,          &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_USER&amp;#39;,          &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBUsername&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_PASSWORD&amp;#39;,      &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBPassword&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_HOST&amp;#39;,          &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Fn::GetAtt&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;DBInstance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Endpoint.Address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]},&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
              &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="aws"></category><category term="cloudformation"></category><category term="rds"></category><category term="mysql"></category><category term="snapshot"></category></entry><entry><title>Watching Jenkins Jobs and CloudFormation Updates with PushoverÂ Notification</title><link href="http://blog.jasonantman.com/2014/12/watching-jenkins-jobs-and-cloudformation-updates-with-pushover-notification/" rel="alternate"></link><published>2014-12-14T19:22:00-05:00</published><updated>2014-12-14T19:22:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-12-14:/2014/12/watching-jenkins-jobs-and-cloudformation-updates-with-pushover-notification/</id><summary type="html">&lt;p&gt;Some scripts to watch the status of Jenkins jobs and CloudFormation updates, and send Pushover&amp;nbsp;notifications.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few months ago I &lt;a href="http://blog.jasonantman.com/2014/09/pushover-notifications-for-shell-command-completion-and-status/"&gt;posted&lt;/a&gt;
about a script I wrote to send &lt;a href="https://pushover.net/"&gt;Pushover&lt;/a&gt; notifications for shell command&amp;nbsp;completion.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve been doing quite a bit of work lately both with testing some &lt;a href="http://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; jobs, and spinning up
&lt;span class="caps"&gt;AWS&lt;/span&gt; stacks using &lt;a href="https://aws.amazon.com/cloudformation/"&gt;CloudFormation&lt;/a&gt;. Last week I wrote two python scripts to aid in&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jantman/misc-scripts/blob/master/watch_cloudformation.py"&gt;watch_cloudformation.py&lt;/a&gt; uses the popular &lt;a href="https://github.com/boto/boto"&gt;boto&lt;/a&gt;
Python &lt;span class="caps"&gt;AWS&lt;/span&gt; interface to list (and display) the events on a specified CloudFormation stack, and exit 0 or 1 when it finds a (&lt;span class="caps"&gt;CREATE&lt;/span&gt;|&lt;span class="caps"&gt;UPDATE&lt;/span&gt;)_(&lt;span class="caps"&gt;FAILED&lt;/span&gt;|&lt;span class="caps"&gt;COMPLETE&lt;/span&gt;) event.
It also optionally uses &lt;a href="https://pypi.python.org/pypi/python-pushover"&gt;python-pushover&lt;/a&gt; to send the notification to your devices via&amp;nbsp;Pushover.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jantman/misc-scripts/blob/master/watch_jenkins.py"&gt;watch_jenkins.py&lt;/a&gt; takes the &lt;span class="caps"&gt;URL&lt;/span&gt; to a Jenkins job or build, and uses
&lt;a href="https://pypi.python.org/pypi/python-jenkins"&gt;python-jenkins&lt;/a&gt; to poll the status of the build (or the latest build, if given a Job url)
and display the result when the build finishes, also optionally using python-pushover to send notifications to your&amp;nbsp;device.&lt;/p&gt;
&lt;p&gt;They&amp;#8217;re really quick-and-dirty scripts and might not be suitable for everyone&amp;#8217;s use case, but I took the time to write them,
so hopefully they&amp;#8217;ll be useful to someone&amp;nbsp;else.&lt;/p&gt;</content><category term="script"></category><category term="pushover"></category><category term="jenkins"></category><category term="hudson"></category><category term="aws"></category><category term="cloudformation"></category></entry><entry><title>Idea for a Generic Method to Communicate Repository/ProjectÂ Status</title><link href="http://blog.jasonantman.com/2014/12/idea-for-a-generic-method-to-communicate-repositoryproject-status/" rel="alternate"></link><published>2014-12-07T18:24:00-05:00</published><updated>2014-12-07T18:24:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-12-07:/2014/12/idea-for-a-generic-method-to-communicate-repositoryproject-status/</id><summary type="html">&lt;p&gt;Some ideas for generic methods of communicating the status of a project / source code repository to humans and&amp;nbsp;machines.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Update 2014-12-24: I actually did something with this. See &lt;a href="http://www.repostatus.org"&gt;repostatus.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, something funny, before my possibly-hair-brained&amp;nbsp;scheme:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.commitstrip.com/en/2014/11/25/west-side-project-story/"&gt;&lt;img alt="commitstrip.com &amp;quot;side project&amp;quot; comic strip" src="http://www.commitstrip.com/wp-content/uploads/2014/11/Strip-Side-project-650-finalenglish.jpg"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I know I&amp;#8217;m not alone in having a mess of &lt;a href="https://github.com/jantman?tab=repositories"&gt;repositories on GitHub&lt;/a&gt;; I personally have over 90, and they&amp;#8217;re
all in various states of &amp;#8220;doneness.&amp;#8221; Some are working and undergoing active development. Some should be
working, but I no longer use them (and sometimes lack &amp;#8220;things&amp;#8221; needed to use them, especially the case
with projects linked to specific hardware). Some of them were ideas that never took off; some of these
I intend on finishing, and some I never want to touch&amp;nbsp;again.&lt;/p&gt;
&lt;p&gt;While GitHub has a &lt;a href="https://help.github.com/articles/about-releases/"&gt;Releases&lt;/a&gt; feature, at best (where everyone
understands and follows &lt;a href="http://semver.org/"&gt;semantic versioning&lt;/a&gt;), it can only differentiate &amp;#8220;initial development&amp;#8221;
(prior to stable public release) versions from those after them. It may be an indication of the usability or completeness
of the software, but not of its current state of&amp;nbsp;maintenance.&lt;/p&gt;
&lt;p&gt;The questions that I&amp;#8217;d really like to be able to answer about a given project or repository&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the &amp;#8220;completeness&amp;#8221; of the code? Should it be usable, or is it functionally&amp;nbsp;incomplete?&lt;/li&gt;
&lt;li&gt;What is the status of development efforts? Is this actively developed, or supported (even if bugfix-only), or totally&amp;nbsp;abandoned?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I&amp;#8217;d like to be able to easily communicate this to people who come across my work, and also
track it for my own needs - I have enough repositories with barely-started concepts that I
occasionally forget about them. I&amp;#8217;d also, of course, like to be able to know this information
about other peoples&amp;#8217; work as&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;Ideally, I thought that this should be a GitHub feature, exposed via the &lt;span class="caps"&gt;API&lt;/span&gt; and the &lt;span class="caps"&gt;UI&lt;/span&gt;. However,
there are a number of problems with&amp;nbsp;that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It would require GitHub to implement the feature. Quite ironically, GitHub is &lt;a href="https://github.com/isaacs/github/issues/6"&gt;not very open&lt;/a&gt;
about issues and feature requests for their platform itself, and the only good way to suggest something is &lt;a href="https://github.com/isaacs/github"&gt;unofficial&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;It would be tied to GitHub directly. When the next big thing comes along, or for projects using other services (like Gitorious, or even non-git hosting),
it would be rendered&amp;nbsp;useless.&lt;/li&gt;
&lt;li&gt;The status really describes the code/project itself, not the GitHub repository per se, so it should live with the&amp;nbsp;code.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, I&amp;#8217;m brainstorming a straightforward semi-standardized way of communicating this information. Assuming
it&amp;#8217;s not implemented in GitHub itself, but rather becomes part of the repository content, that poses some
interesting questions for both what information is communicated and how to communicate it. What follows is
really my brainstorming and initial ideas. I&amp;#8217;d very much appreciate it if anyone who&amp;#8217;s interested submits
their ideas and comments. I fully intend to start using something like this for my own projects but, not to
be too arrogant, I think it&amp;#8217;s a useful idea and could benefit from some accepted&amp;nbsp;standard.&lt;/p&gt;
&lt;h2 id="what-to-communicate"&gt;What to&amp;nbsp;Communicate&lt;/h2&gt;
&lt;p&gt;The first question is what data to communicate. Ideally, this would be one of a standardized set of
repository/project status identifiers, along with a textual description that could be provided by
the author, for additional clarity. My humble suggestion (very much a &lt;span class="caps"&gt;WIP&lt;/span&gt;) of the possible statuses,
along with the suggested (canonical) description of their&amp;nbsp;meanings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Concept&lt;/strong&gt; - Minimal or no implementation has been done&amp;nbsp;yet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="caps"&gt;WIP&lt;/span&gt;&lt;/strong&gt; - Initial development is in progress, but there has not yet been a stable, usable release suitable for the&amp;nbsp;public.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Suspended&lt;/strong&gt; - A &lt;span class="caps"&gt;WIP&lt;/span&gt; project that has had work stopped for the time being; the author(s) intend on resuming&amp;nbsp;work.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Abandoned&lt;/strong&gt; - A &lt;span class="caps"&gt;WIP&lt;/span&gt; project that has been abandoned; the author(s) do not intend on continuing&amp;nbsp;development.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Active&lt;/strong&gt; - The project has reached a stable, usable state and is being actively&amp;nbsp;developed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inactive&lt;/strong&gt; - The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time&amp;nbsp;allows.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unsupported&lt;/strong&gt; - The project has reached a stable, usable state but the author(s) have ceased all work on it. A new maintainer may be&amp;nbsp;desired.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I assume that there might be some dissenting opinions on whether this list of statuses is complete, or perhaps too long.
However I feel that it&amp;#8217;s the minimum set required to describe a project along the two axes which I consider important:
usability (is the code here complete enough to &amp;#8220;work&amp;#8221; for something) and support/development status (is it being worked on,
or are there plans to do so in the future). I&amp;#8217;m certainly open to opinions on&amp;nbsp;this.&lt;/p&gt;
&lt;h2 id="how-to-communicate-it"&gt;How to Communicate&amp;nbsp;It&lt;/h2&gt;
&lt;p&gt;I view this as a more complex question technically, as doing this within the repository content (instead of in a GitHub &lt;span class="caps"&gt;API&lt;/span&gt;)
necessarily involves polluting that repository. My main two technical requirements (at least with my own intended use in mind)
are that the status be readable both by human and machine, and that the status should be available in one place within the
repository (i.e. in only one place for both humans and machines, and not requiring any&amp;nbsp;transformation).&lt;/p&gt;
&lt;p&gt;The best I&amp;#8217;ve been able to come up with so far is either including the status in a special file (likely a specially-named dotfile),
or including it in the &lt;span class="caps"&gt;README&lt;/span&gt;. The dotfile method is optimized for machine-reading - it would be a single file, likely named
&amp;#8220;.repostatus.org&amp;#8221;, with a simple specified format. It&amp;#8217;s easy and cheap for a machine to find and parse, and shouldn&amp;#8217;t be too cumbersome
to add. But it pollutes the repository with another file, and worse, it would be quite unlikely to be found by a human who isn&amp;#8217;t
familiar with this practice, so it loses a lot in terms of human readability and&amp;nbsp;intuitiveness.&lt;/p&gt;
&lt;p&gt;On the other hand, adding something special to the &lt;span class="caps"&gt;README&lt;/span&gt; file is much more human-centric. The &amp;#8220;something&amp;#8221; could be a simple
string or link, or even better, a &lt;a href="http://shields.io/"&gt;badge&lt;/a&gt;. It would appear clearly when rendered on GitHub, and should also appear anywhere
else the readme is rendered (i.e. in online documentation or in packages of the project). However, this poses a few&amp;nbsp;challenges:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It wouldn&amp;#8217;t necessarily be possible to have a status that&amp;#8217;s machine-readable but not rendered to the human observer. Sure, this
sort of goes against half of the purpose of this idea, but some people probably wouldn&amp;#8217;t want this extra piece of information
cluttering up their &lt;span class="caps"&gt;README&lt;/span&gt;. It&amp;#8217;s possible to put comments in &lt;a href="http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html#comments"&gt;rST&lt;/a&gt;,
but &lt;a href="http://stackoverflow.com/questions/4823468/store-comments-in-markdown-syntax"&gt;markdown support&lt;/a&gt; isn&amp;#8217;t nearly as reliable,
being a bit of a&amp;nbsp;hack.&lt;/li&gt;
&lt;li&gt;This is optimized for human readers. In order to be detected by machine, the repository would need to be searched for
a readme file (even assuming the convention of &amp;#8220;^&lt;span class="caps"&gt;README&lt;/span&gt;*&amp;#8221;, there&amp;#8217;s a myriad of possible file extensions that could be used),
which isn&amp;#8217;t necessarily a cheap operation (especially since it would require access to the file listing within the&amp;nbsp;repository).&lt;/li&gt;
&lt;li&gt;Furthermore, machine detection would need to be able to either parse the markup (if any), or do string search on the file
contents. Once again, a more expensive&amp;nbsp;operation.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="current-theory"&gt;Current&amp;nbsp;Theory&lt;/h2&gt;
&lt;p&gt;At the moment, I&amp;#8217;m leaning towards this theory of&amp;nbsp;implementation:&lt;/p&gt;
&lt;p&gt;Badges are placed in the project&amp;#8217;s &lt;span class="caps"&gt;README&lt;/span&gt; indicating the status. The badges would be sourced from specified URLs, served
by &lt;a href="http://repostatus.org"&gt;repostatus.org&lt;/a&gt; and linked to specified URLs describing the status (likely of the form
http://repostatus.org/1.0/#active). Machine determination of status would be made by a string match for one of
the specified status URLs - nothing more is needed. It would be simple enough to simply specify that, for machine
determination, the first file in the repository (sorted in lexicographical order) beginning with &amp;#8220;readme&amp;#8221; (case-insensitive) and containing
a matching &lt;span class="caps"&gt;URL&lt;/span&gt; determines the status. For human users, the badge image could be combined with descriptive alt-text, and
possibly followed by a more descriptive explanation, if the author chose so. This would eliminate the need for a fixed
set of possible readme file names, and the need for machine identification to be able to parse all possible&amp;nbsp;markups.&lt;/p&gt;
&lt;p&gt;The visual impact to the readme document (assuming it&amp;#8217;s rendered) would be minimal. Here are some quick takes on
a first set of badges, along with the alt text set on them (which could be changed by the user, or also included
in plain text next to the&amp;nbsp;badge).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img alt="Repo Status: Concept - Minimal or no implementation has been done yet." src="http://img.shields.io/badge/repo%20status-Concept-ffffff.svg"&gt; Repo Status: Concept - Minimal or no implementation has been done&amp;nbsp;yet.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: WIP - Initial development is in progress, but there has not yet been a stable, usable release suitable for the public." src="http://img.shields.io/badge/repo%20status-WIP-yellow.svg"&gt; Repo Status: &lt;span class="caps"&gt;WIP&lt;/span&gt; - Initial development is in progress, but there has not yet been a stable, usable release suitable for the&amp;nbsp;public.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: Suspended - A WIP project that has had work stopped for the time being; the author(s) intend on resuming work." src="http://img.shields.io/badge/repo%20status-Suspended-orange.svg"&gt; Repo Status: Suspended - A &lt;span class="caps"&gt;WIP&lt;/span&gt; project that has had work stopped for the time being; the author(s) intend on resuming&amp;nbsp;work.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: Abandoned - A WIP project that has been abandoned; the author(s) do not intend on continuing development." src="http://img.shields.io/badge/repo%20status-Abandoned-000000.svg"&gt; Repo Status: Abandoned - A &lt;span class="caps"&gt;WIP&lt;/span&gt; project that has been abandoned; the author(s) do not intend on continuing&amp;nbsp;development.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: Active - The project has reached a stable, usable state and is being actively developed." src="http://img.shields.io/badge/repo%20status-Active-brightgreen.svg"&gt; Repo Status: Active - The project has reached a stable, usable state and is being actively&amp;nbsp;developed.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: Inactive - The project has reached a stable, usable state and is no longer being actively developed; support/maintenance will be done as time allows." src="http://img.shields.io/badge/repo%20status-Inactive-yellowgreen.svg"&gt; Repo Status: Inactive - The project has reached a stable, usable state and is no longer being actively developed; support/maintenance will be done as time&amp;nbsp;allows.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: Unsupported - The project has reached a stable, usable state but the author(s) have ceased all work on it. A new maintainer may be desired." src="http://img.shields.io/badge/repo%20status-Unsupported-lightgrey.svg"&gt; Repo Status: Unsupported - The project has reached a stable, usable state but the author(s) have ceased all work on it. A new maintainer may be&amp;nbsp;desired.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the readme is (for some strange reason) primarily intended for a non-rendered view, it would be acceptable to
include just the &lt;span class="caps"&gt;URL&lt;/span&gt; to the status description, optionally with some human-readable&amp;nbsp;text.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll probably start using something like this for my personal projects. I intend on even writing up a spec
for the &lt;span class="caps"&gt;README&lt;/span&gt;-based variant, along with some formatting and parsing/machine identification rules. Any and
all comments are welcome. This is the result of a few hours&amp;#8217; sporadic thought one afternoon, so I&amp;#8217;m sure there
are some major issues I haven&amp;#8217;t realized yet. Please pass them along, or tell me if this is of any interest to&amp;nbsp;you.&lt;/p&gt;</content><category term="repository"></category><category term="project"></category><category term="git"></category><category term="github"></category></entry><entry><title>Managing EC2 SSH Keys - AnÂ Idea</title><link href="http://blog.jasonantman.com/2014/10/managing-ec2-ssh-keys-an-idea/" rel="alternate"></link><published>2014-10-04T11:59:00-04:00</published><updated>2014-10-04T11:59:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-10-04:/2014/10/managing-ec2-ssh-keys-an-idea/</id><summary type="html">&lt;p&gt;An idea on how to manage &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;SSH&lt;/span&gt; keys for a large number of&amp;nbsp;users&lt;/p&gt;</summary><content type="html">&lt;p&gt;At work, we have a bunch of &lt;span class="caps"&gt;EC2&lt;/span&gt; instances (currently hundreds, and growing quickly). We also have a bunch
(probably now around 100, counting contractors) of users. Some users - mainly engineers - need &lt;span class="caps"&gt;SSH&lt;/span&gt; access to all
of the &lt;span class="caps"&gt;EC2&lt;/span&gt; instances; many others only need access to their team&amp;#8217;s instances. While I usually advocate sanity checks
and training over access control for employees, many teams have expressed legitimate concern that they don&amp;#8217;t want
others on their instances; commands that are safe to run in dev/test (like loading test data) might be disastrous
on production instances. So, as part of our automation and tooling team, I&amp;#8217;ve been trying to come up with a way to manage
access to all these instances. Right now we have a single &amp;#8220;bastion&amp;#8221; (a.k.a. jump box / ssh gateway / keyhole) instance, with a single
shared used keyed to access every &lt;span class="caps"&gt;EC2&lt;/span&gt; instance; that doesn&amp;#8217;t scale and doesn&amp;#8217;t meet the security&amp;nbsp;requirements.&lt;/p&gt;
&lt;p&gt;What follows is one theory of mine on how to solve this problem. I&amp;#8217;ve been thinking about this for the past
day; this might not be the Right answer, and it&amp;#8217;s just a theory at this point, but I think it&amp;nbsp;works.&lt;/p&gt;
&lt;h1 id="requirements-and-assumptions"&gt;Requirements and&amp;nbsp;Assumptions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We have Active Directory as the one source of authentication/authorization truth, but it&amp;#8217;s only in the corporate
  network. For various reasons both technical and political, accessing it from &lt;span class="caps"&gt;AWS&lt;/span&gt; (whether directly, over &lt;span class="caps"&gt;VPN&lt;/span&gt;,
  via replication, or via data feeds to a separate &lt;span class="caps"&gt;LDAP&lt;/span&gt; infrastructure in &lt;span class="caps"&gt;EC2&lt;/span&gt;) is simply not&amp;nbsp;possible.&lt;/li&gt;
&lt;li&gt;We want to control &lt;span class="caps"&gt;SSH&lt;/span&gt; access to a bunch of instances. Some of them are persistent and some are ephemeral. Some
  are pre-baked AMIs in auto-scaling groups, with &lt;em&gt;no&lt;/em&gt; changes made outside the &lt;span class="caps"&gt;AMI&lt;/span&gt;. Some of them are persistent
  or semi-persistent instances that run Puppet every 30 minutes. Some of them are somewhat special, and can&amp;#8217;t be
  trivially torn&amp;nbsp;down.&lt;/li&gt;
&lt;li&gt;Most of our instances are in a &lt;span class="caps"&gt;VPC&lt;/span&gt;, and have proper security controls which include &lt;span class="caps"&gt;SSH&lt;/span&gt; access from only a specifically
  white-listed range of IPs. However, some instances are in &amp;#8220;&lt;span class="caps"&gt;EC2&lt;/span&gt; Classic&amp;#8221; and have &lt;span class="caps"&gt;SSH&lt;/span&gt; open to the world. We want a
  solution that also protects these&amp;nbsp;instances.&lt;/li&gt;
&lt;li&gt;We&amp;#8217;re mainly concerned with securing access from (a) users inadvertently accessing an instance they shouldn&amp;#8217;t be
  on, (b) outside/untrusted parties, and (c) former employees. We trust our employees within reason, and accept that,
  within our security stance, if an employee &lt;em&gt;really&lt;/em&gt; wants privilege escalation, they&amp;#8217;re going to get it. We&amp;#8217;re not
  overly concerned with protecting against determined, malicious users who already have some access but want&amp;nbsp;more.&lt;/li&gt;
&lt;li&gt;Our current process for security cleanup for former employees is largely based on corporate &lt;span class="caps"&gt;IT&lt;/span&gt; (or is it &lt;span class="caps"&gt;HR&lt;/span&gt;?) turning
  off their &lt;span class="caps"&gt;AD&lt;/span&gt; account. We want to minimize additional steps that need to be completed when someone has access&amp;nbsp;revoked.&lt;/li&gt;
&lt;li&gt;Any solution that we choose needs to be usable with self-service &lt;span class="caps"&gt;AWS&lt;/span&gt;; i.e. any user can spin up their own instances
  or stacks, provided that they use an &lt;span class="caps"&gt;AMI&lt;/span&gt; that is either built by our automation team, or follows guidelines on what
  must be included in all&amp;nbsp;AMIs.&lt;/li&gt;
&lt;li&gt;We have some administrative accounts (Jenkins, as well as some shared privileged accounts on select machines) that need
  unrestricted access to&amp;nbsp;everything.&lt;/li&gt;
&lt;li&gt;Local user accounts aren&amp;#8217;t an option. This would mean running Puppet constantly on every image and/or rebuilding
  every image each time we gain or lose an employee. That would be especially difficult when we occasionally have
  project-based&amp;nbsp;contractors.&lt;/li&gt;
&lt;li&gt;We&amp;#8217;re &lt;span class="caps"&gt;OK&lt;/span&gt; with having a bastion/keyhole server in &lt;span class="caps"&gt;AWS&lt;/span&gt;, we just don&amp;#8217;t want everyone to be able to access&amp;nbsp;everything.&lt;/li&gt;
&lt;li&gt;Our intended network security stance is to have bastion/keyhole servers in &lt;span class="caps"&gt;AWS&lt;/span&gt; (ideally one per &lt;span class="caps"&gt;AZ&lt;/span&gt;), which are only
  reachable via &lt;span class="caps"&gt;SSH&lt;/span&gt; from selected public addresses on our corporate network (which can only be reached by current
  employees with valid, working access). All other instances should only allow &lt;span class="caps"&gt;SSH&lt;/span&gt; from these selected&amp;nbsp;hosts.&lt;/li&gt;
&lt;li&gt;Despite the above, we don&amp;#8217;t want to rely on an instance being properly configured as our only security measure;
  if an instance is incorrectly configured to accept &lt;span class="caps"&gt;SSH&lt;/span&gt; from 0.0.0.0/0, we still want to prevent users whose
  access has been revoked from logging in to the&amp;nbsp;instance.&lt;/li&gt;
&lt;li&gt;We don&amp;#8217;t need access to be granted and revoked immediately. We&amp;#8217;ll assume that in normal operating conditions,
  thirty (30) minutes is a reasonable amount of time to either grant or revoke a user&amp;#8217;s&amp;nbsp;access.&lt;/li&gt;
&lt;li&gt;We want to minimize reliance on our existing corporate infrastructure, so that &lt;span class="caps"&gt;AWS&lt;/span&gt; can be used for business
  continuity&amp;nbsp;purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="main-goals"&gt;Main&amp;nbsp;Goals&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Provide users with &lt;span class="caps"&gt;SSH&lt;/span&gt; access to &lt;span class="caps"&gt;EC2&lt;/span&gt; servers; privilege should be able to be granted to a subset of users and/or groups
  for each &amp;#8220;application&amp;#8221;. Users should not be able to access other&amp;nbsp;instances.&lt;/li&gt;
&lt;li&gt;Allow a fixed list of users access to every&amp;nbsp;instance.&lt;/li&gt;
&lt;li&gt;Be able to revoke a user&amp;#8217;s access without rebuilding instances or ssh-in-a-loop&amp;#8217;ing to all of&amp;nbsp;them.&lt;/li&gt;
&lt;li&gt;Many instances are not going to be running Puppet after initial provisioning/&lt;span class="caps"&gt;AMI&lt;/span&gt; creation, so as much as we love Puppet,
  it&amp;#8217;s not an option to solve this&amp;nbsp;problem.&lt;/li&gt;
&lt;li&gt;This should involve a minimum of administrative overhead when a user leaves the&amp;nbsp;company.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="proposed-solution"&gt;Proposed&amp;nbsp;Solution&lt;/h1&gt;
&lt;p&gt;My solution relies on &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding and the &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt; introduced in OpenSSH 6.2 (see &amp;#8220;Limitations&amp;#8221;, below, for more information),
most likely inspired by (or maybe literally the same code)
as the patch formerly used by GitHub. This allows sshd to execute an arbitrary command, passing it the login username, which returns output identical to what would
be in the &lt;code&gt;authorized_keys&lt;/code&gt; file. If none of the keys successfully authenticate the user, authentication continues using the usual &lt;code&gt;AuthorizedKeysFile&lt;/code&gt;. We take
advantage of this feature, in addition to &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding, to provide our granular access control. Public keys are pulled from a central location &lt;em&gt;at login time&lt;/em&gt;
(and cached for a set amount of time); each user has control over their own public keys, and a central process builds sets of public keys authorized to access a given
group of&amp;nbsp;instances.&lt;/p&gt;
&lt;h2 id="infrastructure"&gt;Infrastructure&lt;/h2&gt;
&lt;p&gt;Each &lt;span class="caps"&gt;EC2&lt;/span&gt; instance will be a member of an Access Group, which is a unique identifier for the set of users authorized to access instances
in the group. In implementation, Access Groups will likely just be a tag on &lt;span class="caps"&gt;EC2&lt;/span&gt; instances that maps to a set of predefined values
(see below for&amp;nbsp;more).&lt;/p&gt;
&lt;p&gt;We will have a number of &amp;#8220;bastion&amp;#8221; (keyhole/jump box/&lt;span class="caps"&gt;SSH&lt;/span&gt; gateway) hosts, ideally one in each Availability Zone where we have instances.
These bastion hosts will only be reachable from within our corporate network (or our &lt;span class="caps"&gt;VPN&lt;/span&gt;); therefore, users must have
current access to our corporate network (where we can rely on Active Directory and other systems to handle authorization) in order to
gain access to &lt;span class="caps"&gt;AWS&lt;/span&gt;. All other &lt;span class="caps"&gt;EC2&lt;/span&gt; instances will only be reachable over &lt;span class="caps"&gt;SSH&lt;/span&gt; from one of these bastion hosts. The bastion hosts themselves
will not have &lt;span class="caps"&gt;SSH&lt;/span&gt; keys to access other instances; they will, however, have &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding&amp;nbsp;enabled.&lt;/p&gt;
&lt;p&gt;Users reach &lt;span class="caps"&gt;AWS&lt;/span&gt; instances by SSHing from a host attached to our corporate network (including &lt;span class="caps"&gt;VPN&lt;/span&gt; hosts) to a bastion host in &lt;span class="caps"&gt;EC2&lt;/span&gt;. From there,
they &lt;span class="caps"&gt;SSH&lt;/span&gt; to the destination instance, making use of &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding to use their local key to authenticate to the instance. We get both
a restricted entry point to &lt;span class="caps"&gt;AWS&lt;/span&gt; (the bastion host, which can enforce further security and logging methods) and the ability to authenticate users
using their own personal public keys on the destination&amp;nbsp;instances.&lt;/p&gt;
&lt;p&gt;To make it easier for end-users, we could develop a wrapper script like &lt;a href="https://pypi.python.org/pypi/ec2-ssh"&gt;Instagram&amp;#8217;s ec2-ssh&lt;/a&gt; that
checks for a valid, running ssh agent with keys in it, and then crafts the correct &lt;span class="caps"&gt;SSH&lt;/span&gt; command to land the user on the desired end
host - i.e. something like &lt;code&gt;ec2ssh instance_id&lt;/code&gt; would generate and execute a command like &lt;code&gt;ssh -At bastion_hostname 'ssh instance_ip'&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="on-the-servers-instances"&gt;On the Servers&amp;nbsp;(Instances)&lt;/h2&gt;
&lt;p&gt;Each instance, when initially built/provisioned, is given a &lt;code&gt;get_authorized_keys&lt;/code&gt; script, which is configured to be run by sshd as the
&lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;. This script uses one of the following three public key distribution services to retrieve the authorized public keys
for that instance, which are then echoed on &lt;span class="caps"&gt;STDOUT&lt;/span&gt; and used to authenticate the user. For the sake of simplicity, we&amp;#8217;ll assume (which is
currently the case in our infrastructure) that this script will only run for a single non-root user that is used for logins; it will exit
without returning any output for any other users on the system, effectively preventing logins to&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;The script will first check for authorized keys cached locally (either on disk or in memory, to be determined). If they&amp;#8217;re found and less
than some age threshold (we&amp;#8217;ll say five minutes), the cached version is returned. This is intended to both reduce latency when performing
multiple sequential logins, and to allow logins to continue functioning through short periods of degraded network connectivity. If no recent
keys are found cached on disk, the script will retrieve them from the configured public key distribution service. If the service does not
return an appropriate response within an acceptable time limit, or is unreachable, the script will exit with no output. This will prevent
logins from users authorized with this method, but will fall through to the standard &lt;code&gt;AuthorizedKeysFile&lt;/code&gt; method. A number of permanent
authorized public keys will be included in each instance, to allow emergency administrative access in the event that the key distribution
service&amp;nbsp;fails.&lt;/p&gt;
&lt;p&gt;If we&amp;#8217;re willing to assume that the instances themselves are trusted (which I think is a valid assumption), the key retrieval script on
each instance will determine the Access Group that the instance belongs to, and then request the authorized keys for that Access Group.
Determination of Access Group will likely be made via user data passed into the instance at provisioning time, or via retrieval of a
tag value for the&amp;nbsp;instance.&lt;/p&gt;
&lt;p&gt;If assuming trust locally on the instance is not sufficient, then the burden of identifying the instance&amp;#8217;s access group is shifted
to the key distribution service (likely by identifying the &lt;span class="caps"&gt;IP&lt;/span&gt; address of the requesting instance, and then using the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; to
determine which group that instance belongs to). With this solution, only the second alternative key distribution service is&amp;nbsp;feasible.&lt;/p&gt;
&lt;p&gt;If a shorter delay to authorization changes is needed, it would be feasible for instances to also run a separate process
(cronjob, daemon, etc.) that polls the key distribution service at a regular interval to check for updates (i.e.
&lt;span class="caps"&gt;HTTP&lt;/span&gt; &lt;span class="caps"&gt;HEAD&lt;/span&gt;, something &lt;span class="caps"&gt;SQS&lt;/span&gt;-based, etc.) and updates the local cache when they&amp;nbsp;occur.&lt;/p&gt;
&lt;h1 id="public-key-distribution-service"&gt;Public Key Distribution&amp;nbsp;Service&lt;/h1&gt;
&lt;p&gt;Instances will retrieve their authorized public keys from a key distribution service. Three examples&amp;nbsp;follow:&lt;/p&gt;
&lt;h2 id="alternative-1-scalable-architecture-aws-and-local"&gt;Alternative 1 - Scalable Architecture - &lt;span class="caps"&gt;AWS&lt;/span&gt; and&amp;nbsp;Local&lt;/h2&gt;
&lt;p&gt;Keys will be managed by a web-based application (with a complete and documented &lt;span class="caps"&gt;API&lt;/span&gt;) living in the corporate data center.
The application will provide facilities for authorized users (managers, operations) to define new Access Groups and modify
the list of users allowed to access them. Individual end-users will be able to manage their public keys. At a set interval,
a standalone script will retrieve a list of all users defined in the application and check the status of their corporate Active
Directory accounts. Any users whose accounts have been deactivated or locked will be flagged as such in the application. Whenever
a change is made in the application (including a user being flagged as deactivated), all Access Groups that include that user
will have their authorized_keys file (composed of the authorized_keys files of all users with access) written to an S3 bucket
that&amp;#8217;s only writable by the privileged
user running the application. All instances will have &lt;span class="caps"&gt;IAM&lt;/span&gt; roles that allow them to read the&amp;nbsp;bucket.&lt;/p&gt;
&lt;p&gt;This method allows us to provide self-service to users and application administrators, and keeps all data about users within
the corporate network. It provides automatic revocation of access for disabled Active Directory accounts. It does introduce
a delay in revocation of access for disabled &lt;span class="caps"&gt;AD&lt;/span&gt; accounts, but a delay of ~10 minutes is certainly not a concern in our&amp;nbsp;environment.&lt;/p&gt;
&lt;h2 id="alternative-2-scalable-architecture-entirely-in-aws"&gt;Alternative 2 - Scalable Architecture Entirely in&amp;nbsp;&lt;span class="caps"&gt;AWS&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;A similar application exists, but lives entirely in &lt;span class="caps"&gt;AWS&lt;/span&gt;, utilizing its native high availability technologies (i.e. multi-&lt;span class="caps"&gt;AZ&lt;/span&gt;
&lt;span class="caps"&gt;RDS&lt;/span&gt; as a data store). A script still runs in the corporate data center, but all it does is query the &lt;span class="caps"&gt;API&lt;/span&gt; for a list of all
active users, check &lt;span class="caps"&gt;AD&lt;/span&gt; account status, and deactivate any users that no longer have a valid account. Instead of writing the
authorized key files to an S3 bucket, the application serves them directly in real-time. The application could
store keys and data in a &lt;span class="caps"&gt;RDBMS&lt;/span&gt;, or perhaps something like OpenLDAP, depending on which technologies are best known and
what the performance requirements&amp;nbsp;are.&lt;/p&gt;
&lt;p&gt;This is more of an infrastructure challenge and introduces additional points for failure; if the application above (1)
fails, it will only impact &lt;em&gt;changes&lt;/em&gt; to access, whereas if this application fails, all user access (aside from the static
emergency keys) will break. However, this method allows us to control access at a level finer than Access Groups; rules
could be developed based on any attributes of the requesting instance, including (if the latency was allowable) queries
to the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; for instance-specific&amp;nbsp;data.&lt;/p&gt;
&lt;h2 id="alternative-3-simple-architecture"&gt;Alternative 3 - Simple&amp;nbsp;Architecture&lt;/h2&gt;
&lt;p&gt;A text file stores mappings of Access Groups to the Active Directory users and groups authorized for them. The text file
is manually maintained, stored in version control, and all changes must comply with an access policy and be peer-reviewed.
A script runs at a set interval (let&amp;#8217;s say cron every 5-10 minutes) that reads the user/group mapping, translates groups
to their membership list, and checks the &lt;span class="caps"&gt;AD&lt;/span&gt; account status of every listed user. Users without valid/current/enabled accounts
are removed from the lists in memory. For the remaining (active) users for each Access Group, their &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt;
file is read. All user&amp;#8217;s authorized_keys files are concatenated together per Access Group, and the result is written to
an S3&amp;nbsp;bucket.&lt;/p&gt;
&lt;p&gt;This is by far the simplest method, and relies on our &lt;span class="caps"&gt;NFS&lt;/span&gt; shared home directories to allow users to manage their public
keys by simply using the standard file. This keeps all user-related data in our corporate data center, and means that we
have only one script and its&amp;#8217; cron job to maintain, rather than a whole application. The text-file-based method of access
control isn&amp;#8217;t terribly scalable, but it should work for the ~100 users that we have to deal with. Checking &lt;span class="caps"&gt;AD&lt;/span&gt; account status
when generating the file should provide a feasible safeguard for users whose corporate accounts are locked/revoked without
requiring someone to remember to also remove them from the &lt;span class="caps"&gt;AWS&lt;/span&gt; user&amp;nbsp;list.&lt;/p&gt;
&lt;h2 id="advantages-over-other-solutions"&gt;Advantages Over Other&amp;nbsp;Solutions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Self-service for users and for managers/administrators of&amp;nbsp;applications.&lt;/li&gt;
&lt;li&gt;No manual intervention when a user leaves the company; users automatically deactivated when their &lt;span class="caps"&gt;AD&lt;/span&gt; account&amp;nbsp;is.&lt;/li&gt;
&lt;li&gt;No cron job or daemon to run on instances, and no centralized process to break key distribution; each instance
  automatically pulls the current authorized keys when a login is&amp;nbsp;attempted.&lt;/li&gt;
&lt;li&gt;Doesn&amp;#8217;t depend on Puppet, so it allows individual applications to use Puppet as they desire, without complication
  or&amp;nbsp;confusion.&lt;/li&gt;
&lt;li&gt;Only depends on centralized (corporate data center) infrastructure for key updates (at most). Failure of connectivity
  between &lt;span class="caps"&gt;AWS&lt;/span&gt; and the corporate data center can be worked around assuming there is an alternate path of access (such as
  a bastion host that allows logins from engineers/managers from a trusted outside&amp;nbsp;host).&lt;/li&gt;
&lt;li&gt;Management of access can be delegated to application owners/managers, while still allowing engineers full&amp;nbsp;access.&lt;/li&gt;
&lt;li&gt;Uses the strength of public key authentication; no passwords to&amp;nbsp;change.&lt;/li&gt;
&lt;li&gt;Ensures that select static trusted keys always have access to instances, even during a failure of the key distribution&amp;nbsp;system.&lt;/li&gt;
&lt;li&gt;In emergencies, keys could be distributed directly to the authorized_keys file, bypassing the distribution system,
  or key file cache lifetime could be&amp;nbsp;increased.&lt;/li&gt;
&lt;li&gt;Can be easily audited by having a scheduled job add a key for all instances, wait ~15 minutes, and then attempt &lt;span class="caps"&gt;SSH&lt;/span&gt;
  connections to all&amp;nbsp;instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="trade-offs"&gt;Trade-Offs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Delay between user access addition/removal and updates (though this can be minimized by a shorter cache&amp;nbsp;time).&lt;/li&gt;
&lt;li&gt;Latency during initial login with a cold&amp;nbsp;cache.&lt;/li&gt;
&lt;li&gt;Addition of another system that could&amp;nbsp;break.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="limitations"&gt;Limitations&lt;/h2&gt;
&lt;p&gt;My company is a CentOS shop. The &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt; feature of OpenSSH itself was only released in &lt;a href="http://www.openssh.com/txt/release-6.2"&gt;OpenSSH 6.2&lt;/a&gt;,
on March 22, 2013. A patch for it was backported to the 5.3p1 version of openssh-server in &lt;span class="caps"&gt;RHEL&lt;/span&gt; and CentOS 6. However,
this method will certainly not work on CentOS 5, which is still running OpenSSH 4.3. Be aware that when the new &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;
feature was backported, the man page was not updated; &lt;code&gt;man sshd_config&lt;/code&gt; is still conspicuously missing these options, and I couldn&amp;#8217;t
find anything in the &lt;span class="caps"&gt;RPM&lt;/span&gt; changelog about it, but the &lt;code&gt;openssh-5.3p1-authorized-keys-command.patch&lt;/code&gt; file is clearly there in the
5.3p1 &lt;span class="caps"&gt;SRPM&lt;/span&gt;, and the options are there but commented out in the &lt;code&gt;sshd_config&lt;/code&gt; it provides. I actually thought this would be near-impossible
to do on CentOS 6 until I found the &lt;code&gt;openssh-ldap&lt;/code&gt; package (in the default repos) and discovered that it uses this&amp;nbsp;feature.&lt;/p&gt;
&lt;p&gt;Also, this solution requires (depending on which alternative is chosen) working access to either S3 or instances serving an application.
Assuming proper configuration (and distribution across AZs) this should be a&amp;nbsp;non-issue.&lt;/p&gt;
&lt;h2 id="accountability"&gt;Accountability&lt;/h2&gt;
&lt;p&gt;If accountability is a concern, we will handle this through detailed logging in every step of the key creation, authorization, distribution
and retrieval process. In addition, all instances will run sshd with &lt;code&gt;LogLevel VERBOSE&lt;/code&gt;, which will log the fingerprint of all public keys
used to connect to the instance. Logs will be written to a secure, append-only&amp;nbsp;medium.&lt;/p&gt;
&lt;h1 id="references-and-further-details"&gt;References and Further&amp;nbsp;Details&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;There is an existing &lt;code&gt;openssh-ldap&lt;/code&gt; package in CentOS that provides instructions on setting up public key storage in an &lt;span class="caps"&gt;LDAP&lt;/span&gt; backend,
  using &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://andriigrytsenko.net/2013/05/authorizedkeyscommand-support-and-centosrhel-5-x/"&gt;Someone said&lt;/a&gt; they successfully built the current
  6.2 OpenSSH for &lt;span class="caps"&gt;RHEL&lt;/span&gt;/Cent&amp;nbsp;5.&lt;/li&gt;
&lt;li&gt;An &lt;span class="caps"&gt;EC2&lt;/span&gt; instance can retrieve its own tags using tools such as &lt;code&gt;awscli&lt;/code&gt; or &lt;code&gt;ec2-api-tools&lt;/code&gt; and an appropriate &lt;span class="caps"&gt;IAM&lt;/span&gt; role set on the&amp;nbsp;instance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="rejected-ideas"&gt;Rejected&amp;nbsp;Ideas&lt;/h1&gt;
&lt;p&gt;While thinking through this I considered and rejected a number of alternate methods. Here are some of&amp;nbsp;them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While &lt;span class="caps"&gt;SSH&lt;/span&gt;&amp;#8217;s relatively new Certificate support (&lt;span class="caps"&gt;CA&lt;/span&gt;-based) sounds nice, it doesn&amp;#8217;t solve the problem; according to
  &lt;a href="http://neocri.me/documentation/using-ssh-certificate-authentication/"&gt;this blog post&lt;/a&gt; it uses a &lt;span class="caps"&gt;CA&lt;/span&gt; to sign keys,
  but doesn&amp;#8217;t do a &lt;span class="caps"&gt;CRL&lt;/span&gt; lookup, it relies on a RevokedKeys file manually sync&amp;#8217;ed to all servers. So, this poses the
  same problem as managing authorized_keys as a file distributed to&amp;nbsp;instances.&lt;/li&gt;
&lt;li&gt;Managing per-application users or groups on the &lt;span class="caps"&gt;AWS&lt;/span&gt; bastion hosts requires a lot of administrative overhead, and isn&amp;#8217;t really an option for us.
  Though this would be a simple implementation using either groups for each application with private keys group-readable,
  or using per-application users and the proper sudo&amp;nbsp;configuration.&lt;/li&gt;
&lt;li&gt;Prior to finding out about &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;, my top idea was essentially this same implementation on the
  key distribution server side, but writing it to an S3 bucket, and running a cronjob on each &lt;span class="caps"&gt;EC2&lt;/span&gt; instance to pull
  down the authorized_keys&amp;nbsp;file.&lt;/li&gt;
&lt;li&gt;Just Don&amp;#8217;t - See &lt;a href="https://wblinks.com/notes/aws-tips-i-wish-id-known-before-i-started/"&gt;this blog post&lt;/a&gt;
  as a reference. But the gist is, &amp;#8220;If you have to &lt;span class="caps"&gt;SSH&lt;/span&gt; into your servers, then your automation has failed&amp;#8221;.
  Sure, development and test stacks will be spun up, probably with either a single user&amp;#8217;s key, or a shared
  key. But after that (i.e. in prod), instances are cattle. Logs should be shipped to a central store, CloudWatch
  and/or other monitoring technologies (i.e. NewRelic, Diamond to graphite) should get most of the data that&amp;#8217;s
  needed. I&amp;#8217;m not seriously agreeing to &lt;strong&gt;disable&lt;/strong&gt; &lt;span class="caps"&gt;SSH&lt;/span&gt; access, but to put in place the tools that it&amp;#8217;s needed
  so rarely (on non-dev instances) that it&amp;#8217;s feasible to ask one of a small group of privileged people to
  perform the&amp;nbsp;task.&lt;/li&gt;
&lt;li&gt;Trust our users - If someone can push to master, full control of our systems is just a backtick (or popen) away.
  Recognize that if someone wasn&amp;#8217;t trustworthy, we wouldn&amp;#8217;t hire them. Let everyone access a single bastion host.
  Discourage unauthorized use via strong password policies and other standard security measures
  (perhaps &lt;span class="caps"&gt;OTP&lt;/span&gt;-based two-factor authentication). Discourage malicious use via detailed audit logging, with logs
  shipped to an append-only secure storage&amp;nbsp;location.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;SUID&lt;/span&gt; wrapper script - All users have &lt;span class="caps"&gt;SSH&lt;/span&gt; access to a bastion host as their normal
  active directory user. They run a &lt;span class="caps"&gt;SUID&lt;/span&gt; wrapper script that has a list of which users are allowed to access
  which &lt;span class="caps"&gt;EC2&lt;/span&gt; instances (or security groups, subnets, etc). When the user calls this script, it checks if the
  specified host is in a group they&amp;#8217;re allowed to access, and if so, SSHes to that host using a key only readable
  by the owner of the script. This is somewhat complex; there&amp;#8217;s a good possibility of security issues with the
  script itself, and it means that we&amp;#8217;re probably only allowing interactive logins - we&amp;#8217;re limited by the
  capabilities of the wrapper script, it&amp;#8217;s not just a normal &lt;span class="caps"&gt;SSH&lt;/span&gt;&amp;nbsp;client.&lt;/li&gt;
&lt;li&gt;Key Pushing- A script runs in one central location. It has a mapping of which users/groups are allowed
  to access which &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. Every X minutes the script runs. It grabs &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; for all
  users that are allowed &lt;span class="caps"&gt;EC2&lt;/span&gt; access, and then generates an authorized_keys file for each group of instances.
  The script checks a cache, and if the file has changed for a group of instances since the last run, it queries
  the &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; to determine which instances are in that group, and distributes the authorized_keys file to them.
  The &amp;#8220;distributes&amp;#8221; part would, unfortunately, probably have to be&amp;nbsp;scp.&lt;/li&gt;
&lt;li&gt;Bastion host per application. Users are allowed access to this host either via authorized_keys managed by Puppet,
  or via sudoers rules on a bastion host in the corporate network. But yeah, we&amp;#8217;d end up with a &lt;strong&gt;lot&lt;/strong&gt; of&amp;nbsp;these.&lt;/li&gt;
&lt;li&gt;Various thoughts around &lt;span class="caps"&gt;AD&lt;/span&gt; in the cloud, replicated &lt;span class="caps"&gt;AD&lt;/span&gt; in the cloud, OpenLDAP in the cloud pulling from &lt;span class="caps"&gt;AD&lt;/span&gt;, or
  &lt;span class="caps"&gt;AD&lt;/span&gt; over &lt;span class="caps"&gt;VPN&lt;/span&gt;. These were all rejected either because of corporate security policies, or because relying on internal
  &lt;span class="caps"&gt;AD&lt;/span&gt; for authentication would mean that a data center or connectivity failure also affects&amp;nbsp;&lt;span class="caps"&gt;AWS&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Puppet - We actually &lt;em&gt;run&lt;/em&gt; puppet on every instance. Maybe against our master, maybe masterless with a script
  to deploy some modules before every run. At a minimum, it manages ssh authorized keys for ec2_user. We implement
  some method where each user has a manifest with their own public keys, that they can maintain. Managers can add users
  to the group(s) for their applications, and that users&amp;#8217; keys are automatically deployed. Revoking keys, on the other
  hand, is a bigger problem. This requires some sort of &amp;#8220;this person is going away&amp;#8221; procedure, which currently doesn&amp;#8217;t
  exist (or involve the groups who maintain &lt;span class="caps"&gt;AWS&lt;/span&gt; infrastructure), and would be one more thing for a human to forget.
  There are also instances that have &amp;#8220;special stuff&amp;#8221; going on with Puppet that would complicate&amp;nbsp;this.&lt;/li&gt;
&lt;li&gt;Generate a list of authorized keys, turn it into a manifest, and run puppet masterless on it via a cronjob (pulling
  the manifest from S3). This involves most of the same problems as above, plus means that we have Puppet running
  in two different ways on some instances (triggered via mco against a master, and cron&amp;#8217;ed in apply&amp;nbsp;mode).&lt;/li&gt;
&lt;/ul&gt;</content><category term="ssh"></category><category term="ec2"></category><category term="aws"></category><category term="keys"></category><category term="public key"></category><category term="pubkey"></category></entry><entry><title>Pushover Notifications for Shell Command Completion andÂ Status</title><link href="http://blog.jasonantman.com/2014/09/pushover-notifications-for-shell-command-completion-and-status/" rel="alternate"></link><published>2014-09-27T21:20:00-04:00</published><updated>2014-09-27T21:20:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-09-27:/2014/09/pushover-notifications-for-shell-command-completion-and-status/</id><summary type="html">&lt;p&gt;How to get pushover notifications of shell command completion and&amp;nbsp;status&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lately I&amp;#8217;ve been doing a bunch of work with &lt;a href="http://www.packer.io/"&gt;packer&lt;/a&gt; building &lt;a href="http://www.vagrantup.com/"&gt;Vagrant&lt;/a&gt;
machine images, and using &lt;a href="http://serverspec.org/"&gt;serverspec&lt;/a&gt; to run automated acceptance tests on the images. Unfortunately,
this ends up being a ~40-minute cycle time for the full image to provision and test. So, lots of watching text slowly scroll
down a screen, and finding something else to do. It&amp;#8217;s the weekend; I want to get this project finished, but I&amp;#8217;ve got other
things to&amp;nbsp;do.&lt;/p&gt;
&lt;p&gt;So, I wrote a little bash wrapper around &lt;a href="https://github.com/jnwatts"&gt;jnwatts&amp;#8217;&lt;/a&gt;
&lt;a href="https://raw.githubusercontent.com/jnwatts/pushover.sh/master/pushover.sh"&gt;pushover.sh&lt;/a&gt;. Assuming wherever you put this
is in your path, simply prefix any command with &lt;code&gt;pushover&lt;/code&gt;, and you&amp;#8217;ll get a handy &lt;a href="https://pushover.net/"&gt;Pushover&lt;/a&gt;
notification when it completes, along with the exit status and some other useful&amp;nbsp;information.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;span class="c1"&gt;# Notify command completion and exit status via pushover&lt;/span&gt;
&lt;span class="c1"&gt;# uses pushover.sh from https://raw.githubusercontent.com/jnwatts/pushover.sh/master/pushover.sh&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;

&lt;span class="nv"&gt;&lt;span class="caps"&gt;APIKEY&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Your Pushover &lt;span class="caps"&gt;API&lt;/span&gt; Key Here&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;&lt;span class="caps"&gt;USERKEY&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Your Pushover User Key Here&amp;quot;&lt;/span&gt;

&lt;span class="nv"&gt;stime&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;date &lt;span class="s1"&gt;&amp;#39;+%s&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;$@&lt;/span&gt;
&lt;span class="nv"&gt;exitcode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$?&lt;/span&gt;
&lt;span class="c1"&gt;# timer&lt;/span&gt;
&lt;span class="nv"&gt;etime&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;date &lt;span class="s1"&gt;&amp;#39;+%s&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;etime &lt;span class="o"&gt;-&lt;/span&gt; stime&lt;span class="k"&gt;))&lt;/span&gt;
&lt;span class="nv"&gt;ds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;dt &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="m"&gt;60&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt;
&lt;span class="nv"&gt;dm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;dt &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="m"&gt;60&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="m"&gt;60&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt;
&lt;span class="nv"&gt;dh&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;dt &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="m"&gt;3600&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt;
&lt;span class="nv"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;printf&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;%d:%02d:%02d&amp;#39;&lt;/span&gt; &lt;span class="nv"&gt;$dh&lt;/span&gt; &lt;span class="nv"&gt;$dm&lt;/span&gt; &lt;span class="nv"&gt;$ds&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="c1"&gt;# end timer&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$exitcode&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -eq &lt;span class="m"&gt;0&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
    pushover.sh -p &lt;span class="m"&gt;0&lt;/span&gt; -t &lt;span class="s2"&gt;&amp;quot;Command Succeeded&amp;quot;&lt;/span&gt; -T &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;APIKEY&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -U &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;USERKEY&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;succeeded in &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;times&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; on &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;hostname&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;: &lt;/span&gt;&lt;span class="nv"&gt;$@&lt;/span&gt;&lt;span class="s2"&gt; (in &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;(sent pushover success notification)&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
    pushover.sh -p &lt;span class="m"&gt;0&lt;/span&gt; -s falling -t &lt;span class="s2"&gt;&amp;quot;Command Failed&amp;quot;&lt;/span&gt; -T &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;APIKEY&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; -U &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;USERKEY&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;failed in &lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;times&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; (exit &lt;/span&gt;&lt;span class="nv"&gt;$exitcode&lt;/span&gt;&lt;span class="s2"&gt;) on &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;hostname&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;: &lt;/span&gt;&lt;span class="nv"&gt;$@&lt;/span&gt;&lt;span class="s2"&gt; (in &lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;(sent pushover failure notification)&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, for example, a failing spec&amp;nbsp;test:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;jantman@phoenix:pts/4:~/&lt;span class="caps"&gt;CMG&lt;/span&gt;/git/puppet-cm (&lt;span class="caps"&gt;AUTO&lt;/span&gt;-415=)$ pushover bundle exec rake spec
&amp;lt;lots of failing spec output that exits non-0 after 1 minute 10 seconds&amp;gt;
(sent pushover failure notification)
jantman@phoenix:pts/4:~/&lt;span class="caps"&gt;CMG&lt;/span&gt;/git/puppet-cm (&lt;span class="caps"&gt;AUTO&lt;/span&gt;-415=)$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Would send me a handy pushover message when it&amp;nbsp;finishes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Command Failed
failed in 0:01:10 (exit 1) on phoenix: bundle exec rake spec (in /home/jantman/&lt;span class="caps"&gt;CMG&lt;/span&gt;/git/puppet-cm)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hopefully this is useful to someone else as&amp;nbsp;well&amp;#8230;&lt;/p&gt;</content><category term="pushover"></category><category term="shell"></category><category term="notifications"></category></entry><entry><title>Session Save and Restore with Bash and GNUÂ Screen</title><link href="http://blog.jasonantman.com/2014/07/session-save-and-restore-with-bash-and-gnu-screen/" rel="alternate"></link><published>2014-07-25T10:09:00-04:00</published><updated>2014-07-25T10:09:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-07-25:/2014/07/session-save-and-restore-with-bash-and-gnu-screen/</id><summary type="html">&lt;p&gt;How to automatically save and restore &lt;span class="caps"&gt;GNU&lt;/span&gt; screen sessions including windows, pwd and&amp;nbsp;history&lt;/p&gt;</summary><content type="html">&lt;p&gt;I&amp;#8217;ve been using &lt;a href="http://www.gnu.org/software/screen/"&gt;&lt;span class="caps"&gt;GNU&lt;/span&gt; Screen&lt;/a&gt; for a very long time; I pretty much do &lt;em&gt;all&lt;/em&gt; of my
daily work in it. I have long-lived screen sessions pretty much everywhere; at any given time, I&amp;#8217;ve got a session running
on my desktop (that probably has 19 windows open and active) and a few on various remote hosts. I also have a really
bad habit of using screen windows to hold work in progress, things that I need to revisit, and what I want to do
next. This isn&amp;#8217;t as big of a deal on boxes in a datacenter that rarely go down, but my home desktop ends up getting
rebooted every few weeks (and not always at planned&amp;nbsp;times).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; - what I&amp;#8217;m about to describe is, really, a fragile and somewhat ugly hack. I&amp;#8217;m pretty sure that if I took
the time to learn and switch to zsh (or another, more modern shell) and tmux, I could probably do this easier. But my
shell environment is something I&amp;#8217;m pretty stuck in. So, if this is useful to anyone else, cool. But caveat&amp;nbsp;emptor.&lt;/p&gt;
&lt;p&gt;screen 4.2.0 introduced some extensions to the &lt;code&gt;-Q&lt;/code&gt; remote querying capabilities, including the ability to retrieve a
list of current windows and their titles via &lt;code&gt;screen -Q windows&lt;/code&gt;. A few months ago, I wrapped a python script around
this that reads the currently open windows along with their title and window number, and writes out &lt;code&gt;~/.screenrc.save&lt;/code&gt;
that&amp;#8217;s &lt;code&gt;~/.screenrc&lt;/code&gt; with &lt;code&gt;screen -t&lt;/code&gt; lines to recreate my currently open windows with their titles. After a system
crash or reboot, I could &lt;code&gt;screen -c ~/.screenrc.save&lt;/code&gt; and get all of my windows and their titles back. So, that&amp;#8217;s
a slightly better reminder of what I was working on assuming I keep my titles relevant. But each window just dumped
me into &lt;code&gt;~/&lt;/code&gt; like usual, so I&amp;#8217;d just have the window title to remind me what I was working&amp;nbsp;on. &lt;/p&gt;
&lt;p&gt;I ran this script for a few months; you can see the original version &lt;a href="https://github.com/jantman/misc-scripts/blob/ab6a14774d5dd6250aac98f804c33d3dc26a32eb/savescreen.py"&gt;here&lt;/a&gt;.
However, this still really isn&amp;#8217;t what I&amp;#8217;d call &amp;#8220;session restore&amp;#8221;. I had window titles as &amp;#8220;hints&amp;#8221; to what I was doing,
but everything else was left to my&amp;nbsp;memory.&lt;/p&gt;
&lt;p&gt;Enter some awful &lt;code&gt;bashrc&lt;/code&gt; hackery. Please note that my bashrc is a bit complicated, mainly due to git completion
and getting a proper prompt for python virtualenvs, but here&amp;#8217;s the magic&amp;nbsp;portion:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# git prompt - make it work everywhere&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -e /usr/share/git/completion/git-prompt.sh &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;source&lt;/span&gt; /usr/share/git/completion/git-prompt.sh
&lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -e /usr/share/git-core/contrib/completion/git-prompt.sh &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;source&lt;/span&gt; /usr/share/git-core/contrib/completion/git-prompt.sh
&lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -e ~/bin/git-prompt.sh &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;source&lt;/span&gt; ~/bin/git-prompt.sh
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="c1"&gt;#set the &lt;span class="caps"&gt;PROMPT&lt;/span&gt;&lt;/span&gt;
&lt;span class="nv"&gt;cur_tty&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nv"&gt;temp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;tty&lt;span class="k"&gt;)&lt;/span&gt; &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nv"&gt;5&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="c1"&gt;# git prompt configutation&lt;/span&gt;
&lt;span class="nv"&gt;GIT_PS1_SHOWDIRTYSTATE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="nv"&gt;GIT_PS1_SHOWUNTRACKEDFILES&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="nv"&gt;GIT_PS1_SHOWUPSTREAM&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;auto&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;GIT_PS1_SHOWCOLORHINTS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;

&lt;span class="c1"&gt;# for screen session-saving hack, set per-window history file if in screen&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;STY&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;WINDOW&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;&lt;span class="caps"&gt;HISTFILE&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;readlink -f ~/.screenhist/&lt;span class="nv"&gt;$&lt;span class="caps"&gt;WINDOW&lt;/span&gt;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;shopt&lt;/span&gt; -s histappend

&lt;span class="c1"&gt;# make sure our screen session-saving hack directories exist&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt; -d ~/.screenhist &lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; mkdir ~/.screenhist
&lt;span class="o"&gt;[[&lt;/span&gt; -d ~/.screendirs &lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; mkdir ~/.screendirs

__wrap_git_ps1 &lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="c1"&gt;# commands here now get executed every time bash constructs a prompt&lt;/span&gt;
    &lt;span class="c1"&gt;# for screen pwd saving&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;STY&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;WINDOW&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;
    &lt;span class="k"&gt;then&lt;/span&gt;
        &lt;span class="nv"&gt;&lt;span class="caps"&gt;SCREENLINKDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;readlink -f ~/.screendirs&lt;span class="k"&gt;)&lt;/span&gt;
        rm -f &lt;span class="nv"&gt;$&lt;span class="caps"&gt;SCREENLINKDIR&lt;/span&gt;&lt;/span&gt;/&lt;span class="nv"&gt;$&lt;span class="caps"&gt;WINDOW&lt;/span&gt;&lt;/span&gt;
        ln -sf &lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;/ &lt;span class="nv"&gt;$&lt;span class="caps"&gt;SCREENLINKDIR&lt;/span&gt;&lt;/span&gt;/&lt;span class="nv"&gt;$&lt;span class="caps"&gt;WINDOW&lt;/span&gt;&lt;/span&gt;
    &lt;span class="k"&gt;fi&lt;/span&gt;
    &lt;span class="c1"&gt;# virtualenv stuff for prompt&lt;/span&gt;
    &lt;span class="nv"&gt;venv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="o"&gt;[[&lt;/span&gt; &lt;span class="nv"&gt;$VIRTUAL_ENV&lt;/span&gt; !&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nv"&gt;venv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;\[\033[31m\](&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;VIRTUAL_ENV&lt;/span&gt;&lt;span class="p"&gt;##*/&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;)\e[0m&amp;quot;&lt;/span&gt;
    __git_ps1 &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$venv&lt;/span&gt;&lt;span class="s2"&gt;\u@\h:&lt;/span&gt;&lt;span class="nv"&gt;$cur_tty&lt;/span&gt;&lt;span class="s2"&gt;:\w&amp;quot;&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\\\$ &amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;history&lt;/span&gt; -a
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="nv"&gt;PROMPT_COMMAND&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;__wrap_git_ps1&amp;#39;&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;&lt;span class="caps"&gt;PS2&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;gt; &amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So&amp;#8230; the hack. First we source the git prompt scripts that come with git (trying the
locations they should be at on all of the machines I commonly use, and if it can&amp;#8217;t find
any of them, falling back to a copy in my homedir) and set some configuration variables
for them (as well as capturing the current tty). We then (conditionally on being inside
a screen window) set our history file to a per-screen-window path, and have history append.
At this point we also make sure some directories we&amp;#8217;ll use&amp;nbsp;exist.&lt;/p&gt;
&lt;p&gt;Now the real fun. &lt;code&gt;PROMPT_COMMAND&lt;/code&gt; specifies a function for bash to execute to build the
prompt string; this is called every time bash needs to display the prompt (so, effectively,
every time a command completes in the shell). We set it to &lt;code&gt;__wrap_git_ps1&lt;/code&gt;, a function we
just defined. The magic happens in this function. Screen sets some environment variables
inside each window, including &lt;code&gt;STY&lt;/code&gt; (the name of the screen session you&amp;#8217;re in) and
&lt;code&gt;WINDOW&lt;/code&gt;, the current window number. If both of these are set, we symlink our current
&lt;code&gt;pwd&lt;/code&gt; to &lt;code&gt;~/.screendirs/$WINDOW&lt;/code&gt; (note some hackery, explicitly removing the link if it
already exists, to get this to work correctly). We then throw in some python virtualenv-specific
prompt settings, and pass on the strings we&amp;#8217;ve constructed to &lt;code&gt;__git_ps1&lt;/code&gt; which adds the
git-specific information, and then sets &lt;code&gt;PS1&lt;/code&gt; correctly. Finally, we explicitly append to
current history, to make sure the history on disk is always accurate and&amp;nbsp;up-to-date.&lt;/p&gt;
&lt;p&gt;This works in combination with the &lt;a href="https://github.com/jantman/misc-scripts/blob/master/savescreen.py"&gt;latest version&lt;/a&gt;
of savescreen.py, which has some minor changes. The line to create each window,&amp;nbsp;formerly:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;screen -t &lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt;{name}&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt; {num}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;windows&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;becomes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;screen -t &lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt;{name}&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt; {num} sh -c &lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s2"&gt;cd $(readlink -fn {dirpath}/{num}); bash&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;windows&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dirpath&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dirpath&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When each window is created at startup, we &lt;code&gt;cd&lt;/code&gt; into the previous &lt;code&gt;pwd&lt;/code&gt; (the path
that the &lt;code&gt;~/.screendirs/$WINDOW&lt;/code&gt; symlink, created by bashrc, points to) and then
call our shell. When this is combined with the &lt;code&gt;HISTFILE&lt;/code&gt; change, the effect is that
&lt;code&gt;screen -c ~/.screenrc.save&lt;/code&gt; brings us back into a screen session that has not only
all of our previous windows and their titles, but also a shell in each window&amp;#8217;s previous
working directory, and that window&amp;#8217;s&amp;nbsp;history.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I should&amp;#8217;ve also used &lt;code&gt;$STY&lt;/code&gt; in each of the paths, so this would be multi-session-safe.
   I didn&amp;#8217;t, so this has undefined behavior if more than one screen session is running as
   your&amp;nbsp;user.&lt;/li&gt;
&lt;li&gt;A lot of this is lost, obviously, if you &lt;code&gt;sudo su&lt;/code&gt; or &lt;code&gt;ssh&lt;/code&gt;, or in any other way end up
   as a different&amp;nbsp;user.&lt;/li&gt;
&lt;li&gt;I&amp;#8217;m thinking about rolling in some method of automatic &lt;code&gt;virtualenv&lt;/code&gt; activation (since it,
   unfortunately, doesn&amp;#8217;t have anything like &lt;code&gt;.rvmrc&lt;/code&gt;). Maybe in the next&amp;nbsp;version.&lt;/li&gt;
&lt;/ol&gt;</content><category term="bash"></category><category term="screen"></category><category term="restore"></category><category term="bashrc"></category></entry><entry><title>How Yum and RPM CompareÂ Versions</title><link href="http://blog.jasonantman.com/2014/07/how-yum-and-rpm-compare-versions/" rel="alternate"></link><published>2014-07-11T23:31:00-04:00</published><updated>2014-07-11T23:31:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-07-11:/2014/07/how-yum-and-rpm-compare-versions/</id><summary type="html">&lt;p&gt;A description of the algorithms used by Yum and &lt;span class="caps"&gt;RPM&lt;/span&gt; to compare package&amp;nbsp;versions.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I was recently tripped up by a bug in Puppet, &lt;a href="https://tickets.puppetlabs.com/browse/PUP-1244"&gt;&lt;span class="caps"&gt;PUP&lt;/span&gt;-1244&lt;/a&gt;,
dealing with how it compares package versions. All of Puppet&amp;#8217;s Package types assumed
&lt;a href="http://semver.org/"&gt;semantic versioning&lt;/a&gt;, and that&amp;#8217;s far from the case for RPMs and therefore Yum. This
manifested itself in how Puppet validates package installations - if a version was explicitly specified
and Yum/&lt;span class="caps"&gt;RPM&lt;/span&gt; could install it, puppet would shell out to them and install the package, but then report
a failure in its post-install validation, as the &lt;em&gt;exact&lt;/em&gt; version string specified isn&amp;#8217;t present.
For example, many RedHat/CentOS packages (such as those from &lt;span class="caps"&gt;EPEL&lt;/span&gt;) include a release string with the major
version of the distribution they were packged for - i.e. &amp;#8220;.el5&amp;#8221; or &amp;#8220;.el6&amp;#8221;. If Puppet was instructed to
install package &amp;#8220;foo&amp;#8221; version &amp;#8220;1.2.3&amp;#8221;, but the actual package in the repositories was &amp;#8220;foo-1.2.3-el5&amp;#8221;,
Puppet would cause the package to be installed, but then report&amp;nbsp;failure.&lt;/p&gt;
&lt;p&gt;I cut a &lt;a href="https://github.com/puppetlabs/puppet/pull/2866"&gt;pull request&lt;/a&gt; against the Puppet4 branch to
fix these issues, essentially re-implementing yum and rpm&amp;#8217;s version comparison logic in Ruby. It took
me a few days of research and sorting through source code (and in the process I found that &lt;code&gt;yum&lt;/code&gt;, despite
its use in so many distributions, has no unit tests at all) but I finally got it finished. In the process,
I found out exactly how&amp;#8230; weird&amp;#8230; &lt;span class="caps"&gt;RPM&lt;/span&gt;&amp;#8217;s version comparison rules&amp;nbsp;are.&lt;/p&gt;
&lt;h3 id="package-naming-and-parsing"&gt;Package Naming and&amp;nbsp;Parsing&lt;/h3&gt;
&lt;p&gt;&lt;span class="caps"&gt;RPM&lt;/span&gt; package names are made up of five parts; the package name, epoch, version, release, and architecture.
This format is commonly referred to as the acronym &lt;span class="caps"&gt;NEVRA&lt;/span&gt;. The epoch is not always included; it is assumed
to be zero (0) on any packages that lack it explicitly. The format for the whole string is &lt;code&gt;n-e:v-r.a&lt;/code&gt;.
For my purposes, I was only really concerned with comparing the &lt;span class="caps"&gt;EVR&lt;/span&gt; portion; Puppet knows about package names
and the bug herein was with what Puppet calls the &amp;#8220;version&amp;#8221; (&lt;span class="caps"&gt;EVR&lt;/span&gt; in yum/rpm parlance). Parsing is pretty&amp;nbsp;simple:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If there is a &lt;code&gt;:&lt;/code&gt; in the string, everything before it is the epoch. If not, the epoch is&amp;nbsp;zero.&lt;/li&gt;
&lt;li&gt;If there is a &lt;code&gt;-&lt;/code&gt; in the &lt;em&gt;remaining&lt;/em&gt; string, everything before the first &lt;code&gt;-&lt;/code&gt; is the version,
  and everything after it is the release. If there isn&amp;#8217;t one, the release is considered&amp;nbsp;null/nill/None/whatever.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="how-yum-compares-evr"&gt;How Yum Compares&amp;nbsp;&lt;span class="caps"&gt;EVR&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Once the package string is parsed into its &lt;span class="caps"&gt;EVR&lt;/span&gt; components, yum calls &lt;code&gt;rpmUtils.miscutils.compareEVR()&lt;/code&gt;,
which does some data type massaging for the inputs, and then calls out to &lt;code&gt;rpm.labelCompare()&lt;/code&gt;
(found in &lt;code&gt;rpm.git/python/header-py.c&lt;/code&gt;). &lt;code&gt;labelCompare()&lt;/code&gt; sets each epoch
to &amp;#8220;0&amp;#8221; if it was null/Nonem, and then uses &lt;code&gt;compare_values()&lt;/code&gt; to compare each &lt;span class="caps"&gt;EVR&lt;/span&gt; portion, which in turn falls through
to a function called &lt;code&gt;rpmvercmp()&lt;/code&gt; (see below). The algorithm for &lt;code&gt;labelCompare()&lt;/code&gt; is as&amp;nbsp;follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set each epoch value to 0 if it&amp;#8217;s&amp;nbsp;null/None.&lt;/li&gt;
&lt;li&gt;Compare the epoch values using &lt;code&gt;compare_values()&lt;/code&gt;. If they&amp;#8217;re not equal, return that result, else
   move on to the next portion (version). The logic within &lt;code&gt;compare_values()&lt;/code&gt; is that if one is empty/null
   and the other is not, the non-empty one is greater, and that ends the comparison. If neither of
   them is empty/not present, compare them using &lt;code&gt;rpmvercmp()&lt;/code&gt; and follow the same logic; if one
   is &amp;#8220;greater&amp;#8221; (newer) than the other, that&amp;#8217;s the end result of the comparison. Otherwise, move
   on to the next component&amp;nbsp;(version).&lt;/li&gt;
&lt;li&gt;Compare the versions using the same&amp;nbsp;logic.&lt;/li&gt;
&lt;li&gt;Compare the releases using the same&amp;nbsp;logic.&lt;/li&gt;
&lt;li&gt;If all of the components are &amp;#8220;equal&amp;#8221;, the packages are the&amp;nbsp;same.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The real magic, obviously, happens in &lt;code&gt;rpmvercmp()&lt;/code&gt;, the rpm library function to compare two
versions (or epochs, or releases). That&amp;#8217;s also where the madness&amp;nbsp;happens.&lt;/p&gt;
&lt;h3 id="how-rpm-compares-version-parts"&gt;How &lt;span class="caps"&gt;RPM&lt;/span&gt; Compares Version&amp;nbsp;Parts&lt;/h3&gt;
&lt;p&gt;&lt;span class="caps"&gt;RPM&lt;/span&gt; is written in C. Converting all of the buffer and pointer processing for these strings
over to Ruby was quite a pain. That being said, I didn&amp;#8217;t make this up, this is actually the
algorithm that &lt;code&gt;rpmvercmp()&lt;/code&gt; (&lt;code&gt;lib/rpmvercmp.c&lt;/code&gt;) uses to compare version &amp;#8220;parts&amp;#8221;
(epoch, version, release). This function returns 0 if the strings are equal, 1 if &lt;code&gt;a&lt;/code&gt; (the
first string argument) is newer than &lt;code&gt;b&lt;/code&gt; (the second string argument), or -1 if
&lt;code&gt;a&lt;/code&gt; is older than &lt;code&gt;b&lt;/code&gt;. Also keep in mind that this uses pointers in C, so it works by removing
a sequence of 0 or more characters from the front of each string, comparing them, and then repeating
for the remaining characters in each string until something is unequal, or a string reaches its&amp;nbsp;end.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the strings are binary equal (&lt;code&gt;a == b&lt;/code&gt;), they&amp;#8217;re equal, return&amp;nbsp;0.&lt;/li&gt;
&lt;li&gt;Loop over the strings, left-to-right.&lt;ol&gt;
&lt;li&gt;Trim anything that&amp;#8217;s not &lt;code&gt;[A-Za-z0-9]&lt;/code&gt; or tilde (&lt;code&gt;~&lt;/code&gt;) from the front of both&amp;nbsp;strings.&lt;/li&gt;
&lt;li&gt;If both strings start with a tilde, discard it and move on to the next&amp;nbsp;character.&lt;/li&gt;
&lt;li&gt;If string &lt;code&gt;a&lt;/code&gt; starts with a tilde and string &lt;code&gt;b&lt;/code&gt; does not, return -1 (string &lt;code&gt;a&lt;/code&gt; is older);
   and the inverse if string &lt;code&gt;b&lt;/code&gt; starts with a tilde and string &lt;code&gt;a&lt;/code&gt; does&amp;nbsp;not.&lt;/li&gt;
&lt;li&gt;End the loop if either string has reached zero&amp;nbsp;length.&lt;/li&gt;
&lt;li&gt;If the first character of &lt;code&gt;a&lt;/code&gt; is a digit, pop the leading chunk of continuous digits from
   each string (which may be &amp;#8221; for &lt;code&gt;b&lt;/code&gt; if only one &lt;code&gt;a&lt;/code&gt; starts with digits). If &lt;code&gt;a&lt;/code&gt; begins
   with a letter, do the same for leading&amp;nbsp;letters.&lt;/li&gt;
&lt;li&gt;If the segement from &lt;code&gt;b&lt;/code&gt; had 0 length, return 1 if the segment from &lt;code&gt;a&lt;/code&gt; was numeric, or
   -1 if it was alphabetic. The logical result of this is that if &lt;code&gt;a&lt;/code&gt; begins with numbers
   and &lt;code&gt;b&lt;/code&gt; does not, &lt;code&gt;a&lt;/code&gt; is newer (return 1). If &lt;code&gt;a&lt;/code&gt; begins with letters and &lt;code&gt;b&lt;/code&gt; does not,
   then &lt;code&gt;a&lt;/code&gt; is older (return -1). If the leading character(s) from &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; were both
   numbers or both letters, continue&amp;nbsp;on.&lt;/li&gt;
&lt;li&gt;If the leading segments were both numeric, discard any leading zeros and &lt;em&gt;whichever one is longer
   wins&lt;/em&gt;. If &lt;code&gt;a&lt;/code&gt; is longer than &lt;code&gt;b&lt;/code&gt; (without leading zeroes), return 1, and vice-versa. If
   they&amp;#8217;re of the same length, continue&amp;nbsp;on.&lt;/li&gt;
&lt;li&gt;Compare the leading segments with &lt;code&gt;strcmp()&lt;/code&gt; (or &lt;code&gt;&amp;lt;=&amp;gt;&lt;/code&gt; in Ruby). If that returns a non-zero
   value, then return that value. Else continue to the next iteration of the&amp;nbsp;loop.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;If the loop ended (nothing has been returned yet, either both strings are totally the same or they&amp;#8217;re
   the same up to the end of one of them, like with &amp;#8220;1.2.3&amp;#8221; and &amp;#8220;1.2.3b&amp;#8221;), then the longest wins -
   if what&amp;#8217;s left of &lt;code&gt;a&lt;/code&gt; is longer than what&amp;#8217;s left of &lt;code&gt;b&lt;/code&gt;, return 1. Vice-versa for if what&amp;#8217;s
   left of &lt;code&gt;b&lt;/code&gt; is longer than what&amp;#8217;s left of &lt;code&gt;a&lt;/code&gt;. And finally, if what&amp;#8217;s left of them is the same
   length, return&amp;nbsp;0.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Well there you have it. Quite convoluted. And full of things like the &amp;#8220;~&amp;#8221; magic character (&amp;#8220;~1&amp;#8221; is always
older than&amp;nbsp;&amp;#8220;9999zzzz&amp;#8221;).&lt;/p&gt;</content><category term="rpm"></category><category term="yum"></category><category term="puppet"></category><category term="versions"></category></entry><entry><title>bashrc Vagrant / VirtualBoxÂ reminder</title><link href="http://blog.jasonantman.com/2014/07/bashrc-vagrant-virtualbox-reminder/" rel="alternate"></link><published>2014-07-10T06:45:00-04:00</published><updated>2014-07-10T06:45:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-07-10:/2014/07/bashrc-vagrant-virtualbox-reminder/</id><summary type="html">&lt;p&gt;Add a little reminder about Vagrant/VirtualBox running machines in your&amp;nbsp;profile/bashrc.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Lately I&amp;#8217;ve been using VirtualBox VMs, both managed by Vagrant and otherwise, quite a lot.
I&amp;#8217;ve also been doing a bunch of development work with them. And inevitably, I close a screen
window and fo on with my work and end up with a few &amp;#8220;orphaned&amp;#8221; virtualbox VMs running that
I&amp;#8217;ve forgotten&amp;nbsp;about.&lt;/p&gt;
&lt;p&gt;Below is the snippet I&amp;#8217;ve added to my &lt;code&gt;~/.bashrc&lt;/code&gt; to keep me aware of this situation. Unfortunately
the &lt;code&gt;vagrant global-status&lt;/code&gt; command is relatively slow, so this adds (on my machine) about
1.5 seconds of wall-clock time to my &lt;code&gt;.bashrc&lt;/code&gt; (hence the process check&amp;nbsp;first).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Vagrant/VirtualBox reminder&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; pgrep VBoxHeadless &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&amp;gt;/dev/null&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nv"&gt;vblist&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;VBoxManage list runningvms&lt;span class="k"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;[&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;vblist&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;\e[1;31mRunning VirtualBox VMs:\e[0m\n&lt;/span&gt;&lt;span class="si"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;vblist&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;\n&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; which vagrant &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&amp;gt; /dev/null &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; vagrant &lt;span class="nb"&gt;help&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep -q global-status&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;then&lt;/span&gt;
        &lt;span class="nv"&gt;vagrantstatus&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;vagrant global-status &lt;span class="p"&gt;|&lt;/span&gt; sed &lt;span class="s1"&gt;&amp;#39;/^\s*$/q&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$vagrantstatus&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep -q running &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;\e[1;31mRunning Vagrant Machines:\e[0m&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$vagrantstatus&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; head -2&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$vagrantstatus&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; grep running&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="vagrant"></category><category term="bashrc"></category><category term="profile"></category><category term="virtualbox"></category></entry><entry><title>Remotely-controlled deck.js slideÂ presentations</title><link href="http://blog.jasonantman.com/2014/05/remotely-controlled-deckjs-slide-presentations/" rel="alternate"></link><published>2014-05-12T09:34:00-04:00</published><updated>2014-05-12T09:34:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-05-12:/2014/05/remotely-controlled-deckjs-slide-presentations/</id><summary type="html">&lt;p&gt;A wonderful GitHub project by chrisjaure to remotely control deck.js slide&amp;nbsp;presentations.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I&amp;#8217;ve been struggling to find a good, cross-platform remote meeting solution. We&amp;#8217;re using &lt;a href="http://www.imeet.com"&gt;iMeet&lt;/a&gt;
at work at the moment, but there&amp;#8217;s no way to present or screen share from a Linux machine. For most of our ops and
automation team daily and weekly meetings, we use &lt;a href="http://www.teamspeak.com/"&gt;TeamSpeak&lt;/a&gt; - sure it&amp;#8217;s not open source,
but it&amp;#8217;s simple, supports all OSes that matter to us (Mac, Linux, Windows, Android and iOS), can be self-hosted,
and has the holy grail, functional push-to-talk. But it&amp;#8217;s audio&amp;nbsp;only.&lt;/p&gt;
&lt;p&gt;On Friday I was running two short elaboration meetings, and had quick little slide decks done up in &lt;a href="http://imakewebthings.com/deck.js/"&gt;deck.js&lt;/a&gt;
to keep us on track. I couldn&amp;#8217;t help but think, gee, it sure would be nice if instead of switching to Mac or a &lt;span class="caps"&gt;VM&lt;/span&gt; and sharing my screen,
we could just use the audio communication mediums that we already do, and I could simply control the slides in a&amp;nbsp;browser.&lt;/p&gt;
&lt;p&gt;Well this morning I stumbled on &lt;a href="http://cleverchris.com/"&gt;Chris Jaure&lt;/a&gt;&amp;#8216;s &lt;a href="https://github.com/chrisjaure/deckjs-remote"&gt;deckjs-remote&lt;/a&gt;
project that does exactly that. It&amp;#8217;s a nodejs npm module that runs a websocket server, and allows people to join a session and follow
along as the presenter changes&amp;nbsp;slides.&lt;/p&gt;
&lt;p&gt;I did have a few hiccups getting it working - mainly some issues with &lt;span class="caps"&gt;CORS&lt;/span&gt;. The &lt;span class="caps"&gt;README&lt;/span&gt;.md has a large block of markup to be added to
the slide deck html to support &amp;#8220;older browsers that don&amp;#8217;t support &lt;span class="caps"&gt;CORS&lt;/span&gt;.&amp;#8221; I&amp;#8217;m running Firefox 28.0 (Firefox has supported &lt;span class="caps"&gt;CORS&lt;/span&gt; since
3.0, quite a few years back) and still needed to add this to get everything working. I also needed to manually add a script tag
for socket.io coming from the nodejs server in order to get everything&amp;nbsp;working.&lt;/p&gt;
&lt;p&gt;There&amp;#8217;s a bit of a delay for the socket connection to come up after initially loading the page, but once that&amp;#8217;s done, the presenter
(&amp;#8220;master&amp;#8221; session) should get the password prompt, and any guests should get a prompt asking if they want to join the current
session. Perhaps the best part is that the nodejs server interally stores each deck by &lt;span class="caps"&gt;URL&lt;/span&gt;, so it seems to work perfectly fine
when running one instance for N presenters (i.e. a single instance running persistently on a shared&amp;nbsp;server).&lt;/p&gt;</content><category term="slide"></category><category term="presentation"></category><category term="deck.js"></category><category term="deckjs"></category><category term="javascript"></category></entry><entry><title>dashsnap.py - A Script to Snapshot a GraphiteÂ Dashboard</title><link href="http://blog.jasonantman.com/2014/05/dashsnap-a-script-to-snapshot-a-graphite-dashboard/" rel="alternate"></link><published>2014-05-07T21:58:00-04:00</published><updated>2014-05-07T21:58:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-05-07:/2014/05/dashsnap-a-script-to-snapshot-a-graphite-dashboard/</id><summary type="html">&lt;p&gt;dashsnap.py, a script to snapshot a Graphite dashboard at various&amp;nbsp;intervals&lt;/p&gt;</summary><content type="html">&lt;p&gt;As we push more and more and more metrics into &lt;a href="http://graphite.wikidot.com/"&gt;Graphite&lt;/a&gt;
at work, we&amp;#8217;ve found the need to preserve data from an incident or outage to be quite
important. Especially now that we&amp;#8217;re feeding a &lt;em&gt;lot&lt;/em&gt; of our data at 10-second intervals,
and our storage schemas generally start aggregating that past 24 hours (God only knows
how many spikes are gone if you look a week later), it&amp;#8217;s important to capture as much
data as we think we&amp;#8217;ll need as soon after the incident as&amp;nbsp;possible.&lt;/p&gt;
&lt;p&gt;To this end, a few days (and nights) into a relatively major crisis, I wrote a little
python script, &lt;a href="https://github.com/jantman/misc-scripts/blob/master/dashsnap.py"&gt;dashsnap.py&lt;/a&gt;.
It&amp;#8217;s horribly simple; pass it the hostname to your graphite server (if &amp;#8220;graphite&amp;#8221; doesn&amp;#8217;t
resolve to what you want), the name of a dashboard, optionally a height and width for images
(the default is currently 1024x768), and either a from and to date/time or a list of graphite
&lt;span class="caps"&gt;URL&lt;/span&gt;-style intervals (the default is a ginormous &amp;#8220;-10minutes,-30minutes,-1hours,-2hours,-4hours,-6hours,-12hours,-24hours,-36hours&amp;#8221;).
It will find all graphs on your dashboard, and locally save (in a horribly named directory)
both PNGs of all the graphs, as well as the &lt;em&gt;raw &lt;span class="caps"&gt;JSON&lt;/span&gt; data&lt;/em&gt; for them. It&amp;#8217;ll also write
(2 &lt;span class="caps"&gt;AM&lt;/span&gt;-simple) &lt;span class="caps"&gt;HTML&lt;/span&gt; index files to all of the intervals and graphs within&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s a view of the index page using the default&amp;nbsp;intervals:&lt;/p&gt;
&lt;p&gt;&lt;a href="/GFX/dashsnap_index.png"&gt;&lt;img alt="screenshot of rendered index page" src="/GFX/dashsnap_index_sm.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And here&amp;#8217;s the page showing graphs and &lt;span class="caps"&gt;JSON&lt;/span&gt; links for an individual dashboard for one&amp;nbsp;interval:&lt;/p&gt;
&lt;p&gt;&lt;a href="/GFX/dashsnap_page.png"&gt;&lt;img alt="screenshot of one interval page" src="/GFX/dashsnap_page_sm.png"&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll quickly admit right now that this is alpha software, if you can even call it that.
I guess in reality it&amp;#8217;s a late-night fix to a problem that deserves more. But, if it can
save someone else a few hours late at night, it&amp;#8217;s worth mentioning. PRs are welcome, as
are issues and suggestions on GitHub for bugs, or for where I should take this; I like
the handy little &lt;span class="caps"&gt;CLI&lt;/span&gt; script (though the output could use quite a bit of visual work),
but I&amp;#8217;m also toying around with the idea of creating a service to take the snapshots
and store them, mostly thinking about it being part of something like
&lt;a href="https://github.com/etsy/morgue"&gt;Etsy&amp;#8217;s Morgue&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The latest version of the source for dashsnap will (within the forseeable future)
be available&amp;nbsp;at:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jantman/misc-scripts/blob/master/dashsnap.py"&gt;https://github.com/jantman/misc-scripts/blob/master/dashsnap.py&lt;/a&gt;&lt;/p&gt;</content><category term="graphite"></category><category term="monitoring"></category></entry><entry><title>NSA TargetingÂ SysAdmins</title><link href="http://blog.jasonantman.com/2014/03/nsa-targeting-sysadmins/" rel="alternate"></link><published>2014-03-22T09:38:00-04:00</published><updated>2014-03-22T09:38:00-04:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-03-22:/2014/03/nsa-targeting-sysadmins/</id><summary type="html">&lt;p&gt;Newly leaked documents reveal &lt;span class="caps"&gt;NSA&lt;/span&gt; is targeting SysAdmins - disturbing new&amp;nbsp;information.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I try not to rant too much, but I feel that this one was needed. If you&amp;#8217;re looking for
objective, technical information, skip this&amp;nbsp;one.&lt;/p&gt;
&lt;p&gt;When I was younger and more naive, I had near complete trust in the &lt;span class="caps"&gt;US&lt;/span&gt; government. There&amp;#8217;s a Constitution,
and they abide by it, and so do I. Since 2001, that feeling has eroded a bit. Since 2008, it&amp;#8217;s eroded even
more. But the documents leaked by &lt;a href="http://en.wikipedia.org/wiki/Edward_Snowden"&gt;Edward Snowden&lt;/a&gt; have been
the &amp;#8216;icing on the cake&amp;#8217;. I know there&amp;#8217;s disagreement about whether what he did was right or not - I&amp;#8217;m pretty
decided on how I feel, but I know many others who feel that he should be taken out and shot - but one thing
that&amp;#8217;s no longer deniable is, since he released those documents, it&amp;#8217;s come to light that the &lt;span class="caps"&gt;US&lt;/span&gt; government
is routinely performing unconstitutional acts in the furtherance of &amp;#8220;national&amp;nbsp;security.&amp;#8221;&lt;/p&gt;
&lt;p&gt;I feel like many people, who said over and over again that it could never get to this point, that the
Government might cut some corners but they&amp;#8217;d never blatantly do things like &lt;a href="http://www.washingtonpost.com/world/national-security/agencies-collected-data-on-americans-cellphone-use-in-thousands-of-tower-dumps/2013/12/08/20549190-5e80-11e3-be07-006c776266ed_story.html"&gt;mass collection of cellular data&lt;/a&gt;,
&lt;a href="http://www.washingtonpost.com/world/national-security/nsa-tracking-cellphone-locations-worldwide-snowden-documents-show/2013/12/04/5492873a-5cf2-11e3-bc56-c6ca94801fac_story.html"&gt;tracking the physical location of cell phones worldwide&lt;/a&gt;, or &lt;a href="http://www.washingtonpost.com/world/national-security/nsa-infiltrates-links-to-yahoo-google-data-centers-worldwide-snowden-documents-say/2013/10/30/e51d661e-4166-11e3-8b74-d89d714ca4dd_story.html"&gt;tapping into the networks of private companies like Google and Yahoo&lt;/a&gt;. It seems that
every new document leaked is worse than the last. I refuse to accept the arguments that this is all &amp;#8220;legal&amp;#8221;,
they simply don&amp;#8217;t keep up with the times. I couldn&amp;#8217;t care less if the government reads my postal mail (which
they still need an actual warrant for, &lt;span class="caps"&gt;AFAIK&lt;/span&gt;) - they probably get the same credit card and siding offers anyway.
I do, however, care that the government - out of their own self-interest and excitement at near-effortless surveillance -
refuses to extend the same protections that wireline phone calls and postal mail get to their electronic and
wireless&amp;nbsp;equivalents.&lt;/p&gt;
&lt;p&gt;When it came to light that the &lt;span class="caps"&gt;NSA&lt;/span&gt; &lt;a href="http://www.reuters.com/article/2013/12/20/us-usa-security-rsa-idUSBRE9BJ1C220131220"&gt;paid $10M to put a backdoor in &lt;span class="caps"&gt;RSA&lt;/span&gt; encryption&lt;/a&gt;
I was astonished. Late last year a number of other deals came to light, where the &lt;span class="caps"&gt;NSA&lt;/span&gt; paid or extorted software
and hardware manufacturers to intentionally introduce flaws in security to make it easier for the &lt;span class="caps"&gt;NSA&lt;/span&gt;
to gain access. The worst part is these weren&amp;#8217;t &amp;#8220;master passwords&amp;#8221; available only to the &lt;span class="caps"&gt;NSA&lt;/span&gt;; for the most
part, they appear to be mathematical flaws known to the &lt;span class="caps"&gt;NSA&lt;/span&gt;, but just as easily discovered by our enemies.
This part seems to have been glossed over by the media&amp;#8230; the consequences of some mathematician
or security researcher discovering those flaws and selling them to a national enemy or terrorist
would be flat-out devastating to the country and economy. Our government deliberately put a flaw in
an encryption standard, and then used its&amp;#8217; influence via &lt;span class="caps"&gt;NIST&lt;/span&gt;, the National Institute of Standards
and Technology, to &lt;a href="http://arstechnica.com/security/2013/09/the-nsas-work-to-make-crypto-worse-and-better/"&gt;recommend that standard for use&lt;/a&gt;
including by banks, e-commerce sites and financial&amp;nbsp;institutions.&lt;/p&gt;
&lt;p&gt;What has me even more upset, though, is the recent revelation that the &lt;span class="caps"&gt;NSA&lt;/span&gt; is &lt;a href="https://firstlook.org/theintercept/article/2014/03/20/inside-nsa-secret-efforts-hunt-hack-system-administrators/"&gt;systematically targeting
the private, personal accounts of system administrators to gain access to their employers&amp;#8217; networks&lt;/a&gt;. The bulk of the information came from an internal classified blog entry of an &lt;span class="caps"&gt;NSA&lt;/span&gt; employee,
&lt;a href="https://s3.amazonaws.com/s3.documentcloud.org/documents/1094387/i-hunt-sys-admins.pdf"&gt;available as a &lt;span class="caps"&gt;PDF&lt;/span&gt;&lt;/a&gt;
(or &lt;a href="/GFX/i-hunt-sys-admins.pdf"&gt;local copy&lt;/a&gt;). It&amp;#8217;s pretty technical, but it&amp;#8217;s also a startling view into
both the mindsets of &lt;em&gt;individuals&lt;/em&gt; within the &lt;span class="caps"&gt;NSA&lt;/span&gt;, and the organization&amp;#8217;s overall goals. Just two of the many
worthy&amp;nbsp;excerpts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(S/&lt;span class="caps"&gt;SI&lt;/span&gt;//&lt;span class="caps"&gt;REL&lt;/span&gt;) One of the coolest things about it is &lt;strong&gt;how much&lt;/strong&gt; data we have at our fingertips. If we
&lt;em&gt;only&lt;/em&gt; collected the data we knew we wanted&amp;#8230; yeah, we&amp;#8217;d fill some of our requirements, but it is
a whole world of possibilities we&amp;#8217;d be missing! It would be like going on a road-trip, but wearing a
blindfold the entire time, and only removing it when you&amp;#8217;re at one of your destinations&amp;#8230; yeah,
you&amp;#8217;ll still see stuff, but you&amp;#8217;ll be missing out on the entire&amp;nbsp;journey!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So&amp;#8230; ok, they&amp;#8217;re admitting that they collect more data than they want (or legally can?). This person&amp;#8217;s
blog series is about &amp;#8220;using passive collect to identify/enable &lt;span class="caps"&gt;CNE&lt;/span&gt; efforts&amp;#8221; (&lt;span class="caps"&gt;CNE&lt;/span&gt; being Computer Network
Exploitation), which is also implying that they have access to massive amounts of data from non-target
persons, including American&amp;nbsp;citizens.&lt;/p&gt;
&lt;p&gt;Within the document, multiple references are made to the &lt;a href="https://firstlook.org/theintercept/article/2014/03/12/nsa-plans-infect-millions-computers-malware/"&gt;&lt;span class="caps"&gt;QUANTUM&lt;/span&gt;&lt;/a&gt;
program which seems to be viewed as, in short, a tool that lets the &lt;span class="caps"&gt;NSA&lt;/span&gt; input someone&amp;#8217;s Facebook,
webmail, or other online service account, and take control of the computers they use to access&amp;nbsp;it&amp;#8230;&lt;/p&gt;
&lt;p&gt;Now, for people in my line of work, the more troubling&amp;nbsp;part:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Now, fade off with me into dream-land. Pretend that we had some master list. This master list
contained tons of networks around the world, and the personal accounts of admins of each of
those networks. And any time you wanted to target a new network, you could just find the admin
associated with it, queue his accounts up for &lt;span class="caps"&gt;QUANTUM&lt;/span&gt;, get access to his box and proceed to pwn
the network. Wouldn&amp;#8217;t that be&amp;nbsp;swell?&lt;/p&gt;
&lt;p&gt;(S/&lt;span class="caps"&gt;SI&lt;/span&gt;//&lt;span class="caps"&gt;REL&lt;/span&gt;) Well, you can stop dreaming my friends, I think it&amp;#8217;s possible (at least kinda&amp;nbsp;partially).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So&amp;#8230; we&amp;#8217;re talking about deliberately targeting the personal accounts of innocent third parties,
in order to compromise their credentials to also innocent networks, in order to eventually
gain access to the information of a target. This seems so horribly illegal I can barely
explain. It&amp;#8217;s also a direct affront to the people in my industry who have an ethical obligation
to protect the data of their employers, customers and users against illegal disclosure. And,
maybe even more troubling, it&amp;#8217;s being perpetrated by other people &amp;#8220;in our industry&amp;#8221;, other
technical people, who obviously have a very clear picture of exactly what they&amp;#8217;re&amp;nbsp;doing.&lt;/p&gt;
&lt;p&gt;My first thought was to make an analogy between our resposibility as those &amp;#8220;with the keys to
the kingdom&amp;#8221; to a more legally entrenched privacy, like that between a doctor and their patient,
or between a lawyer and their client, that would be much more obviously illegal for the government
to breach. But, apparently that&amp;#8217;s an all-too-correct analogy, since it came to light last month
that the &lt;a href="http://www.nytimes.com/2014/02/16/us/eavesdropping-ensnared-american-law-firm.html"&gt;&lt;span class="caps"&gt;NSA&lt;/span&gt; was intercepting privileged lawyer-client communications through the use of a
foreign intermediary, namely Australia&lt;/a&gt;.
Given the intelligence allicances between the &lt;span class="caps"&gt;US&lt;/span&gt;, the &lt;span class="caps"&gt;UK&lt;/span&gt; and Australia, and the scope of what
has been already disclosed, I find it entirely probable that in all of these leaked documents
that discuss doing this to &amp;#8220;foreign&amp;#8221; entities only, the reality is that to do the same to &lt;span class="caps"&gt;US&lt;/span&gt;
citizens, it&amp;#8217;s as simple as logging in to the Australian or &lt;span class="caps"&gt;UK&lt;/span&gt; equivalent&amp;nbsp;system.&lt;/p&gt;
&lt;p&gt;This is truly disturbing to me. I continue to feel that (1) if the &lt;span class="caps"&gt;NSA&lt;/span&gt; had provable cause to
collect this information, they&amp;#8217;d obtain a warrant like the Constitution says they have to,
and (2) their electronic data collection is the equivalent of wiretapping every phone in the
country and hoping for something useful - which has been continually held to be unconstitutional,
but because of the nature (already digital) of electronic communications, it&amp;#8217;s actually feasible
to&amp;nbsp;do.&lt;/p&gt;
&lt;p&gt;On a related note, a while ago I enabled &lt;a href="http://en.wikipedia.org/wiki/Time-based_One-time_Password_Algorithm"&gt;&lt;span class="caps"&gt;TOTP&lt;/span&gt;&lt;/a&gt;-based
two factor authentication on a number of my accounts (Google, GitHub, &lt;span class="caps"&gt;AWS&lt;/span&gt;, etc.) to try
and keep them more secure against the possibility of a password compromise. Yes, that still works
to keep an unscrupulous person out of them. However, if &lt;a href="http://www.washingtonpost.com/blogs/the-switch/wp/2013/08/24/loveint-when-nsa-officers-use-their-spying-power-on-love-interests/"&gt;&lt;span class="caps"&gt;NSA&lt;/span&gt; officials use classified systems to
spy on love interests&lt;/a&gt;,
it&amp;#8217;s entirely possible that an unscrupulous &lt;span class="caps"&gt;NSA&lt;/span&gt; employee (or even worse, someone who manages to
compromise the &lt;span class="caps"&gt;NSA&lt;/span&gt;&amp;#8217;s systems? Though I imagine they only use in-house-developed security for now-obvious reasons)
could decide to compromise an individual&amp;#8217;s accounts for personal gain. Given all this news, I
find it highly unlikely that such an event would ever be reported to the proper oversight authorities,
let alone become public knowledge or known to the&amp;nbsp;victim.&lt;/p&gt;
&lt;p&gt;However, if we look at some of the information about what the &lt;span class="caps"&gt;NSA&lt;/span&gt; is doing, like their
&lt;a href="http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/"&gt;&lt;span class="caps"&gt;NSA&lt;/span&gt;&amp;#8217;s Utah Data Center&lt;/a&gt;
and their plans for an &lt;a href="http://www.theregister.co.uk/2014/01/03/snowden_docs_show_nsa_building_encryptioncracking_quantum_system/"&gt;encryption-cracking quantum computer&lt;/a&gt;,
and assume that they probably have a datacenter full of FPGAs, it&amp;#8217;s entirely conceivable
that they can calculate this faster than most people think possible. On the other hand,
if they have passive taps on backbone providers, it&amp;#8217;s also possible they can just hijack
a session with the click of a&amp;nbsp;mouse.&lt;/p&gt;
&lt;p&gt;I don&amp;#8217;t want to sound like too much of a nut. I&amp;#8217;ve never been terribly concerned about the
government snooping on my data because, well, I&amp;#8217;m not doing anything illegal. And aside from
my stance in favor of tighter controls and more electronic freedom, I&amp;#8217;m not in many groups
that I think would be targeted. However, I do feel very strongly about what&amp;#8217;s going on in general
(we already &lt;a href="http://www.marquette.edu/library/archives/Mss/JRM/JRM-main.shtml"&gt;learned&lt;/a&gt; that
government records on individuals&amp;#8217; activities can be horribly misused). Even more so, I&amp;#8217;m
deeply disturbed that &lt;em&gt;my&lt;/em&gt; personal data and accounts could be compromised simply as a way
for a government employee to gain access to my employer&amp;#8217;s computer systems, and then to
those of our employees and&amp;nbsp;customers.&lt;/p&gt;</content><category term="nsa"></category><category term="government"></category><category term="privacy"></category><category term="security"></category></entry><entry><title>Python script to backup DisqusÂ comments</title><link href="http://blog.jasonantman.com/2014/03/python-script-to-backup-disqus-comments/" rel="alternate"></link><published>2014-03-01T19:01:00-05:00</published><updated>2014-03-01T19:01:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-03-01:/2014/03/python-script-to-backup-disqus-comments/</id><summary type="html">&lt;p&gt;Quick Python script to backup Disqus comments for a&amp;nbsp;forum&lt;/p&gt;</summary><content type="html">&lt;p&gt;Since I just &lt;a href="/2014/03/wordpress-to-pelican-with-disqus-comments"&gt;switched this blog to using Disqus for commenting&lt;/a&gt;,
I wanted a way to back up comments in case something goes wrong (like,
Disqus going the way of del.icio.us&amp;nbsp;bookmarking).&lt;/p&gt;
&lt;p&gt;I whipped up a quick &lt;a href="https://github.com/jantman/misc-scripts/blob/master/disqus_backup.py"&gt;Python script&lt;/a&gt;
using the official &lt;a href="https://github.com/disqus/disqus-python"&gt;Disqus Python &lt;span class="caps"&gt;API&lt;/span&gt; client&lt;/a&gt;. It grabs the forum details,
threads list and posts (comments) list, and writes them out to a &lt;span class="caps"&gt;JSON&lt;/span&gt;&amp;nbsp;file.&lt;/p&gt;
&lt;p&gt;It doesn&amp;#8217;t have any restore feature, but it captures all of the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;My first test made it look like there &lt;em&gt;may&lt;/em&gt; be some posts and theads missing (my import from
wordpress showed 56 threads and 146 comments, but this script only grabbed 52 and 125 respectively),
so exercise some caution until I verify what the problem is. If you happen to figure it out,
please submit a&amp;nbsp;&lt;span class="caps"&gt;PR&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The script is available on GitHub at &lt;a href="https://github.com/jantman/misc-scripts/blob/master/disqus_backup.py"&gt;https://github.com/jantman/misc-scripts/blob/master/disqus_backup.py&lt;/a&gt;.&lt;/p&gt;</content><category term="pelican"></category><category term="disqus"></category><category term="python"></category></entry><entry><title>Blog Moved from Self-hosted WordPress to Pelican on GitHubÂ Pages</title><link href="http://blog.jasonantman.com/2014/03/blog-moved-from-self-hosted-wordpress-to-pelican-on-github-pages/" rel="alternate"></link><published>2014-03-01T14:14:00-05:00</published><updated>2014-03-01T14:14:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-03-01:/2014/03/blog-moved-from-self-hosted-wordpress-to-pelican-on-github-pages/</id><summary type="html">&lt;p&gt;My blog is finally migrated from self-hosted WordPress to Pelican on GitHub Pages. Quite an&amp;nbsp;ordeal.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I just finally finished my migration from self-hosted WordPress to &lt;a href="http://getpelican.com"&gt;Pelican&lt;/a&gt;, a Python-based
static site generator, hosted on &lt;a href="http://pages.github.com/"&gt;GitHub Pages&lt;/a&gt;. It&amp;#8217;s not only easier and free, but also the
first step in my plan to migrate off of my &lt;a href="http://linode.com"&gt;Linode&lt;/a&gt; &lt;span class="caps"&gt;VM&lt;/span&gt; and onto a mix of &lt;span class="caps"&gt;EC2&lt;/span&gt; and free&amp;nbsp;services.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m sure this post will be &lt;a href="/2009/02/wordpress-installation-finished/"&gt;around in five years&lt;/a&gt; when there&amp;#8217;s a smarter way
to do all this, but until then&amp;#8230;&amp;nbsp;yay!&lt;/p&gt;
&lt;p&gt;I hit a number of bumps during the migration, mainly around &lt;a href="/2014/02/converting-wordpress-posts-to-pelican-markdown/"&gt;Migrating &lt;span class="caps"&gt;HTML&lt;/span&gt; posts from WordPress to Markdown in Pelican&lt;/a&gt;
and migrating &lt;a href="/2014/03/wordpress-to-pelican-with-disqus-comments/"&gt;WordPress comments to Disqus&lt;/a&gt;, but in the end everything
seems to be working. Hopefully someone will find this and save a few hours or days of work if they try the same&amp;nbsp;thing.&lt;/p&gt;
&lt;p&gt;Post-go-live I still had some issues - Disqus was displaying an&amp;nbsp;error:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We were unable to load Disqus. If you are a moderator please see our troubleshooting&amp;nbsp;guide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;on all posts created after the WordPress migration, and FeedBurner rejected my attempts to change the &lt;span class="caps"&gt;RSS&lt;/span&gt; feed &lt;span class="caps"&gt;URL&lt;/span&gt; to
its new value (though I&amp;#8217;m pretty sure that&amp;#8217;s because I neglected to drop the &lt;span class="caps"&gt;TTL&lt;/span&gt; on the &lt;span class="caps"&gt;DNS&lt;/span&gt; record, and I can&amp;#8217;t
find a way to tell Feedburner to purge it from&amp;nbsp;cache).&lt;/p&gt;</content><category term="blog"></category><category term="wordpress"></category><category term="pelican"></category><category term="github"></category></entry><entry><title>Wordpress to Pelican with DisqusÂ comments</title><link href="http://blog.jasonantman.com/2014/03/wordpress-to-pelican-with-disqus-comments/" rel="alternate"></link><published>2014-03-01T09:09:00-05:00</published><updated>2014-03-01T09:09:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-03-01:/2014/03/wordpress-to-pelican-with-disqus-comments/</id><summary type="html">&lt;p&gt;Solutions to some of the many problems converting a WordPress blog to a Pelican blog with Disqus for&amp;nbsp;commenting.&lt;/p&gt;</summary><content type="html">&lt;p&gt;This is the second part of my WordPress to &lt;a href="http://getpelican.com"&gt;Pelican&lt;/a&gt; conversion saga.
In the &lt;a href="/2014/03/converting-wordpress-posts-to-pelican-markdown/"&gt;last post&lt;/a&gt; I ran through some
of the issues that I faced when converting the posts themselves, setting up my theme and settings,
etc. In this post, I&amp;#8217;ll discuss the saga of moving from WordPress comments to Disqus&amp;nbsp;comments.&lt;/p&gt;
&lt;p&gt;Be sure to read &lt;strong&gt;all&lt;/strong&gt; of this before trying it yourself, as I had some serious problems with my
first&amp;nbsp;attempt.&lt;/p&gt;
&lt;h2 id="initial-import-to-disqus"&gt;Initial Import to&amp;nbsp;Disqus&lt;/h2&gt;
&lt;p&gt;Initially, I installed the Disqus WordPress plugin as instructed in
&lt;a href="http://help.disqus.com/customer/portal/articles/466255-importing-comments-from-wordpress"&gt;Disqus&amp;#8217; Import from WordPress documentation&lt;/a&gt;.
The automatic import imported three of 134 comments, and froze there,
even though the status said it was 100% complete. I emailed Disqus&amp;#8217; support,
and was told that this meant the import failed (even though there was no explicit
notification, their admin &lt;span class="caps"&gt;UI&lt;/span&gt; said the import was successful) and I had to manually
import my comments. I did this, as instructed in the same docs, by disabling all
plugins except for Disqus and generating an &lt;span class="caps"&gt;XML&lt;/span&gt; export from WordPress, then re-enabling
the plugins, and uploading the export to Disqus. This time, I ended up with all 134
comments in Disqus, so I assumed that all went&amp;nbsp;well.&lt;/p&gt;
&lt;h2 id="previewing-comments-in-pelican"&gt;Previewing Comments in&amp;nbsp;Pelican&lt;/h2&gt;
&lt;p&gt;I added my Disqus &lt;code&gt;shortname&lt;/code&gt; to the &lt;code&gt;DISQUS_SITENAME&lt;/code&gt; field in &lt;code&gt;pelicanconf.py&lt;/code&gt; and
re-built. I ended up having an issue with &lt;code&gt;SITE_URL&lt;/code&gt; being set incorrectly for some testing
that I did, so that killed 10 minutes. I rebuilt locally with &lt;code&gt;SITE_URL&lt;/code&gt; not defined, and
then used &lt;code&gt;fab serve&lt;/code&gt; to serve locally. I was using my
&lt;a href="/2014/01/planning-migration-from-wordpress-to-static-site/"&gt;Planning Migration from Wordpress to Static Site&lt;/a&gt;
post to test, as it was both the most recent post, and had a five comments in WordPress, which imported
correctly into Disqus and were visible both in the Disqus moderation tool and on the now-Disqus-powered
WordPress&amp;nbsp;blog.&lt;/p&gt;
&lt;p&gt;Once I rebuilt with &lt;code&gt;DISQUS_SITENAME&lt;/code&gt; set and served locally with SimpleHTTPServer (&lt;code&gt;fab serve&lt;/code&gt;),
I checked the post and saw only a &amp;#8220;We were unable to load Disqus&amp;#8221; message below the post. It contained
a &lt;a href="http://help.disqus.com/customer/portal/articles/472007-i-m-receiving-the-message-%22we-were-unable-to-load-disqus-%22"&gt;link to their help article for that problem&lt;/a&gt;,
which pointed me at a domain mismatch/different origin problem. As indicated on that page,
I went to Settings -&amp;gt; Advanced in the Disqus admin, found the &amp;#8220;Trusted Domains&amp;#8221; box, and added
both my test domain (newblog.jasonantman.com - pointing at GitHub pages until I was ready to
shut WordPress down and actually move the live site) and &amp;#8220;localhost&amp;#8221; for testing, and&amp;nbsp;saved.&lt;/p&gt;
&lt;p&gt;I refreshed the page I was looking at, and now could see the Disqus commenting below my post,
but it wasn&amp;#8217;t showing any of the comments&amp;nbsp;yet.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Disqus commenting with no comments" src="/GFX/disqus_wrong_url.png"&gt;&lt;/p&gt;
&lt;p&gt;I pulled up the source of the page, and saw in the Disqus javascript just below the post&amp;nbsp;content:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;            &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_shortname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;jasonantman&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// required: replace example with your forum shortname&lt;/span&gt;
            &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_identifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;planning-migration-from-wordpress-to-static-site&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;../../../2014/01/planning-migration-from-wordpress-to-static-site/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Everything looked &lt;span class="caps"&gt;OK&lt;/span&gt; to me except for &lt;code&gt;disqus_url&lt;/code&gt;, which I&amp;#8217;d seen mention of on the
&lt;a href="http://help.disqus.com/customer/portal/articles/472007-i-m-receiving-the-message-%22we-were-unable-to-load-disqus-%22"&gt;help page&lt;/a&gt;
I&amp;#8217;d just been looking at. Sure enough, it indicated that the &lt;code&gt;disqus_url&lt;/code&gt; var must be
an absolute &lt;span class="caps"&gt;URL&lt;/span&gt; to the post, not a relative path. I assume this was because I&amp;#8217;d generated
the content without having &lt;code&gt;SITE_URL&lt;/code&gt; set, so I hand-edited the generated page to change this
to the correct &lt;span class="caps"&gt;URL&lt;/span&gt;, http://blog.jasonantman.com/2014/01/planning-migration-from-wordpress-to-static-site/,
and tested again. Unfortunately, still zero&amp;nbsp;comments.&lt;/p&gt;
&lt;h2 id="wordpress-disqus-plugin-permalinks"&gt;WordPress Disqus Plugin&amp;nbsp;Permalinks&lt;/h2&gt;
&lt;p&gt;Fearing the worst, I pulled up the same post on my now-Disqus-powered WordPress blog,
and took a peek at the source. The javascript over there revealed a&amp;nbsp;problem:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;http://blog.jasonantman.com/2014/01/planning-migration-from-wordpress-to-static-site/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_identifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;1546 http://blog.jasonantman.com/?p=1546&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_container_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;disqus_thread&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_domain&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;disqus.com&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_shortname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;jasonantman&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Planning Migration from WordPress to Static Site&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While the &lt;span class="caps"&gt;URL&lt;/span&gt; is correct, the Disqus WordPress plugin uses the WordPress
post &lt;span class="caps"&gt;ID&lt;/span&gt; and permalink for the &amp;#8220;identifier&amp;#8221;, but the Pelican plugin uses the slug.
That&amp;#8217;s a problem, as my Pelican site will have the same URLs, but the WordPress
post-&lt;span class="caps"&gt;ID&lt;/span&gt;-based permalinks are gone (since it&amp;#8217;s a static site, and there&amp;#8217;s no easy
way of replicating things that are query param based). The WordPress post IDs
are thrown out by Pelican, so there&amp;#8217;s no way to connect the&amp;nbsp;two.&lt;/p&gt;
&lt;p&gt;Even worse, I remembered that Disqus&amp;#8217;
&lt;a href="http://help.disqus.com/customer/portal/articles/466255-importing-comments-from-wordpress"&gt;Importing Comments from WordPress help page&lt;/a&gt;
clearly&amp;nbsp;stated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Imported comments can&amp;#8217;t be permanently deleted. Consider following our &lt;a href="http://help.disqus.com/customer/portal/articles/1053796-best-practices-for-staging-development-and-preview-sites"&gt;guidelines for development sites&lt;/a&gt; to make sure the data you&amp;#8217;re importing is correct. You can &lt;a href="http://disqus.com/register"&gt;register a new forum&lt;/a&gt; if you have imported the wrong&amp;nbsp;comments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="solution-to-permalink-issue"&gt;Solution to Permalink&amp;nbsp;Issue&lt;/h2&gt;
&lt;p&gt;Not seeing any way around it, I figured it was time to &amp;#8220;bite the bullet&amp;#8221;. I disabled the Disqus plugin
in WordPress and then installed and activated the
&lt;a href="http://wordpress.org/extend/plugins/code-freeze/"&gt;WordPress Code Freeze Plugin&lt;/a&gt;
to disable comments. (&lt;em&gt;Note&lt;/em&gt; ironically, this plugin also uses JavaScript to disable your ability to
deactivate plugins, including itself. So before you activate it, copy the &amp;#8220;Activate&amp;#8221; link and save it
somewhere; changing &lt;code&gt;action=activate&lt;/code&gt; to &lt;code&gt;action=deactivate&lt;/code&gt; will let you get rid of it if you&amp;nbsp;want).&lt;/p&gt;
&lt;p&gt;Disqus has some documentation on &lt;a href="http://help.disqus.com/customer/portal/articles/1104797-importing-exporting"&gt;Importing and Exporting&lt;/a&gt;
which includes &lt;a href="http://help.disqus.com/customer/portal/articles/472150"&gt;Custom &lt;span class="caps"&gt;XML&lt;/span&gt; Imports&lt;/a&gt; based on the
WordPress &lt;span class="caps"&gt;XML&lt;/span&gt; export format. So, I figured that I just had to decide that WordPress commenting would be
turned off, and do a point-in-time migration to Disqus (maybe circling back to hack the Disqus &lt;span class="caps"&gt;WP&lt;/span&gt; plugin
to keep comments working there for the time&amp;nbsp;being).&lt;/p&gt;
&lt;p&gt;Before anything else, I decided to actually set up a test forum/site in Disqus like they suggested.
I updated the &lt;code&gt;DISQUS_SITENAME&lt;/code&gt; in &lt;code&gt;pelicanconf.py&lt;/code&gt;, and then started in on the &lt;span class="caps"&gt;XML&lt;/span&gt; munging. The
&lt;a href="http://help.disqus.com/customer/portal/articles/472150"&gt;Custom &lt;span class="caps"&gt;XML&lt;/span&gt; Imports&lt;/a&gt; documentation implies
that the import engine recognizes a &lt;code&gt;dsq:thread_identifier&lt;/code&gt; &lt;span class="caps"&gt;XML&lt;/span&gt; element that holds the thread identifier,
but that element wasn&amp;#8217;t present in my WordPress &lt;span class="caps"&gt;XML&lt;/span&gt; export. It appeared that Disqus was concatenating the
&lt;code&gt;wp:post_id&lt;/code&gt; and &lt;code&gt;guid&lt;/code&gt; fields (with a space in between) to come up with the&amp;nbsp;identifier.&lt;/p&gt;
&lt;p&gt;So, I wrote a script (&lt;a href="https://github.com/jantman/blog/blob/master/dev/wp-move/wp_comment_xml_munge.py"&gt;wp_comment_xml_munge.py&lt;/a&gt;)
using &lt;a href="http://lxml.de/"&gt;lxml&lt;/a&gt; that adds the &lt;code&gt;dsq:&lt;/code&gt; namespace to the WordPress &lt;span class="caps"&gt;XML&lt;/span&gt; export (unfortunately using
string replacement and a temp file, due to a &lt;a href="https://bugs.launchpad.net/lxml/+bug/555602"&gt;bug in lxml&lt;/a&gt;)
and then adds the &lt;code&gt;dsq:thread_identifier&lt;/code&gt; tag to each post item, setting its value to the same
string as &lt;code&gt;wp:post_name&lt;/code&gt;, the &lt;span class="caps"&gt;URL&lt;/span&gt; slug (and post identifier in&amp;nbsp;Pelican).&lt;/p&gt;
&lt;p&gt;I imported the &lt;span class="caps"&gt;XML&lt;/span&gt; written by the script into my test forum in Disqus and rebuilt the Pelican content.
Magically, the first time I looked, the comments were&amp;nbsp;there.&lt;/p&gt;
&lt;p&gt;Now, time to see if I could get the same effect with the existing Disqus&amp;nbsp;site/forum:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In the Disqus moderation interface, delete all comments. You&amp;#8217;ll have to do this in batches of 10, as that&amp;#8217;s
   how they&amp;#8217;re paged in the interface. The comments don&amp;#8217;t seem to be permanently deleted, but do show as&amp;nbsp;&amp;#8220;deleted&amp;#8221;.&lt;/li&gt;
&lt;li&gt;Go to &lt;a href="http://import.disqus.com"&gt;import.disqus.com&lt;/a&gt; and select your &amp;#8220;forum&amp;#8221; (site). You should see your existing
   (previous) import, as 100% complete, with the correct count of threads and comments. Do another import with the
   &lt;code&gt;_disqus.xml&lt;/code&gt; munged &lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;export.&lt;/li&gt;
&lt;li&gt;Comments should now be linked to the correct post in&amp;nbsp;Pelican.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, Pelican seemed to be working, but WordPress was still left with only the old internal commenting,
and that was disabled by the Code Freeze plugin. I probably could have manually patched the Disqus plugin to
reflect the new thread identifiers, but instead, I chose to just push forward with the switch from WordPress to&amp;nbsp;Pelican.&lt;/p&gt;
&lt;p&gt;That only took a few hours, and I&amp;#8217;m happy to say that I&amp;#8217;m now up and running with a Pelican blog, hosted for free
by GitHub&amp;nbsp;Pages.&lt;/p&gt;</content><category term="wordpress"></category><category term="pelican"></category><category term="blog"></category><category term="disqus"></category><category term="comments"></category></entry><entry><title>Converting WordPress Posts to PelicanÂ MarkDown</title><link href="http://blog.jasonantman.com/2014/02/converting-wordpress-posts-to-pelican-markdown/" rel="alternate"></link><published>2014-02-28T22:21:00-05:00</published><updated>2014-02-28T22:21:00-05:00</updated><author><name>Jason Antman</name></author><id>tag:blog.jasonantman.com,2014-02-28:/2014/02/converting-wordpress-posts-to-pelican-markdown/</id><summary type="html">&lt;p&gt;My adventures converting my WordPress blog to Pelican, the problems I encountered, and how I solved&amp;nbsp;them.&lt;/p&gt;</summary><content type="html">&lt;p&gt;A few weeks ago, I
&lt;a href="/2014/01/planning-migration-from-wordpress-to-static-site/"&gt;posted&lt;/a&gt; about my
plans to convert my self-hosted WordPress blog to a static site using a static
blog generator. Since then, I&amp;#8217;ve decided to stop working on my exhaustive
&lt;a href="https://docs.google.com/spreadsheet/ccc?key=0AnHh-ye5DNiNdF9DWkJrT2kzSkNsNVp6cjMzLXJ6VEE&amp;amp;usp=sharing"&gt;static blog generator comparison spreadsheet&lt;/a&gt;
and just try &lt;a href="http://getpelican.com"&gt;Pelican&lt;/a&gt; - mainly because it&amp;#8217;s written in
Python which is my current strongest language, comes highly recommended, seems
to have most of the features I want, and seems to be easily&amp;nbsp;extensible.&lt;/p&gt;
&lt;p&gt;So, I walked through the documentation for the latest version (3.3.0), started
a &lt;a href="https://github.com/jantman/blog"&gt;GitHub repo&lt;/a&gt;, and tweaked a bunch of
settings. The repo is public, so if you want to take a look behind the scenes,
see my &lt;a href="https://github.com/jantman/blog/blob/master/fabfile.py"&gt;fabfile&lt;/a&gt;,
etc. feel&amp;nbsp;free. &lt;/p&gt;
&lt;h2 id="initial-wordpress-import-attempt"&gt;Initial WordPress Import&amp;nbsp;Attempt&lt;/h2&gt;
&lt;p&gt;I used the WordPress &lt;span class="caps"&gt;XML&lt;/span&gt; Export tool, as instructed in the &lt;a href="http://docs.getpelican.com/en/latest/importer.html"&gt;Pelican Importer documentation&lt;/a&gt;.
At first, I attempted to do a more-or-less default import from WordPress using
the &lt;code&gt;pelican-import&lt;/code&gt; tool, which writes rST, and then build the blog. What I
ended up with was thousands of errors complaining about &amp;#8220;Inline interpreted
text or phrase reference start-string without end-string&amp;#8221;, &amp;#8220;Explicit markup
ends without a blank line; unexpected uninden&amp;#8221;, &amp;#8220;malformed hyperlink target&amp;#8221;,
&amp;#8220;Unknown target name&amp;#8221; on all of my links, and a bevy of other Docutils
errors. It was so utterly awful that I gave&amp;nbsp;up.&lt;/p&gt;
&lt;h2 id="wordpress-import-as-markdown"&gt;WordPress Import as&amp;nbsp;MarkDown&lt;/h2&gt;
&lt;p&gt;Next I tried importing as MarkDown instead of rST,&amp;nbsp;using:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pelican-import --markup markdown --wpfile -o content/ --dir-page jasonantman039sblog.wordpress.2014-01-11.xml
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That built without errors, and the posts looked somewhat right out of the
box, without any of the previous thousands of errors. And the links looked
mostly right - even the captions for images. Though I&amp;#8217;m working at a Python
shop and writing a lot of Python these days, my knowledge of MarkDown is still
much better than rST, so this is fine for me. (I even wrote a &lt;code&gt;fab post&lt;/code&gt; task
that prompts for a title, generates all of the post metadata, writes it to the
right file, and opens up an editor on&amp;nbsp;it.)&lt;/p&gt;
&lt;p&gt;The first problem was that the import script gave me one &amp;#8220;content&amp;#8221; directory
with 346 &amp;#8220;.md&amp;#8221; files in it - not exactly easy to work with. Luckily the
metadata was right, so a quick little
&lt;a href="https://github.com/jantman/blog/blob/master/move_wordpress.sh"&gt;bash script&lt;/a&gt;
moved the posts into a &lt;span class="caps"&gt;YYYY&lt;/span&gt;/&lt;span class="caps"&gt;MM&lt;/span&gt; directory&amp;nbsp;hierarchy.&lt;/p&gt;
&lt;h2 id="obvious-problems-with-imported-posts"&gt;Obvious Problems with Imported&amp;nbsp;Posts&lt;/h2&gt;
&lt;p&gt;After getting the MarkDown import working, and the posts moved to the proper
paths, I was still having some&amp;nbsp;issues&amp;#8230;&lt;/p&gt;
&lt;h3 id="syntax-hilighting-gone"&gt;Syntax Hilighting&amp;nbsp;Gone&lt;/h3&gt;
&lt;p&gt;In WordPress, I was using the
&lt;a href="http://wordpress.org/extend/plugins/wp-syntax/"&gt;&lt;span class="caps"&gt;WP&lt;/span&gt;-Syntax&lt;/a&gt; plugin to perform
syntax hilighting via &lt;a href="http://qbnz.com/highlighter/"&gt;GeSHi&lt;/a&gt;. The plugin uses
pre tags with a &lt;code&gt;lang=&lt;/code&gt; attribute to specify the language,&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;lt;pre lang=&amp;quot;bash&amp;quot;&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Unfortunately, these translated to some really ugly MarkDown fenced blocks&amp;nbsp;like:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;~~~~ {lang=&amp;quot;bash&amp;quot;}
cp /boot/efi/&lt;span class="caps"&gt;EFI&lt;/span&gt;/fedora/grub.cfg /boot/efi/&lt;span class="caps"&gt;EFI&lt;/span&gt;/fedora/grub.cfg.bak
echo &amp;#39;GRUB_DISABLE_OS_PROBER=&amp;quot;true&amp;quot;&amp;#39; &amp;gt;&amp;gt; /etc/default/grub
grub2-mkconfig &amp;gt; /boot/efi/&lt;span class="caps"&gt;EFI&lt;/span&gt;/fedora/grub.cfg
~~~~
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;that seem to be just a bit off from what MarkDown/Pygments can handle. The
places where I just used bare &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt; blocks translated&amp;nbsp;fine.&lt;/p&gt;
&lt;p&gt;http://blog.gastove.com/2013-09-17_enabling_line_numbers_for_pygments.html&lt;/p&gt;
&lt;p&gt;Fixed this by using fenced blocks with the &amp;#8216;lang=&amp;#8217; stuff removed, and in class
syntax like the MarkDown docs suggest. Some four-tab-indents with
:::identifier&amp;nbsp;work.&lt;/p&gt;
&lt;h3 id="broken-links"&gt;Broken&amp;nbsp;Links&lt;/h3&gt;
&lt;p&gt;It seems that something in the conversion process introduced line wraps (could
it really be Pandoc itself???) Unfortunately, this wreaks havoc with any
explicit reference links
that use long (long enough to break across lines) titles, depending on where
they are in the line. It seems that in some places they end up breaking
differently in the link in the text and in the link definition, which MarkDown misses, and
then renders broken links and plain text of the link table at the bottom of
the page. Manually removing the line breaks and any extraneous spaces seems to
fix&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;So, yes, Pandoc was doing this because of the &lt;code&gt;--reference-links&lt;/code&gt; parameter
that &lt;code&gt;pelican-import&lt;/code&gt; was calling it with. There was an
&lt;a href="https://github.com/getpelican/pelican/issues/348"&gt;issue&lt;/a&gt; and
&lt;a href="https://github.com/getpelican/pelican/pull/642"&gt;pull request&lt;/a&gt; to fix this,
but when I started with Pelican the last release was 3.3.0 (4 months ago) and
the &lt;span class="caps"&gt;PR&lt;/span&gt; was merged after that. So, if you&amp;#8217;re having the same problem and the
latest release of Pelican is still 3.3.0, you might as well just apply
&lt;a href="https://github.com/getpelican/pelican/commit/83e4d35b44a422ee8d4b077f505970d03e555f45"&gt;the patch&lt;/a&gt;
yourself - it&amp;#8217;s just a very simple removal of a parameter in
&lt;code&gt;pelican_import.py&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="overall-results"&gt;Overall&amp;nbsp;Results&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;m quite happy with the overall results. I also spent a &lt;em&gt;lot&lt;/em&gt; of time manually fixing
markup issues that didn&amp;#8217;t translate well through Pandoc, but I suppose that&amp;#8217;s to be
expected given that many of my older blog posts had &lt;span class="caps"&gt;HTML&lt;/span&gt;&amp;nbsp;issues.&lt;/p&gt;</content><category term="pelican"></category><category term="wordpress"></category><category term="blog"></category><category term="markdown"></category></entry><entry><title>Planning Migration from WordPress to StaticÂ Site</title><link href="http://blog.jasonantman.com/2014/01/planning-migration-from-wordpress-to-static-site/" rel="alternate"></link><published>2014-01-01T15:15:00-05:00</published><updated>2014-01-01T15:15:00-05:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2014-01-01:/2014/01/planning-migration-from-wordpress-to-static-site/</id><summary type="html">&lt;p&gt;Right now, this blog, my email, and a whole bunch of other services are
hosted on a &lt;a href="http://linode.com"&gt;Linode&lt;/a&gt; Xen &lt;span class="caps"&gt;VM&lt;/span&gt;. I don&amp;#8217;t really keep up
to date with administration and upgrades the way I used to, and
honestly, I&amp;#8217;d rather spend my time working on other things (like â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Right now, this blog, my email, and a whole bunch of other services are
hosted on a &lt;a href="http://linode.com"&gt;Linode&lt;/a&gt; Xen &lt;span class="caps"&gt;VM&lt;/span&gt;. I don&amp;#8217;t really keep up
to date with administration and upgrades the way I used to, and
honestly, I&amp;#8217;d rather spend my time working on other things (like
actually writing all of the blog posts that I&amp;#8217;ve been planning to. The
first thing I&amp;#8217;ve identified for migration is this blog itself. It&amp;#8217;s
currently on WordPress and, frankly, I don&amp;#8217;t either need nor like it.
But there are some features I like. I&amp;#8217;d like to end up with a static
site generator, hosted from either S3 or GitHub Pages. I know that means
I&amp;#8217;ll lost comments (unless I move to a third-party, &lt;span class="caps"&gt;JS&lt;/span&gt;-based comment
system like &lt;a href="http://disqus.com/"&gt;Disqus&lt;/a&gt;, which means I&amp;#8217;ll lose
&lt;em&gt;control&lt;/em&gt; over my comments) but I suppose I can live with that. What I
really want is something simple, static, cheap or free (that I&amp;#8217;ll likely
put behind a small ec2 instance running nginx for&amp;nbsp;redirects/rewrites).&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m still in the planning phase, and trying to come up with a
feature-by-feature comparison of my options. I&amp;#8217;ll likely post that when
I finally have it done (at the moment it&amp;#8217;s in a very rough &lt;a href="https://docs.google.com/spreadsheet/ccc?key=0AnHh-ye5DNiNdF9DWkJrT2kzSkNsNVp6cjMzLXJ6VEE&amp;amp;usp=sharing"&gt;Google Docs
spreadsheet&lt;/a&gt;).
I&amp;#8217;m trying to round up my static site generator options and see which
ones will do most, if not all, of what I want (though I still haven&amp;#8217;t
discounted using hosted wordpress if it comes down to it). Here are the
features I currently &amp;#8220;use&amp;#8221; (have) on my WordPress&amp;nbsp;blog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User-defined permalinks to&amp;nbsp;posts&lt;/li&gt;
&lt;li&gt;Overall &lt;span class="caps"&gt;RSS&lt;/span&gt; feed of blog (currently powered by FeedBurner) and of&amp;nbsp;comments)&lt;/li&gt;
&lt;li&gt;Categories (a post can be in multiple&amp;nbsp;categories)&lt;/li&gt;
&lt;li&gt;Tags&lt;/li&gt;
&lt;li&gt;Category and Tag&amp;nbsp;pages&lt;/li&gt;
&lt;li&gt;per-Category and per-Tag feeds&amp;nbsp;(&lt;span class="caps"&gt;RSS&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;Tag cloud &amp;#8220;widget&amp;#8221; in&amp;nbsp;sidebar&lt;/li&gt;
&lt;li&gt;Themes. I actually like my current &lt;span class="caps"&gt;WP&lt;/span&gt;&amp;nbsp;theme&amp;#8230;&lt;/li&gt;
&lt;li&gt;Visitor statistics (currently self-hosted
    &lt;a href="http://piwik.org/"&gt;Piwik&lt;/a&gt;, formerly Google&amp;nbsp;Analytics)&lt;/li&gt;
&lt;li&gt;Post publishing via cron&amp;#8217;ed script (&lt;em&gt;see below&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Draft/Pending status (i.e. let me save a partial post, and let me
    save a complete post but mark it &amp;#8220;pending&amp;#8221; so I can just publish it&amp;nbsp;later)&lt;/li&gt;
&lt;li&gt;Commenting (this will probably be the big sticking&amp;nbsp;point)&lt;/li&gt;
&lt;li&gt;Syntax&amp;nbsp;hilighting&lt;/li&gt;
&lt;li&gt;As &amp;#8220;weird&amp;#8221; as this is, I write all my posts in raw &lt;span class="caps"&gt;HTML&lt;/span&gt;, and am
    perfectly happy doing&amp;nbsp;that.&lt;/li&gt;
&lt;li&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Subscribe via Email&amp;#8221; FeedBurner&amp;nbsp;widget&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;Sitemap&lt;/li&gt;
&lt;li&gt;Twitter&amp;nbsp;box/widget&lt;/li&gt;
&lt;li&gt;Pingbacks (not that these are really useful for anything other than
    spam these&amp;nbsp;days)&lt;/li&gt;
&lt;li&gt;Automatic or manual post excerpts for feeds,&amp;nbsp;etc.&lt;/li&gt;
&lt;li&gt;Remote publishing via &lt;span class="caps"&gt;XML&lt;/span&gt;-&lt;span class="caps"&gt;RPC&lt;/span&gt;/Android app (not that I&amp;#8217;ve used it
    more than once or&amp;nbsp;twice)&lt;/li&gt;
&lt;li&gt;Advertising - I currently use Google AdSense on my blog. The revenue
    from my tiny hit count isn&amp;#8217;t enough to offset the cost of a Linode,
    but if I moved to a much less expensive hosting service, it might be
    worth considering (you can&amp;#8217;t run ads on the free hosted WordPress,
    and I doubt you can on GitHub Pages&amp;nbsp;either).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I should be updating this post when I do some more research and have a
comparison of the&amp;nbsp;options.&lt;/p&gt;
&lt;p&gt;Note on &amp;#8220;Post publishing via cron&amp;#8217;ed script&amp;#8221; - sometimes I sit down and
write half a dozen or so blog posts at a time. But I don&amp;#8217;t want them all
to show up immediately, and spam the few people who still use &lt;span class="caps"&gt;RSS&lt;/span&gt;
readers after the death of Google Reader. So I set the posts to
&amp;#8220;Pending&amp;#8221; status, and I have a cron&amp;#8217;ed script that runs every weekday
morning and publishes the one oldest &amp;#8220;pending&amp;#8221; post. Who knows if this
actually does any good or&amp;nbsp;not&amp;#8230;&lt;/p&gt;</content><category term="blog"></category><category term="jekyll"></category><category term="pelican"></category><category term="python"></category><category term="static site"></category><category term="wordpress"></category></entry><entry><title>Testing GPG KeyÂ Passphrases</title><link href="http://blog.jasonantman.com/2013/08/testing-gpg-key-passphrases/" rel="alternate"></link><published>2013-08-26T06:00:00-04:00</published><updated>2013-08-26T06:00:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-08-26:/2013/08/testing-gpg-key-passphrases/</id><summary type="html">&lt;p&gt;So hypothetically, you have a &lt;span class="caps"&gt;GPG&lt;/span&gt; public/private keypair (from a backup
or old computer), but you don&amp;#8217;t remember the passphrase. Here&amp;#8217;s a
relatively simple way to find it from a number of possible options. This
&lt;em&gt;requires&lt;/em&gt; that you have a computer secure enough to store the possible â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;So hypothetically, you have a &lt;span class="caps"&gt;GPG&lt;/span&gt; public/private keypair (from a backup
or old computer), but you don&amp;#8217;t remember the passphrase. Here&amp;#8217;s a
relatively simple way to find it from a number of possible options. This
&lt;em&gt;requires&lt;/em&gt; that you have a computer secure enough to store the possible
options in a text file. I&amp;#8217;d recommend storing that file on a
ramdisk/tmpfs, and using a temporary &lt;span class="caps"&gt;VM&lt;/span&gt; for this, which you&amp;#8217;ll wipe away
when you&amp;#8217;re&amp;nbsp;done.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Preparation:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You have an appropriately secure place to do this with &lt;span class="caps"&gt;GPG&lt;/span&gt;
    installed, and a safe place to store a text file of sample
    passphrases (i.e. a&amp;nbsp;ramdisk).&lt;/li&gt;
&lt;li&gt;Copy your backed up public and private keys to &lt;code&gt;~/.gnupg&lt;/code&gt; on that
    host. Let&amp;#8217;s assume they&amp;#8217;re called &lt;code&gt;TestUser_public.key&lt;/code&gt; and
    &lt;code&gt;TestUser_private.key&lt;/code&gt;. We&amp;#8217;re assuming that you &lt;span class="caps"&gt;KNOW&lt;/span&gt;, &lt;span class="caps"&gt;BEYOND&lt;/span&gt; A &lt;span class="caps"&gt;DOUBT&lt;/span&gt;
    that these are your keys (i.e. you got them from a secure offline
    backup medium, you&amp;#8217;ve verified against a printed key fingerprint,
    you&amp;#8217;ve verified the fingerprints against a
    &lt;a href="http://pgp.mit.edu/"&gt;keyserver&lt;/a&gt; that you know is authoritative for
    your keys,&amp;nbsp;etc.).&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;First, we import the public and private keys to&amp;nbsp;&lt;span class="caps"&gt;GPG&lt;/span&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;testuser:~$&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; .gnupg
&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; gpg --import TestUser_public.key 
&lt;span class="go"&gt;gpg: keyring `/home/testuser/.gnupg/secring.gpg` created&lt;/span&gt;
&lt;span class="go"&gt;gpg: key &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;: public key &amp;quot;Test User (Test User) &amp;quot; imported&lt;/span&gt;
&lt;span class="go"&gt;gpg: Total number processed: 1&lt;/span&gt;
&lt;span class="go"&gt;gpg:               imported: 1  (&lt;span class="caps"&gt;RSA&lt;/span&gt;: 1)&lt;/span&gt;

&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; gpg --allow-secret-key-import --import TestUser_secret.key 
&lt;span class="go"&gt;gpg: key &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;: secret key imported&lt;/span&gt;
&lt;span class="go"&gt;gpg: key &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;: &amp;quot;Test User (Test User) &amp;quot; not changed&lt;/span&gt;
&lt;span class="go"&gt;gpg: Total number processed: 1&lt;/span&gt;
&lt;span class="go"&gt;gpg:              unchanged: 1&lt;/span&gt;
&lt;span class="go"&gt;gpg:       secret keys read: 1&lt;/span&gt;
&lt;span class="go"&gt;gpg:   secret keys imported: 1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check that the keys are&amp;nbsp;there:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; gpg --list-keys
&lt;span class="go"&gt;/home/testuser/.gnupg/pubring.gpg&lt;/span&gt;
&lt;span class="go"&gt;--------------------------------&lt;/span&gt;
&lt;span class="go"&gt;pub   2048R/&lt;span class="caps"&gt;17AD8D3D&lt;/span&gt; 2013-08-24&lt;/span&gt;
&lt;span class="go"&gt;uid                  Test User (Test User) &lt;/span&gt;
&lt;span class="go"&gt;sub   2048R/&lt;span class="caps"&gt;40D9F35E&lt;/span&gt; 2013-08-24&lt;/span&gt;

&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; gpg --list-secret-keys
&lt;span class="go"&gt;/home/testuser/.gnupg/secring.gpg&lt;/span&gt;
&lt;span class="go"&gt;--------------------------------&lt;/span&gt;
&lt;span class="go"&gt;sec   2048R/&lt;span class="caps"&gt;17AD8D3D&lt;/span&gt; 2013-08-24&lt;/span&gt;
&lt;span class="go"&gt;uid                  Test User (Test User) &lt;/span&gt;
&lt;span class="go"&gt;ssb   2048R/&lt;span class="caps"&gt;40D9F35E&lt;/span&gt; 2013-08-24&lt;/span&gt;

&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note the fingerprint of the key which is, in this case, &lt;code&gt;17AD8D3D&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Testing&amp;nbsp;Passphrases:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Now that we have the keys imported, we&amp;#8217;re ready to test some
    passphrases. Enter your passphrases, one per line, in a text file.
    We&amp;#8217;re assuming that we&amp;#8217;re working on a totally secured host
    (ideally, a &lt;span class="caps"&gt;VM&lt;/span&gt; running on a standalone, non-networked machine) that
    will be destroyed when we&amp;#8217;re done. For added security, I&amp;#8217;d put this
    file on a ramdisk. In this example, the actual passphrase for the
    key is &amp;#8220;test&amp;#8221;. Here&amp;#8217;s our text&amp;nbsp;file:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; cat /tmp/passphrases 
&lt;span class="go"&gt;bad&lt;/span&gt;
&lt;span class="go"&gt;notgood&lt;/span&gt;
&lt;span class="go"&gt;notright&lt;/span&gt;
&lt;span class="go"&gt;test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, create a test data file to try to&amp;nbsp;sign/encrypt:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;test input&amp;quot;&lt;/span&gt; &amp;gt; /tmp/test.in
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now we run the actual test (see below for more&amp;nbsp;information&amp;#8230;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; p in &lt;span class="sb"&gt;`&lt;/span&gt;cat /tmp/passphrases&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$p&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; gpg -q --sign --local-user &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt; --passphrase-fd &lt;span class="m"&gt;0&lt;/span&gt; --output /dev/null --yes /tmp/test.in &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;CORRECT&lt;/span&gt; passphrase: &lt;/span&gt;&lt;span class="nv"&gt;$p&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;break&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="k"&gt;done&lt;/span&gt;
&lt;span class="go"&gt;Reading passphrase from file descriptor 0&lt;/span&gt;

&lt;span class="go"&gt;You need a passphrase to unlock the secret key for&lt;/span&gt;
&lt;span class="go"&gt;user: &amp;quot;Test User (Test User) &amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;2048-bit &lt;span class="caps"&gt;RSA&lt;/span&gt; key, &lt;span class="caps"&gt;ID&lt;/span&gt; &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;, created 2013-08-24&lt;/span&gt;

&lt;span class="go"&gt;gpg: skipped &amp;quot;&lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;&amp;quot;: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;gpg: signing failed: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;Reading passphrase from file descriptor 0&lt;/span&gt;

&lt;span class="go"&gt;You need a passphrase to unlock the secret key for&lt;/span&gt;
&lt;span class="go"&gt;user: &amp;quot;Test User (Test User) &amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;2048-bit &lt;span class="caps"&gt;RSA&lt;/span&gt; key, &lt;span class="caps"&gt;ID&lt;/span&gt; &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;, created 2013-08-24&lt;/span&gt;

&lt;span class="go"&gt;gpg: skipped &amp;quot;&lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;&amp;quot;: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;gpg: signing failed: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;Reading passphrase from file descriptor 0&lt;/span&gt;

&lt;span class="go"&gt;You need a passphrase to unlock the secret key for&lt;/span&gt;
&lt;span class="go"&gt;user: &amp;quot;Test User (Test User) &amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;2048-bit &lt;span class="caps"&gt;RSA&lt;/span&gt; key, &lt;span class="caps"&gt;ID&lt;/span&gt; &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;, created 2013-08-24&lt;/span&gt;

&lt;span class="go"&gt;gpg: skipped &amp;quot;&lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;&amp;quot;: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;gpg: signing failed: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;Reading passphrase from file descriptor 0&lt;/span&gt;

&lt;span class="go"&gt;You need a passphrase to unlock the secret key for&lt;/span&gt;
&lt;span class="go"&gt;user: &amp;quot;Test User (Test User) &amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;2048-bit &lt;span class="caps"&gt;RSA&lt;/span&gt; key, &lt;span class="caps"&gt;ID&lt;/span&gt; &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;, created 2013-08-24&lt;/span&gt;

&lt;span class="go"&gt;&lt;span class="caps"&gt;CORRECT&lt;/span&gt; passphrase: test&lt;/span&gt;
&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And there we have it, the working passphrase. I&amp;#8217;m sure there&amp;#8217;s a
    more efficient way to do this, and probably a more secure way, but
    I&amp;#8217;m not trying to brute-force someone&amp;#8217;s &lt;span class="caps"&gt;GPG&lt;/span&gt; key, I&amp;#8217;m trying to
    remember which one of my (many, many) passwords I used for a &lt;span class="caps"&gt;GPG&lt;/span&gt; key
    that I generated a decade&amp;nbsp;ago.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The actual command that we ran, rewritten with some linebreaks for
legibility,&amp;nbsp;is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;for&lt;/span&gt; p in &lt;span class="sb"&gt;`&lt;/span&gt;cat /tmp/passphrases&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$p&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; gpg -q --sign --local-user &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt; --passphrase-fd &lt;span class="m"&gt;0&lt;/span&gt; --output /dev/null --yes /tmp/test.in &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;CORRECT&lt;/span&gt; passphrase: &lt;/span&gt;&lt;span class="nv"&gt;$p&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;break&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This loops over each line in the passphrases file (each passphrase that
we want to try), and for each one, echoes the password and pipes it to
&lt;span class="caps"&gt;STDIN&lt;/span&gt; of &lt;code&gt;gpg&lt;/code&gt;, which tries to sign /tmp/test.in (sending the output
to /dev/null) using the key with &lt;span class="caps"&gt;ID&lt;/span&gt; &lt;code&gt;17AD8D3D&lt;/code&gt; (from #5 in the
Preparation steps above) and a password provided on &lt;span class="caps"&gt;STDIN&lt;/span&gt;. If the &lt;span class="caps"&gt;GPG&lt;/span&gt;
command succeeds, we echo the passphrase and stop looping through the
passphrases&amp;nbsp;file.&lt;/p&gt;
&lt;p&gt;I hope I wouldn&amp;#8217;t have to say this for anyone who&amp;#8217;s reading my blog, but
this information (as easy as it is to be figured out), is not to be used
for unethical&amp;nbsp;purposes.&lt;/p&gt;</content><category term="encryption"></category><category term="gnupg"></category><category term="gpg"></category><category term="key"></category><category term="passphrase"></category><category term="pgp"></category></entry><entry><title>Quick Tip: Timestamping bashÂ history</title><link href="http://blog.jasonantman.com/2013/06/quick-tip-timestamping-bash-history/" rel="alternate"></link><published>2013-06-11T07:09:00-04:00</published><updated>2013-06-11T07:09:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-06-11:/2013/06/quick-tip-timestamping-bash-history/</id><summary type="html">&lt;p&gt;Here&amp;#8217;s a tiny little snippet that I have in my &lt;code&gt;.bashrc&lt;/code&gt; which really
comes in handy when trying to figure out what I did on a system when.
One of the first things I do when (eek) building out or working on a
one-off machine (or setting up a â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Here&amp;#8217;s a tiny little snippet that I have in my &lt;code&gt;.bashrc&lt;/code&gt; which really
comes in handy when trying to figure out what I did on a system when.
One of the first things I do when (eek) building out or working on a
one-off machine (or setting up a new laptop/desktop, as I am right now)
is set this in bashrc for my user and root, so I can go back and
document the setup process with a little more ease and sanity. Just add
this (it&amp;#8217;s just a &lt;a href="http://linux.die.net/man/3/strftime"&gt;strftime (3)&lt;/a&gt;
format string &lt;a href="http://www.gnu.org/software/bash/manual/bashref.html#index-HISTTIMEFORMAT"&gt;according to the
docs&lt;/a&gt;,
so adjust as desired) to &lt;code&gt;.bashrc&lt;/code&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;&lt;span class="caps"&gt;HISTTIMEFORMAT&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;%F %T &amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and bash will store commented-out integer timestamps before each line in
&lt;code&gt;.bash_history&lt;/code&gt; like&amp;nbsp;so:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;#&lt;/span&gt;&lt;span class="m"&gt;1370950005&lt;/span&gt;
&lt;span class="go"&gt;less .bashrc&lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt;&lt;span class="m"&gt;1370950017&lt;/span&gt;
&lt;span class="go"&gt;history &lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt;&lt;span class="m"&gt;1370950279&lt;/span&gt;
&lt;span class="go"&gt;tail -30 .bash_history &lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt;&lt;span class="m"&gt;1370950293&lt;/span&gt;
&lt;span class="go"&gt;exit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the output of &lt;code&gt;history&lt;/code&gt; now uses the specified time&amp;nbsp;format:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt; 997  2013-06-11 07:26:45 less .bashrc
 998  2013-06-11 07:26:57 history 
 999  2013-06-11 07:31:19 tail -30 .bash_history 
1000  2013-06-11 07:31:33 exit
&lt;/pre&gt;&lt;/div&gt;</content><category term="bash"></category><category term="history"></category><category term="shell"></category><category term="timestamp"></category></entry><entry><title>Python script to check a list of URLs for return code, and final return code ifÂ redirected</title><link href="http://blog.jasonantman.com/2013/06/python-script-to-check-a-list-of-urls-for-return-code-and-final-return-code-if-redirected/" rel="alternate"></link><published>2013-06-10T06:00:00-04:00</published><updated>2013-06-10T06:00:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-06-10:/2013/06/python-script-to-check-a-list-of-urls-for-return-code-and-final-return-code-if-redirected/</id><summary type="html">&lt;p&gt;Every once in a while I need to add a bunch of redirects in Apache.
Here&amp;#8217;s a handy, dead simple Python script which takes a list of URLs on
&lt;span class="caps"&gt;STDIN&lt;/span&gt;, and for each one prints out either the response code, or, if the
response is a redirect, the response â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Every once in a while I need to add a bunch of redirects in Apache.
Here&amp;#8217;s a handy, dead simple Python script which takes a list of URLs on
&lt;span class="caps"&gt;STDIN&lt;/span&gt;, and for each one prints out either the response code, or, if the
response is a redirect, the response code of what is redirected to.
Pretty useful when you&amp;#8217;ve just added a bunch of redirects and want to
make sure none of them&amp;nbsp;404.&lt;/p&gt;
&lt;p&gt;The latest source of this script lives at
&lt;a href="https://github.com/jantman/misc-scripts/blob/master/check_url_list.py"&gt;https://github.com/jantman/misc-scripts/blob/master/check_url_list.py&lt;/a&gt;.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;Script to check a list of URLs (passed on stdin) for response code, and for response code of the final path in a series of redirects.&lt;/span&gt;
&lt;span class="sd"&gt;Outputs (to stdout) a list of count of a given &lt;span class="caps"&gt;URL&lt;/span&gt;, response code, and if redirected, the final &lt;span class="caps"&gt;URL&lt;/span&gt; and its response code&lt;/span&gt;

&lt;span class="sd"&gt;Optionally, with verbose flag, report on all &lt;span class="caps"&gt;URL&lt;/span&gt; checks on &lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;

&lt;span class="sd"&gt;Copyright 2013 Jason Antman  all rights reserved&lt;/span&gt;
&lt;span class="sd"&gt;This script is distributed under the terms of the GPLv3, as per the&lt;/span&gt;
&lt;span class="sd"&gt;&lt;span class="caps"&gt;LICENSE&lt;/span&gt; file in this repository.&lt;/span&gt;

&lt;span class="sd"&gt;The canonical version of this script can be found at:&lt;/span&gt;

&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;urllib2&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_url_nofollow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getcode&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HTTPError&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;urls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;readlines&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;+ checking &lt;span class="caps"&gt;URL&lt;/span&gt;: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;get_url_nofollow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;++ &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="http"></category><category term="python"></category><category term="redirect"></category><category term="urllib"></category></entry><entry><title>Modern (0.10.x+) NodeJS RPMs on CentOS/REHL 5 andÂ 6</title><link href="http://blog.jasonantman.com/2013/06/modern-0-10-x-nodejs-rpms-on-centosrehl-5-and-6/" rel="alternate"></link><published>2013-06-06T20:47:00-04:00</published><updated>2013-06-06T20:47:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-06-06:/2013/06/modern-0-10-x-nodejs-rpms-on-centosrehl-5-and-6/</id><summary type="html">&lt;p&gt;I posted back in January about &lt;a href="/2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/"&gt;&lt;span class="caps"&gt;RPM&lt;/span&gt; Spec Files for nodejs 0.9.5 and v8
on CentOS
6&lt;/a&gt;. In
that post I also said that I was unable to get recent NodeJS to build on
CentOS 5 because of a long chain of dependencies including node-gyp, v8,
http-parser, glibc â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I posted back in January about &lt;a href="/2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/"&gt;&lt;span class="caps"&gt;RPM&lt;/span&gt; Spec Files for nodejs 0.9.5 and v8
on CentOS
6&lt;/a&gt;. In
that post I also said that I was unable to get recent NodeJS to build on
CentOS 5 because of a long chain of dependencies including node-gyp, v8,
http-parser, glibc, etc. I said I couldn&amp;#8217;t get it to build. Well, I have
good news for both distro&amp;nbsp;versions.&lt;/p&gt;
&lt;p&gt;On the CentOS/&lt;span class="caps"&gt;RHEL&lt;/span&gt; 6 side, thanks to a lot of work by &lt;span class="caps"&gt;T. C.
&lt;/span&gt;Hollingsworth and others, NodeJS 0.10.5 is currently in the official
&lt;a href="http://fedoraproject.org/wiki/EPEL"&gt;&lt;span class="caps"&gt;EPEL&lt;/span&gt;&lt;/a&gt; repositories. They seem to be
keeping the packages pretty current, but if you need newer, you can
always grab the SRPMs from &lt;span class="caps"&gt;EPEL&lt;/span&gt; and build the newer versions. This is
great, because it means I no longer need to maintain the spec files and
do my own builds. I don&amp;#8217;t think I really did anything to help get this
package in &lt;span class="caps"&gt;EPEL&lt;/span&gt;, other than ping a few people and comment on a few&amp;nbsp;tickets.&lt;/p&gt;
&lt;p&gt;For CentOS/&lt;span class="caps"&gt;RHEL&lt;/span&gt; 5, I finally have packages, but they&amp;#8217;re not exactly
pretty. The dependency solving issues still stand; they&amp;#8217;re rooted at the
dependency of node-gyp which requires the v8 C++ JavaScript library, and
is required to compile shared object addons. The best solution that I
(and a few others) could find is simply not to build node-gyp, and not
to have support for addons or package any addons; we just have the
binaries that NodeJS&amp;#8217;s Makefile creates, and everything else is
interpreted. A &lt;a href="https://twitter.com/toxigenicpoem"&gt;coworker&lt;/a&gt; found
&lt;a href="https://github.com/kazuhisya/nodejs-rpm"&gt;https://github.com/kazuhisya/nodejs-rpm&lt;/a&gt;
which contains a configure patch and specfile for a dead-simple CentOS
5/6 &lt;span class="caps"&gt;RPM&lt;/span&gt; of NodeJS 0.10.9, which essentially just uses &lt;span class="caps"&gt;EPEL&lt;/span&gt;&amp;#8217;s python26
packages to power the NodeJS build process, configures and uses the
Makefile&amp;#8217;s &lt;code&gt;make binary&lt;/code&gt; command to spit out a NodeJS binary tarball,
and then packages that. That whole process way out of line from the
&lt;a href="http://fedoraproject.org/wiki/Packaging:Guidelines"&gt;Fedora Packaging
Guidelines&lt;/a&gt;, and
also only dumps out nodejs, nodejs-binary and nodejs-debuginfo packages,
so I also can&amp;#8217;t just substitute in a different package name in my puppet
manifests (which install nodejs, nodejs-devel and npm packages). So I
&lt;a href="https://github.com/jantman/nodejs-rpm-centos5"&gt;forked that repository&lt;/a&gt;
and made some changes to the specfile: I gave the package name a prefix
(&amp;#8220;cmgd_&amp;#8221;, since that&amp;#8217;s where I work these days) and some warnings in
the description, to make it abundantly clear that these packages are
very far from what you find in &lt;span class="caps"&gt;EPEL&lt;/span&gt; and other repositories, and broke
npm and the devel files out into their own subpackages. Hopefully this
spec file will be of use to someone else who also has the unfortunate
need of supporting recent NodeJS on CentOS 5. If there&amp;#8217;s enough
interest, I&amp;#8217;ll consider building the packages and putting them in a
repository&amp;nbsp;somewhere.&lt;/p&gt;
&lt;p&gt;You can see the NodeJS 0.10.9 on CentOS 5 spec file, a patch, and the
READMEs at
&lt;a href="https://github.com/jantman/nodejs-rpm-centos5"&gt;https://github.com/jantman/nodejs-rpm-centos5&lt;/a&gt;.
Patches and/or pull requests are greatly appreciated, especially from
anyone who wants to make the spec file more Fedora guidelines&amp;nbsp;compliant.&lt;/p&gt;</content><category term="build"></category><category term="centos"></category><category term="EPEL"></category><category term="node"></category><category term="nodejs"></category><category term="package"></category><category term="packaging"></category><category term="redhat"></category><category term="RHEL"></category><category term="rpm"></category><category term="specfile"></category></entry><entry><title>Script to easily rebuild aÂ SRPM</title><link href="http://blog.jasonantman.com/2013/05/script-to-easily-rebuild-a-srpm/" rel="alternate"></link><published>2013-05-28T10:26:00-04:00</published><updated>2013-05-28T10:26:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-05-28:/2013/05/script-to-easily-rebuild-a-srpm/</id><summary type="html">&lt;p&gt;Between &lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS 5 and 6 the default &lt;span class="caps"&gt;RPM&lt;/span&gt; compression format was
changed to xz. As such, trying to build a recent Fedora or Cent6 &lt;span class="caps"&gt;SRPM&lt;/span&gt; on
Cent5 will error out with a message like
&lt;code&gt;error: unpacking of archive failed on file foo;51a4c2a5: cpio: MD5 sum mismatch&lt;/code&gt;
because tar â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;Between &lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS 5 and 6 the default &lt;span class="caps"&gt;RPM&lt;/span&gt; compression format was
changed to xz. As such, trying to build a recent Fedora or Cent6 &lt;span class="caps"&gt;SRPM&lt;/span&gt; on
Cent5 will error out with a message like
&lt;code&gt;error: unpacking of archive failed on file foo;51a4c2a5: cpio: MD5 sum mismatch&lt;/code&gt;
because tar on CentOS 5 doesn&amp;#8217;t support&amp;nbsp;xz.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s a quick and dirty little script to use &lt;code&gt;rpm2cpio&lt;/code&gt; to rebuild a
&lt;span class="caps"&gt;SRPM&lt;/span&gt; using the host&amp;#8217;s native &lt;span class="caps"&gt;RPM&lt;/span&gt; compression. The latest version will
live at
&lt;a href="https://github.com/jantman/misc-scripts/blob/master/rebuild_srpm.sh"&gt;https://github.com/jantman/misc-scripts/blob/master/rebuild_srpm.sh&lt;/a&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="ch"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;span class="c1"&gt;# Script to rebuild a &lt;span class="caps"&gt;SRPM&lt;/span&gt; 1:1, useful when you want to build a &lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS 6&lt;/span&gt;
&lt;span class="c1"&gt;# &lt;span class="caps"&gt;SRPM&lt;/span&gt; on a &lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS 5 system that doesn&amp;#39;t support newer compression (cpio: &lt;span class="caps"&gt;MD5&lt;/span&gt; sum mismatch)&lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;
&lt;span class="c1"&gt;# by Jason Antman &lt;/span&gt;
&lt;span class="c1"&gt;# The latest version of this script will always live at:&lt;/span&gt;
&lt;span class="c1"&gt;# &lt;/span&gt;
&lt;span class="c1"&gt;#&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -z &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;-h&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--help&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;USAGE&lt;/span&gt;: rebuild_srpm.sh  &amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -z &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$2&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nv"&gt;&lt;span class="caps"&gt;OUTDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
    &lt;span class="nv"&gt;&lt;span class="caps"&gt;OUTDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$2&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; ! -e &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;ERROR&lt;/span&gt;: &lt;span class="caps"&gt;SRPM&lt;/span&gt; file not found: &lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; ! which rpmbuild &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&amp;gt; /dev/null
&lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rpmbuild could not be found. please install. (sudo yum install rpm-build)&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; ! which rpm2cpio &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&amp;gt; /dev/null
&lt;span class="k"&gt;then&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rpm2cpio could not be found. please install. (sudo yum install rpm)&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit&lt;/span&gt; &lt;span class="m"&gt;1&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="nv"&gt;&lt;span class="caps"&gt;SRPM&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;dirname &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;basename &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="nv"&gt;$1&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="nv"&gt;&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;mktemp -d&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="nv"&gt;&lt;span class="caps"&gt;STARTPWD&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Rebuilding &lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;SRPM&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;...&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;# copy srpm into tempdir&lt;/span&gt;
cp &lt;span class="nv"&gt;$&lt;span class="caps"&gt;SRPM&lt;/span&gt;&lt;/span&gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;

&lt;span class="nb"&gt;pushd&lt;/span&gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt; &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&amp;gt;/dev/null

&lt;span class="c1"&gt;# setup local build dir structure&lt;/span&gt;
mkdir -p rpm rpm/&lt;span class="caps"&gt;BUILD&lt;/span&gt; rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt; rpm/&lt;span class="caps"&gt;SOURCES&lt;/span&gt; rpm/&lt;span class="caps"&gt;SPECS&lt;/span&gt; rpm/&lt;span class="caps"&gt;SRPMS&lt;/span&gt; rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt;/athlon rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt;/i&lt;span class="se"&gt;\[&lt;/span&gt;&lt;span class="m"&gt;3456&lt;/span&gt;&lt;span class="se"&gt;\]&lt;/span&gt;&lt;span class="m"&gt;86&lt;/span&gt; rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt;/i386 rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt;/noarch rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt;/x86_64

&lt;span class="c1"&gt;# setup rpmmacros file&lt;/span&gt;
cat /dev/null &amp;gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;/.rpmmacros
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;%_topdir        &lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;/rpm&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; ~/.rpmmacros

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Extracting &lt;span class="caps"&gt;SRPM&lt;/span&gt;...&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;pushd&lt;/span&gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;/rpm/&lt;span class="caps"&gt;SOURCES&lt;/span&gt;/ &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&amp;gt;/dev/null
rpm2cpio &lt;span class="nv"&gt;$&lt;span class="caps"&gt;SRPM&lt;/span&gt;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; cpio -idmv &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&amp;gt;/dev/null
&lt;span class="nb"&gt;popd&lt;/span&gt; &lt;span class="p"&gt;&amp;amp;&lt;/span&gt;&amp;gt;/dev/null

&lt;span class="c1"&gt;# build the &lt;span class="caps"&gt;SRPM&lt;/span&gt; from the spec and sources&lt;/span&gt;
&lt;span class="c1"&gt;# we&amp;#39;re just building a &lt;span class="caps"&gt;SRPM&lt;/span&gt; so we can ignore dependencies&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Rebuilding &lt;span class="caps"&gt;SRPM&lt;/span&gt;...&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;NEW_SRPM&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;rpmbuild -bs --nodeps --macros&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;/.rpmmacros &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;/rpm/&lt;span class="caps"&gt;SOURCES&lt;/span&gt;/*.spec &lt;span class="p"&gt;|&lt;/span&gt; grep &lt;span class="s2"&gt;&amp;quot;^Wrote: &amp;quot;&lt;/span&gt; &lt;span class="p"&gt;|&lt;/span&gt; awk &lt;span class="s1"&gt;&amp;#39;{print $2}&amp;#39;&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Copying to &lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;OUTDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
cp &lt;span class="nv"&gt;$NEW_SRPM&lt;/span&gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;OUTDIR&lt;/span&gt;&lt;/span&gt;/

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Wrote file to &lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;OUTDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="s2"&gt;/`basename &lt;/span&gt;&lt;span class="nv"&gt;$NEW_SRPM&lt;/span&gt;&lt;span class="s2"&gt;`&amp;quot;&lt;/span&gt;

&lt;span class="c1"&gt;# cleanup&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;STARTPWD&lt;/span&gt;&lt;/span&gt;
rm -Rf &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="lzma"></category><category term="packaging"></category><category term="rpm"></category><category term="rpm2cpio"></category><category term="rpmbuild"></category><category term="srpm"></category><category term="xz"></category></entry><entry><title>Git CheatÂ Sheet</title><link href="http://blog.jasonantman.com/2013/05/git-cheat-sheet/" rel="alternate"></link><published>2013-05-14T05:00:00-04:00</published><updated>2013-05-14T05:00:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-05-14:/2013/05/git-cheat-sheet/</id><summary type="html">&lt;p&gt;I use &lt;a href="http://git-scm.com/"&gt;git&lt;/a&gt; quite a bit these days, both with an
internal server at work and with a bunch of my projects and random code
that now live on &lt;a href="https://github.com/jantman/"&gt;my github account&lt;/a&gt;. The
transition from &lt;span class="caps"&gt;SVN&lt;/span&gt; hasn&amp;#8217;t always been easy. Here&amp;#8217;s a quick cheat sheet
of some of â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I use &lt;a href="http://git-scm.com/"&gt;git&lt;/a&gt; quite a bit these days, both with an
internal server at work and with a bunch of my projects and random code
that now live on &lt;a href="https://github.com/jantman/"&gt;my github account&lt;/a&gt;. The
transition from &lt;span class="caps"&gt;SVN&lt;/span&gt; hasn&amp;#8217;t always been easy. Here&amp;#8217;s a quick cheat sheet
of some of the things that I usually&amp;nbsp;forget.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Show diff of the last&amp;nbsp;commit:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git diff &lt;span class="caps"&gt;HEAD&lt;/span&gt;^..&lt;span class="caps"&gt;HEAD&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Roll back to version xyz of a specific file &lt;em&gt;(where xyz is a &lt;span class="caps"&gt;SHA1&lt;/span&gt;
    commit ref)&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git checkout xyz path/to/file
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Undo any &lt;em&gt;unstaged&lt;/em&gt; changes to your&amp;nbsp;branch:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git checkout -f
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Undo any staged and working directory&amp;nbsp;changes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git reset --hard
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update submodules after cloning a&amp;nbsp;repository:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git submodule update --init
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rebase on current master to pull in new&amp;nbsp;changes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git rebase master
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rebase on current master, but for files that changed, take our
    version &lt;em&gt;(for some reason, a plain rebase seems to sometimes show
    conflicts on files that haven&amp;#8217;t changed in ages on master)&lt;/em&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git rebase -s recursive -Xtheirs master
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete a local&amp;nbsp;branch:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git branch -d BranchName
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete a remote branch from&amp;nbsp;origin:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git push origin --delete BranchName
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Roll back your branch to the same state as the branch in&amp;nbsp;origin:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git reset --hard origin/BranchName
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Revert a specific&amp;nbsp;commit:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git revert COMMIT_HASH
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Track an upstream branch (i.e. in a project you&amp;nbsp;forked):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git remote add --track master upstream https://github.com/user/project.git
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pull in upstream&amp;nbsp;changes:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git checkout master &amp;amp;&amp;amp; git fetch upstream &amp;amp;&amp;amp; git merge upstream/master
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Merge &amp;#8220;stuff&amp;#8221; from someone else&amp;#8217;s fork into&amp;nbsp;yours:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git remote add other-guys-repo URL_TO_REPO
git fetch other-guys-repo
git checkout my_new_branch
git merge other-guys-repo/master
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prune local branches that have been deleted in the remote&amp;nbsp;(origin):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git remote prune origin
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;</content><category term="git"></category></entry><entry><title>Search for a small-scale but automated RPM buildÂ system</title><link href="http://blog.jasonantman.com/2013/05/search-for-a-small-scale-but-automated-rpm-build-system/" rel="alternate"></link><published>2013-05-13T05:00:00-04:00</published><updated>2013-05-13T05:00:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-05-13:/2013/05/search-for-a-small-scale-but-automated-rpm-build-system/</id><summary type="html">&lt;p&gt;&lt;strong&gt;This post is part of a series of older draft posts from a few months
ago that I&amp;#8217;m just getting around to publishing. Unfortunately, I have
yet to find a build system that meets my requirements (see the last&amp;nbsp;paragraph).&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At work, we have a handful - currently a really â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;This post is part of a series of older draft posts from a few months
ago that I&amp;#8217;m just getting around to publishing. Unfortunately, I have
yet to find a build system that meets my requirements (see the last&amp;nbsp;paragraph).&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At work, we have a handful - currently a really small number - of &lt;span class="caps"&gt;RPM&lt;/span&gt;
packages that we need to build and deploy internally for our CentOS
server infrastructure. A number of them are just pulled down from
specific third-party repositories and rebuilt to have the vendor set as
us, and some are internally patched or developed software. We run
websites, and on the product side, we&amp;#8217;re a
Python/&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; shop (in fact, probably
one of the largest Django apps out there). We don&amp;#8217;t deploy our Django
apps via &lt;span class="caps"&gt;RPM&lt;/span&gt;, so building and distributing RPMs is definitely not one of
our core competencies. In fact, we really only want to do it when we&amp;#8217;re
testing/deploying a new distro, or when an upstream package is&amp;nbsp;updated.&lt;/p&gt;
&lt;p&gt;Last week I pulled a ticket to deploy &lt;a href="http://nodejs.org/"&gt;node.js&lt;/a&gt; to
one of our build hosts, and we&amp;#8217;ve got a few things in the pipeline that
also rely on it. I found the
&lt;a href="https://github.com/puppetlabs/puppetlabs-nodejs"&gt;puppetlabs-nodejs&lt;/a&gt;
module on Github that&amp;#8217;s supposed to install it on &lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS, but it
pulls packages from
&lt;a href="http://patches.fedorapeople.org/oldnode/stable/"&gt;http://patches.fedorapeople.org/oldnode/stable/&lt;/a&gt;,
and the newest version of nodejs there is 0.6.18, which is quite old. I
can&amp;#8217;t find any actively maintained sources of newer nodejs packages for
&lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS (yeah, I know, that&amp;#8217;s one down side to the
distributions&amp;#8230;). However, I did find that nodejs 0.9.5 is being &lt;a href="http://koji.fedoraproject.org/koji/packageinfo?packageID=15154"&gt;built
for Fedora 18/19 in the Fedora build
system&lt;/a&gt;,
is already in the Fedora 18 Testing and Fedora Rawhide repos, but is
failing its &lt;span class="caps"&gt;EL6&lt;/span&gt; builds in their system. The decision I&amp;#8217;ve come to is to
use the puppetlabs-nodejs module to install it, but try and rebuild the
Fedora 18 RPMs under CentOS 5 and&amp;nbsp;6.&lt;/p&gt;
&lt;p&gt;So that&amp;#8217;s the background. Now, my current task: to search for an &lt;span class="caps"&gt;RPM&lt;/span&gt;
build system for my current job. My core requirements, in no specific
order,&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Be relatively easy and quick to use for people who have a specfile
    or &lt;span class="caps"&gt;SRPM&lt;/span&gt; and want to be able to &amp;#8220;ensure =&gt; present&amp;#8221; the finished &lt;span class="caps"&gt;RPM&lt;/span&gt;
    on a system. i.e., require as little per-package configuration as&amp;nbsp;possible.&lt;/li&gt;
&lt;li&gt;Be able to handle rebuilding &amp;#8220;all&amp;#8221; of our RPMs when we roll out a
    new distro version. Doesn&amp;#8217;t necessarily need to be automatic, but
    should be relatively&amp;nbsp;simple.&lt;/li&gt;
&lt;li&gt;Ideally, not need to be running constantly - i.e. something that
    will cope well with build hosts being VMs that are shut down when
    they&amp;#8217;re not&amp;nbsp;needed.&lt;/li&gt;
&lt;li&gt;Handle automatically putting successfully built packages into a
    repository, ideally with some sort of (manual) promotion process
    from staging to&amp;nbsp;stable.&lt;/li&gt;
&lt;li&gt;Have minimal external (infrastructure) dependencies that we can&amp;#8217;t
    satisfy with existing&amp;nbsp;systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, the first step was to research existing &lt;span class="caps"&gt;RPM&lt;/span&gt; build systems and how
others do this. Here&amp;#8217;s a list of what I could find online, though most
of these are from distributions and software vendors/projects, not
end-user companies that are only building for internal&amp;nbsp;use.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://fedorahosted.org/koji/wiki"&gt;Koji&lt;/a&gt; is the build system used
    by &lt;a href="http://fedoraproject.org/wiki/Koji"&gt;Fedora&lt;/a&gt; and RedHat. It&amp;#8217;s
    about as full-featured as any can be, and I&amp;#8217;m familiar with it from
    my time at &lt;a href="http://koji.rutgers.edu/koji/"&gt;Rutgers University&lt;/a&gt;, as
    it&amp;#8217;s used to maintain their CentOS/&lt;span class="caps"&gt;RHEL&lt;/span&gt; packages. It&amp;#8217;s based largely
    on Mock. However, &lt;a href="http://fedoraproject.org/wiki/Koji/ServerHowTo"&gt;setting up the build
    server&lt;/a&gt; is no
    trivial task; there are few installations outside of Fedora/RedHat,
    and it relies on either Kerberos or an &lt;span class="caps"&gt;SSL&lt;/span&gt; &lt;span class="caps"&gt;CA&lt;/span&gt; infrastructure to
    authenticate machines and clients. So, it&amp;#8217;s designed for too large a
    scale and too much infrastructure for&amp;nbsp;me.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;PLD&lt;/span&gt; Linux has a &lt;a href="https://www.pld-linux.org/developingpld/builderscript"&gt;builder
    script&lt;/a&gt; that
    seems to automate &lt;code&gt;rpmbuild&lt;/code&gt; as well as fetching sources and
    resolving/building dependencies. I haven&amp;#8217;t looked at the script yet,
    but apparently it&amp;#8217;s in &lt;span class="caps"&gt;PLD&lt;/span&gt;&amp;#8217;s &amp;#8220;rpm-build-tools&amp;#8221;&amp;nbsp;package.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;PLD&lt;/span&gt; Linux also has a &lt;span class="caps"&gt;CVS&lt;/span&gt; repository for something called
    &lt;a href="http://cvs.pld-linux.org/cgi-bin/cvsweb/pld-builder.new"&gt;pld-builder.new&lt;/a&gt;.
    The
    &lt;a href="http://cvs.pld-linux.org/cgi-bin/cvsweb/pld-builder.new/doc/README?rev=1.5"&gt;&lt;span class="caps"&gt;README&lt;/span&gt;&lt;/a&gt;
    and
    &lt;a href="http://cvs.pld-linux.org/cgi-bin/cvsweb/pld-builder.new/doc/ARCHITECTURE?rev=1.6"&gt;&lt;span class="caps"&gt;ARCHITECTURE&lt;/span&gt;&lt;/a&gt;
    files make it sound like a relatively simple mainly-Python system
    that builds &lt;span class="caps"&gt;SRPMS&lt;/span&gt; and binary packages when requested, and most
    importantly, seems like a simple system that uses little more than
    shared filesystem access for communication and&amp;nbsp;coordination.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;ALT&lt;/span&gt; Linux has &lt;a href="http://en.altlinux.org/Sisyphus"&gt;Sisyphus&lt;/a&gt;, which
    combines repository management and web interface tools, package
    building and testing tools, and&amp;nbsp;more.&lt;/li&gt;
&lt;li&gt;The Dries &lt;span class="caps"&gt;RPM&lt;/span&gt; repository uses (or at least used&amp;#8230; my reference is
    quite old) &lt;a href="http://dries.ulyssis.org/rpm/pydar2/index.html"&gt;pydar2&lt;/a&gt;,
    &amp;#8220;a distributed client/server program which allows you to build
    multiple spec files on multiple distribution/architecture
    combinations automatically.&amp;#8221; That sounds like it could be what I
    need, but the last update says that it isn&amp;#8217;t finished yet, and that
    was in &lt;strong&gt;2005&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Mandriva Linux has pretty extensive information on their build
    system &lt;a href="http://wiki.mandriva.com/en/Category:Build_System"&gt;on their
    wiki&lt;/a&gt; and a
    &lt;a href="http://wiki.mandriva.com/en/Development/Packaging/BuildSystem/Theory"&gt;build system theory
    page&lt;/a&gt;,
    but it seems to be largely a hodgepodge of shell scripts and
    cronjobs, and is likely not a candidate for use by anyone other than
    its&amp;nbsp;designers.&lt;/li&gt;
&lt;li&gt;Argeo provides the &lt;a href="https://www.argeo.org/wiki/SLC"&gt;&lt;span class="caps"&gt;SLC&lt;/span&gt; framework&lt;/a&gt;
    which has a &amp;#8220;&lt;span class="caps"&gt;RPM&lt;/span&gt; Factory&amp;#8221; component, but I can&amp;#8217;t seem to find much
    more than a wiki page, and can&amp;#8217;t tell if it&amp;#8217;s a build automation
    system or just handles mocking packages and putting them in a repo
    on a single&amp;nbsp;host.&lt;/li&gt;
&lt;li&gt;Dag Wieers&amp;#8217; repositories use (or used) a set of python scripts
    called &lt;a href="http://dag.wieers.com/home-made/dar/"&gt;&lt;span class="caps"&gt;DAR&lt;/span&gt;, &amp;#8220;Dynamic Apt Repository
    builder&amp;#8221;&lt;/a&gt;. They&amp;#8217;re on
    &lt;a href="https://github.com/dagwieers/dar"&gt;github&lt;/a&gt; but are listed as &amp;#8220;old&amp;#8221;
    and haven&amp;#8217;t been updated in at least 2 years. The features sound
    quite interesting, and though it&amp;#8217;s based on the Apt repo format, it
    might provide some good ideas for implementing a similar&amp;nbsp;system.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Update four months later:&lt;/strong&gt; I&amp;#8217;ve yet to find a build system that meets
my requirements above. For the moment I&amp;#8217;m only managing \~20 packages,
so my &amp;#8220;build system&amp;#8221; is a single shell script that reads in some
environment variables and runs through using
&lt;a href="http://fedoraproject.org/wiki/Projects/Mock"&gt;mock&lt;/a&gt; to build them in the
correct order (including pushing the finished RPMs back into the local
repository that mock reads from) and then pushing the finished packages
to our internal repository. Maybe when I have some spare time, I&amp;#8217;ll
consider a project to either make a slightly better (but simple) &lt;span class="caps"&gt;RPM&lt;/span&gt;
build system based on Python, or get our
&lt;a href="http://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; install to handle this for&amp;nbsp;me.&lt;/p&gt;</content><category term="build"></category><category term="linux"></category><category term="nodejs"></category><category term="package"></category><category term="packaging"></category><category term="repository"></category><category term="rpm"></category><category term="rpmbuild"></category><category term="software"></category><category term="sysadmin"></category><category term="yum"></category></entry><entry><title>Environment Variable Substitution in Apache httpdÂ Configs</title><link href="http://blog.jasonantman.com/2013/05/environment-variable-substitution-in-apache-httpd-configs/" rel="alternate"></link><published>2013-05-11T12:01:00-04:00</published><updated>2013-05-11T12:01:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-05-11:/2013/05/environment-variable-substitution-in-apache-httpd-configs/</id><summary type="html">&lt;p&gt;I&amp;#8217;ve been configuring Apache httpd for over a decade, from a single
personal web server to web farms running thousands of vhosts. In most of
the &amp;#8220;real&amp;#8221; environments I&amp;#8217;ve worked in, we&amp;#8217;ve had some variation of
production, stage/test/&lt;span class="caps"&gt;QA&lt;/span&gt; and development hosts; and usually some method â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I&amp;#8217;ve been configuring Apache httpd for over a decade, from a single
personal web server to web farms running thousands of vhosts. In most of
the &amp;#8220;real&amp;#8221; environments I&amp;#8217;ve worked in, we&amp;#8217;ve had some variation of
production, stage/test/&lt;span class="caps"&gt;QA&lt;/span&gt; and development hosts; and usually some method
of managing configurations between them, whether it&amp;#8217;s source control or
generating them from template. And in all of these environments, there
has invariably been drift between the configurations in the various
environments, whether it&amp;#8217;s because of poor tools to maintain a unified
configuration or many of those emergency redirect requests that make it
into production but are never backported. This is made all the worse
because everywhere I&amp;#8217;ve worked, the real difference between what
production and other environments &lt;em&gt;should&lt;/em&gt; be is really just a string
replacement in Apache configurations - &lt;code&gt;/prod/&lt;/code&gt; to &lt;code&gt;/test/&lt;/code&gt; or
&lt;code&gt;www.example.com&lt;/code&gt; to &lt;code&gt;www.dev.example.com&lt;/code&gt; or something along those&amp;nbsp;lines.&lt;/p&gt;
&lt;p&gt;Well a few days ago I was having a discussion with some co-workers that
dovetailed into this topic, and when I started some research, I found
(&lt;em&gt;finally after using httpd for years&lt;/em&gt;) that the &lt;a href="http://httpd.apache.org/docs/2.2/configuring.html#syntax"&gt;Apache httpd 2.2
configuration file syntax
documentation&lt;/a&gt;
states that httpd supports environment variable interpolation anywhere
in the config files (and &lt;a href="http://httpd.apache.org/docs/2.4/configuring.html#syntax"&gt;httpd
2.4&lt;/a&gt; supports
it with Defines as&amp;nbsp;well).&lt;/p&gt;
&lt;p&gt;Yup, that&amp;#8217;s right. All those different Apache configs I&amp;#8217;ve worked with
for years that define separate vhosts, document roots, rewrite targets,
ServerAliases, etc. for &lt;code&gt;www.example.com&lt;/code&gt; and &lt;code&gt;www.qa.example.com&lt;/code&gt; and
&lt;code&gt;www.dev.example.com&lt;/code&gt; really only had to be
&lt;code&gt;www.${ENV_URL_PART}example.com&lt;/code&gt;, and set &lt;code&gt;ENV_URL_PART&lt;/code&gt; in the init
script or sysconfig file. (Of course this all assumes that you have your
different environments served by different httpd instances, which you
do, of&amp;nbsp;course&amp;#8230;)&lt;/p&gt;
&lt;p&gt;For me, this is a very big deal. It means that finally, instead of
maintaining separate sets of configs for different environments which
are (theoretically, except for those emergencies) kept identical by
hand, or updating templates and then re-generating each environment&amp;#8217;s
configs, we can finally follow the same
commit/merge/promotion-between-environments workflow that we use for
other production code and Puppet configuration. It also means that those
pesky little rewrites and other minor tweaks will make it all the way
back to development&amp;nbsp;environments.&lt;/p&gt;
&lt;p&gt;So, here&amp;#8217;s a little example of how this would work in reality. Let&amp;#8217;s
assume that we have 3 main environments, &lt;code&gt;prod&lt;/code&gt;, &lt;code&gt;qa&lt;/code&gt; and &lt;code&gt;dev&lt;/code&gt; (though
this should work for N environments) and that domains are prefixed with
&amp;#8220;qa.&amp;#8221; or &amp;#8220;dev.&amp;#8221; for the respective internal environments. We set
environment variables before httpd is started, on a per-host basis,
depending on what environment that host is in. On RedHat based systems,
we&amp;#8217;d add the variables to &lt;code&gt;/etc/sysconfig/httpd&lt;/code&gt; for&amp;nbsp;production:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;HTTPD_ENV_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prod&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;HTTPD_ENV_URL_PART&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or for&amp;nbsp;&lt;span class="caps"&gt;QA&lt;/span&gt;:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;HTTPD_ENV_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;qa&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;export&lt;/span&gt; &lt;span class="nv"&gt;HTTPD_ENV_URL_PART&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;qa.&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Those variables will now be available to httpd within the configurations
(and also to any applications or scripts that have access to the web
server&amp;#8217;s environment&amp;nbsp;variables).&lt;/p&gt;
&lt;p&gt;Now let&amp;#8217;s look at an example vhost configuration file that uses the
environment&amp;nbsp;variables:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nb"&gt;ServerName&lt;/span&gt; example.com
&lt;span class="nb"&gt;ServerAlias&lt;/span&gt; www.example.com
&lt;span class="c"&gt;# Aliases including proper environment name&lt;/span&gt;
&lt;span class="nb"&gt;ServerAlias&lt;/span&gt; www.${HTTPD_ENV_NAME}.example.com ${HTTPD_ENV_NAME}.example.com

&lt;span class="nb"&gt;ErrorLog&lt;/span&gt; &lt;span class="sx"&gt;/var/log/httpd/example.com-error_log&lt;/span&gt;
&lt;span class="nb"&gt;CustomLog&lt;/span&gt; &lt;span class="sx"&gt;/var/log/httpd/example.com-access_log&lt;/span&gt; combined

&lt;span class="nb"&gt;DocumentRoot&lt;/span&gt; &lt;span class="sx"&gt;/sites/example.com/&lt;/span&gt;${HTTPD_ENV_NAME}/

&lt;span class="c"&gt;# Environment-specific configuration, if we absolutely need it:&lt;/span&gt;
&lt;span class="nb"&gt;Include&lt;/span&gt; &lt;span class="sx"&gt;/etc/httpd/sites/&lt;/span&gt;${HTTPD_ENV_NAME}/env.conf


&lt;span class="nb"&gt;RewriteEngine&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt;
&lt;span class="nb"&gt;RewriteRule&lt;/span&gt; &lt;span class="sx"&gt;/foobar/.&lt;/span&gt;* http://www.${HTTPD_ENV_URL_PART}example.com/baz/ [R=302,L]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Every instance of &lt;code&gt;${HTTPD_ENV_NAME}&lt;/code&gt; will be replaced with the value
set in the sysconfig file, and likewise with every instance of
&lt;code&gt;${HTTPD_ENV_URL_PART}&lt;/code&gt;. This way, we can have one set of configurations
and use our normal source control branch/promotion process to both test
and promote changes through the environments along with application
code, and ensure that any straight-to-production emergency changes
(everyone has customer-ordered rewrites like that, right?) make it back
to development and&amp;nbsp;qa.&lt;/p&gt;
&lt;p&gt;One caveat is that, if the environment variable is not defined, the
&lt;code&gt;${VAR_NAME}&lt;/code&gt; will be left as a literal string in the configuration
file. There doesn&amp;#8217;t seem to be any way to protect against this in httpd
2.2, other than making sure the variables are set before the server
starts (and maybe setting logical default values, like an empty string,
in your init script which should be overridden by the sysconfig&amp;nbsp;file).&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;re running httpd 2.4+, you can turn on
&lt;a href="http://httpd.apache.org/docs/2.4/mod/mod_info.html"&gt;mod_info&lt;/a&gt; and
browse to &lt;code&gt;http://servername/server-info?config&lt;/code&gt; to dump the current
configuration, which will show the variable&amp;nbsp;substitution.&lt;/p&gt;</content><category term="apache"></category><category term="environment"></category><category term="httpd"></category><category term="variable"></category></entry><entry><title>RPM Spec Files for nodejs 0.9.5 and v8 on CentOSÂ 6</title><link href="http://blog.jasonantman.com/2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/" rel="alternate"></link><published>2013-01-31T14:13:00-05:00</published><updated>2013-01-31T14:13:00-05:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-01-31:/2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/</id><summary type="html">&lt;p&gt;The latest version of nodejs that I could find as an &lt;span class="caps"&gt;RPM&lt;/span&gt; for CentOS was
0.6.16, from
&lt;a href="http://patches.fedorapeople.org/oldnode/stable/"&gt;http://patches.fedorapeople.org/oldnode/stable/&lt;/a&gt;.
That&amp;#8217;s the one that puppetlabs currently uses in their
&lt;a href="https://github.com/puppetlabs/puppetlabs-nodejs"&gt;puppetlabs-nodejs&lt;/a&gt;
module. There is, however, a nodejs 0.9.5 &lt;span class="caps"&gt;RPM&lt;/span&gt; in the Fedora Rawhide â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;The latest version of nodejs that I could find as an &lt;span class="caps"&gt;RPM&lt;/span&gt; for CentOS was
0.6.16, from
&lt;a href="http://patches.fedorapeople.org/oldnode/stable/"&gt;http://patches.fedorapeople.org/oldnode/stable/&lt;/a&gt;.
That&amp;#8217;s the one that puppetlabs currently uses in their
&lt;a href="https://github.com/puppetlabs/puppetlabs-nodejs"&gt;puppetlabs-nodejs&lt;/a&gt;
module. There is, however, a nodejs 0.9.5 &lt;span class="caps"&gt;RPM&lt;/span&gt; in the Fedora Rawhide (19)
repository. Below are some patches to that specfile, and the specfile
for its v8 dependency, to get them to build on CentOS 6. You can also
find the full specfiles on my &lt;a href="https://github.com/jantman/specfiles"&gt;github specfile
repository&lt;/a&gt;. I had originally
wanted to get them built on CentOS 5 as well, but after following the
dependency tree from nodejs to http-parser to gyp, and then finding
issues in the gyp source that are incompatible with CentOS 5&amp;#8217;s python
2.4, I gave up on that&amp;nbsp;target.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nodejs.spec&lt;/strong&gt;, diff from Fedora Rawhide nodejs-0.9.5-9.fc18.src.rpm,
buildID=377755 (&lt;a href="https://raw.github.com/jantman/specfiles/master/nodejs.spec"&gt;full
specfile&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gh"&gt;diff --git a/nodejs.spec b/nodejs.spec&lt;/span&gt;
&lt;span class="gh"&gt;index 050ed86..86c0f4b 100644&lt;/span&gt;
&lt;span class="gd"&gt;--- a/nodejs.spec&lt;/span&gt;
&lt;span class="gi"&gt;+++ b/nodejs.spec&lt;/span&gt;
&lt;span class="gu"&gt;@@ -1,6 +1,6 @@&lt;/span&gt;
 Name: nodejs
 Version: 0.9.5
&lt;span class="gd"&gt;-Release: 9%{?dist}&lt;/span&gt;
&lt;span class="gi"&gt;+Release: 10%{?dist}&lt;/span&gt;
 Summary: JavaScript runtime
 License: &lt;span class="caps"&gt;MIT&lt;/span&gt; and &lt;span class="caps"&gt;ASL&lt;/span&gt; 2.0 and &lt;span class="caps"&gt;ISC&lt;/span&gt; and &lt;span class="caps"&gt;BSD&lt;/span&gt;
 Group: Development/Languages
&lt;span class="gu"&gt;@@ -25,7 +25,7 @@ Source6: nodejs-fixdep&lt;/span&gt;
 BuildRequires: v8-devel &amp;gt;= %{v8_ge}
 BuildRequires: http-parser-devel &amp;gt;= 2.0
 BuildRequires: libuv-devel
&lt;span class="gd"&gt;-BuildRequires: c-ares-devel&lt;/span&gt;
&lt;span class="gi"&gt;+BuildRequires: c-ares-devel &amp;gt;= 1.9.0&lt;/span&gt;
 BuildRequires: zlib-devel
 # Node.js requires some features from openssl 1.0.1 for &lt;span class="caps"&gt;SPDY&lt;/span&gt; support
 BuildRequires: openssl-devel &amp;gt;= 1:1.0.1
&lt;span class="gu"&gt;@@ -165,9 +165,13 @@ cp -p common.gypi %{buildroot}%{_datadir}/node&lt;/span&gt;

 %files docs
 %{_defaultdocdir}/%{name}-docs-%{version}
&lt;span class="gd"&gt;-%doc &lt;span class="caps"&gt;LICENSE&lt;/span&gt;&lt;/span&gt;

 %changelog
&lt;span class="gi"&gt;+* Thu Jan 31 2013 Jason Antman  - 0.9.5-10&lt;/span&gt;
&lt;span class="gi"&gt;+- specify build requirement of c-ares-devel &amp;gt;= 1.9.0&lt;/span&gt;
&lt;span class="gi"&gt;+- specify build requirement of libuv-devel 0.9.4&lt;/span&gt;
&lt;span class="gi"&gt;+- remove duplicate %doc &lt;span class="caps"&gt;LICENSE&lt;/span&gt; that was causing cpio &amp;#39;Bad magic&amp;#39; error on CentOS6&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
 * Sat Jan 12 2013 &lt;span class="caps"&gt;T.C.&lt;/span&gt; Hollingsworth  - 0.9.5-9
 - fix brown paper bag bug in requires generation script
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;v8.spec&lt;/strong&gt;, diff from Fedora Rawhide 3.13.7.5-2 (&lt;a href="https://raw.github.com/jantman/specfiles/master/v8.spec"&gt;full
specfile&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gd"&gt;--- v8.spec.orig       2013-01-26 16:03:18.000000000 -0500&lt;/span&gt;
&lt;span class="gi"&gt;+++ v8.spec     2013-01-31 09:04:51.068029459 -0500&lt;/span&gt;
&lt;span class="gu"&gt;@@ -21,9 +21,11 @@&lt;/span&gt;

 # %%global svnver 20110721svn8716

&lt;span class="gi"&gt;+%{!?python_sitelib: %define python_sitelib %(%{__python} -c &amp;quot;import distutils.sysconfig as d; print d.get_python_lib()&amp;quot;)}&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
 Name:          v8
 Version:       %{somajor}.%{sominor}.%{sobuild}.%{sotiny}
&lt;span class="gd"&gt;-Release:       2%{?dist}&lt;/span&gt;
&lt;span class="gi"&gt;+Release:       5%{?dist}&lt;/span&gt;
 Epoch:         1
 Summary:       JavaScript Engine
 Group:         System Environment/Libraries
&lt;span class="gu"&gt;@@ -32,7 +34,7 @@&lt;/span&gt;
 Source0:       http://commondatastorage.googleapis.com/chromium-browser-official/v8-%{version}.tar.bz2
 BuildRoot:     %{_tmppath}/%{name}-%{version}-%{release}-root-%(%{__id_u} -n)
 ExclusiveArch: %{ix86} x86_64 %{arm}
&lt;span class="gd"&gt;-BuildRequires: scons, readline-devel, libicu-devel&lt;/span&gt;
&lt;span class="gi"&gt;+BuildRequires: scons, readline-devel, libicu-devel, ncurses-devel&lt;/span&gt;

 %description
 V8 is Google&amp;#39;s open source JavaScript engine. V8 is written in C++ and is used 
&lt;span class="gu"&gt;@@ -51,8 +53,13 @@&lt;/span&gt;
 %setup -q -n %{name}-%{version}

 # -fno-strict-aliasing is needed with gcc 4.4 to get past some ugly code
&lt;span class="gd"&gt;-PARSED_OPT_FLAGS=`echo \&amp;#39;$RPM_OPT_FLAGS -fPIC -fno-strict-aliasing -Wno-unused-parameter -Wno-error=strict-overflow -Wno-error=unused-local-typedefs -Wno-unused-but-set-variable\&amp;#39;| sed &amp;quot;s/ /&amp;#39;,/g&amp;quot; | sed &amp;quot;s/&amp;#39;,/&amp;#39;, &amp;#39;/g&amp;quot;`&lt;/span&gt;
&lt;span class="gi"&gt;+%if 0%{?el5}&lt;/span&gt;
&lt;span class="gi"&gt;+PARSED_OPT_FLAGS=`echo \&amp;#39;$RPM_OPT_FLAGS -fPIC -fno-strict-aliasing -Wno-unused-parameter -lncurses\&amp;#39;| sed &amp;quot;s/ /&amp;#39;,/g&amp;quot; | sed &amp;quot;s/&amp;#39;,/&amp;#39;, &amp;#39;/g&amp;quot;`&lt;/span&gt;
&lt;span class="gi"&gt;+sed -i &amp;quot;s|&amp;#39;-O3&amp;#39;,|$PARSED_OPT_FLAGS,|g&amp;quot; SConstruct&lt;/span&gt;
&lt;span class="gi"&gt;+%else&lt;/span&gt;
&lt;span class="gi"&gt;+PARSED_OPT_FLAGS=`echo \&amp;#39;$RPM_OPT_FLAGS -fPIC -fno-strict-aliasing -Wno-unused-parameter -Wno-error=strict-overflow -Wno-unused-but-set-variable\&amp;#39;| sed &amp;quot;s/ /&amp;#39;,/g&amp;quot; | sed &amp;quot;s/&amp;#39;,/&amp;#39;, &amp;#39;/g&amp;quot;`&lt;/span&gt;
 sed -i &amp;quot;s|&amp;#39;-O3&amp;#39;,|$PARSED_OPT_FLAGS,|g&amp;quot; SConstruct
&lt;span class="gi"&gt;+%endif&lt;/span&gt;

 # clear spurious executable bits
 find . \( -name \*.cc -o -name \*.h -o -name \*.py \) -a -executable   
&lt;span class="gu"&gt;@@ -198,6 +205,17 @@&lt;/span&gt;
 %{python_sitelib}/j*.py*

 %changelog
&lt;span class="gi"&gt;+* Thu Jan 31 2013 Jason Antman  - 1:3.13.7.5-5&lt;/span&gt;
&lt;span class="gi"&gt;+- remove -Werror=unused-local-typedefs on cent6&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+* Wed Jan 30 2013 Jason Antman  - 1:3.13.7.5-4&lt;/span&gt;
&lt;span class="gi"&gt;+- define python_sitelib if it isn&amp;#39;t already (CentOS 5)&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+* Wed Jan 30 2013 Jason Antman  - 1:3.13.7.5-3&lt;/span&gt;
&lt;span class="gi"&gt;+- pull 3.13.7.5-2 &lt;span class="caps"&gt;SRPM&lt;/span&gt; from Fedora 19 Koji most recent build&lt;/span&gt;
&lt;span class="gi"&gt;+- add ncurses-devel BuildRequires&lt;/span&gt;
&lt;span class="gi"&gt;+- modify PARSED_OPT_FLAGS to work with g++ 4.1.2 on CentOS 5&lt;/span&gt;
&lt;span class="gi"&gt;+ &lt;/span&gt;
 * Sat Jan 26 2013 &lt;span class="caps"&gt;T.C.&lt;/span&gt; Hollingsworth  - 1:3.13.7.5-2
 - rebuild for icu-50
 - ignore new &lt;span class="caps"&gt;GCC&lt;/span&gt; 4.8 warning
&lt;/pre&gt;&lt;/div&gt;</content><category term="build"></category><category term="centos"></category><category term="node"></category><category term="nodejs"></category><category term="package"></category><category term="packaging"></category><category term="redhat"></category><category term="RHEL"></category><category term="rpm"></category><category term="specfile"></category></entry><entry><title>Fedora Linux and OSX Dual Boot on Mid-2010 (6,2) 15â MacBook ProÂ Laptop</title><link href="http://blog.jasonantman.com/2013/01/fedora-linux-and-osx-dual-boot-on-mid-2010-62-15-macbook-pro-laptop/" rel="alternate"></link><published>2013-01-21T12:13:00-05:00</published><updated>2013-01-21T12:13:00-05:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-01-21:/2013/01/fedora-linux-and-osx-dual-boot-on-mid-2010-62-15-macbook-pro-laptop/</id><summary type="html">&lt;p&gt;As part of the transition from a contractor to a full-time employee of
&lt;a href="http://www.cmgdigital.com"&gt;Cox Media Group Digital &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Strategy&lt;/a&gt; (check
out our &lt;a href="https://github.com/cmgdigital"&gt;github&lt;/a&gt;), I&amp;#8217;ve been issued a
&lt;a href="http://support.apple.com/kb/SP582"&gt;Mid-2010 (6,2)&lt;/a&gt; 15&amp;#8221; &lt;a href="http://en.wikipedia.org/wiki/Macbook_pro#Technical_specifications_2"&gt;MacBook
Pro&lt;/a&gt;
laptop, to replace my current &lt;a href="http://support.apple.com/kb/SP11"&gt;Early-2008
(3,1)&lt;/a&gt; MacPro desktop. The desktop is
currently running &lt;a href="http://fedoraproject.org/"&gt;Fedora&lt;/a&gt; 17 â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;As part of the transition from a contractor to a full-time employee of
&lt;a href="http://www.cmgdigital.com"&gt;Cox Media Group Digital &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Strategy&lt;/a&gt; (check
out our &lt;a href="https://github.com/cmgdigital"&gt;github&lt;/a&gt;), I&amp;#8217;ve been issued a
&lt;a href="http://support.apple.com/kb/SP582"&gt;Mid-2010 (6,2)&lt;/a&gt; 15&amp;#8221; &lt;a href="http://en.wikipedia.org/wiki/Macbook_pro#Technical_specifications_2"&gt;MacBook
Pro&lt;/a&gt;
laptop, to replace my current &lt;a href="http://support.apple.com/kb/SP11"&gt;Early-2008
(3,1)&lt;/a&gt; MacPro desktop. The desktop is
currently running &lt;a href="http://fedoraproject.org/"&gt;Fedora&lt;/a&gt; 17, dual-boot with
with Mac &lt;span class="caps"&gt;OS&lt;/span&gt; X (left in place for firmware updates and emergencies) using
the &lt;a href="http://www.rodsbooks.com/refind/index.html"&gt;rEFInd boot manager&lt;/a&gt; to
choose between the two OSes. It took me two days to get this working
right on my desktop, but it had been my plan to duplicate this setup on
my laptop. I found a lot of conflicting information online, but I
decided to give it a&amp;nbsp;try.&lt;/p&gt;
&lt;p&gt;Well, I have Fedora 18 and &lt;span class="caps"&gt;OS&lt;/span&gt; X 10.8 dual-booting on the laptop, but not
as planned. After a day and a half of research, troubleshooting and
re-installs, here&amp;#8217;s what I found to actually work, in the hope that
nobody else will go through the ordeal I went through. Following that
are some notes about the new Fedora 18 installer (Anaconda 18),
especially important for anyone who&amp;#8217;s used Linux for a while. To those
who are new to Linux, don&amp;#8217;t be dissuaded by the above. Most of the
frustration I experienced is because I&amp;#8217;ve been using Linux for a
relatively long time (about 10 years), had my own ideas about exactly
how I wanted things setup (which are decidedly &lt;em&gt;not&lt;/em&gt; supported by
Fedora), and had some assumptions about the installation process based
on earlier&amp;nbsp;versions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to get it&amp;nbsp;working:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Forget about rEFInd. This had been the original advice from &lt;a href="http://mjg59.dreamwidth.org/"&gt;Matthew
Garrett&lt;/a&gt;,
&lt;a href="https://twitter.com/mjg59"&gt;@mjg59&lt;/a&gt;, kernel coder, contributor to the
Anaconda project, and all-around authority on booting Linux on &lt;span class="caps"&gt;EFI&lt;/span&gt;/&lt;span class="caps"&gt;UEFI&lt;/span&gt;
hardware. My advice, and the method that worked for&amp;nbsp;me:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Shrink your Mac partitions and leave as much free space as you want
    for Fedora. using the Disk Utility tool in &lt;span class="caps"&gt;OS&lt;/span&gt; X (I also created an
    &lt;span class="caps"&gt;8GB&lt;/span&gt; &lt;span class="caps"&gt;VFAT&lt;/span&gt; partition that both OSes can read/write&amp;nbsp;to).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://fedoraproject.org/en/get-fedora"&gt;Download Fedora 18&lt;/a&gt; 64-bit
    &lt;span class="caps"&gt;DVD&lt;/span&gt; image, I chose the &lt;span class="caps"&gt;KDE&lt;/span&gt; version. Verify the sha256 sum if you
    want (they don&amp;#8217;t have a readily visible link to the checksum file.
    Copy the download link, paste it into your address bar and remove
    the filename. You should get a directory index that includes a
    &lt;code&gt;-CHECKSUM&lt;/code&gt; file.&lt;/li&gt;
&lt;li&gt;Per the Installation Guide&amp;#8217;s &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/18/html/Installation_Guide/Making_USB_Media-UNIX_Linux.html"&gt;Making Fedora &lt;span class="caps"&gt;USB&lt;/span&gt; Media
    page&lt;/a&gt;,
    use &lt;code&gt;liveusb-creator&lt;/code&gt; to setup the installation image on the &lt;span class="caps"&gt;USB&lt;/span&gt;
    flash drive (I needed to start it with the &lt;code&gt;--reset-mbr&lt;/code&gt; option).
    You can also use other tools (dd if you&amp;#8217;re not on a Fedora-based
    distro), or a &lt;span class="caps"&gt;DVD&lt;/span&gt;, but this is the method I&amp;nbsp;chose.&lt;/li&gt;
&lt;li&gt;Due to a &lt;a href="https://fedorahosted.org/liveusb-creator/ticket/810"&gt;bug in
    liveusb-creator&lt;/a&gt;,
    you may need to manually edit &lt;code&gt;/EFI/boot/grub.cfg&lt;/code&gt; on the created
    &lt;span class="caps"&gt;USB&lt;/span&gt; stick if grub gives you a file not found error. If that happens,
    please see my bug report above for the action to take (in short, you
    need to mount the &lt;span class="caps"&gt;USB&lt;/span&gt; stick, &lt;code&gt;chmod u+w /EFI/boot/grub.cfg&lt;/code&gt; then
    edit that file and replace every occurrence of &amp;#8220;isolinux&amp;#8221; with
    &amp;#8220;syslinux&amp;#8221; and every occurrence of
    &amp;#8220;root=live:&lt;span class="caps"&gt;LABEL&lt;/span&gt;=Fedora-18-x86_64-Live-&lt;span class="caps"&gt;KDE&lt;/span&gt;.iso&amp;#8221; with&amp;nbsp;&amp;#8220;root=live:&lt;span class="caps"&gt;LABEL&lt;/span&gt;=&lt;span class="caps"&gt;LIVE&lt;/span&gt;&amp;#8221;).&lt;/li&gt;
&lt;li&gt;Boot the &lt;span class="caps"&gt;USB&lt;/span&gt; drive (use the alt key when you turn on the laptop to
    select the &lt;span class="caps"&gt;USB&lt;/span&gt; drive) and just install Fedora normally, letting it
    do its thing. Select a boot disk and let it put &lt;span class="caps"&gt;GRUB2&lt;/span&gt; on the &lt;span class="caps"&gt;EFI&lt;/span&gt;&amp;nbsp;partition.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you boot, it will boot to &lt;span class="caps"&gt;GRUB&lt;/span&gt;. There will be some options for Mac
&lt;span class="caps"&gt;OS&lt;/span&gt; there, but they don&amp;#8217;t work (more on that below). If you want to boot
Mac, hold down the alt/option key when you power on the laptop, which
will bring you to the boot disk selector and you can pick the Mac disk.
I know it&amp;#8217;s not pretty or ideal, but it&amp;#8217;s the best option right&amp;nbsp;now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Making it&amp;nbsp;Better:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;GRUB2&lt;/span&gt; tries to automatically detect other OSes and configure them in the
boot loader (this is done through &lt;code&gt;/etc/grub.d/30_os-prober&lt;/code&gt;, commonly
just referred to as &lt;code&gt;os-prober&lt;/code&gt;). It tries to boot Mac directly through
the xnu_kernel64 module, which not only isn&amp;#8217;t installed on the boot
partition by default, but just doesn&amp;#8217;t work with at least Mountain Lion
(10.8). So getting &lt;span class="caps"&gt;GRUB&lt;/span&gt; to boot Mac means either having the bugs in the
xnu module fixed, or figuring out how to setup a chainloader to boot
from &lt;span class="caps"&gt;GRUB&lt;/span&gt; to Mac. The latter is probably the method I&amp;#8217;ll investigate,
but for now, since I rarely use Mac, I&amp;#8217;m happy having to use the alt key
at boot to get there. To remove the annoying, broken Mac &lt;span class="caps"&gt;OS&lt;/span&gt; options from
the grub screen, run the following commands as root (they assume you
have your &lt;span class="caps"&gt;EFI&lt;/span&gt; partition mounted at &lt;code&gt;/boot/efi&lt;/code&gt; which I believe Fedora
should do by&amp;nbsp;default:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;cp /boot/efi/&lt;span class="caps"&gt;EFI&lt;/span&gt;/fedora/grub.cfg /boot/efi/&lt;span class="caps"&gt;EFI&lt;/span&gt;/fedora/grub.cfg.bak
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;GRUB_DISABLE_OS_PROBER=&amp;quot;true&amp;quot;&amp;#39;&lt;/span&gt; &amp;gt;&amp;gt; /etc/default/grub
grub2-mkconfig &amp;gt; /boot/efi/&lt;span class="caps"&gt;EFI&lt;/span&gt;/fedora/grub.cfg
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Thoughts on the Fedora 18 Anaconda&amp;nbsp;Installer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I found a couple of issues with the new Anaconda 18 installer that were
either unweildy or confusing for someone who&amp;#8217;s been installing Linux for
a long time. Overall, the new installer is very nice. It has a clean,
even elegant &lt;span class="caps"&gt;UI&lt;/span&gt;, a relatively nice flow from start to completion, and is
certainly beginner-friendly. It has fewer options than any Linux
installer I&amp;#8217;ve ever used before - not even options for package
selection, firewall or SELinux configuration, etc. - but I guess this is
in line with the goal of making Fedora a desktop &lt;span class="caps"&gt;OS&lt;/span&gt; for the masses. I
would have appreciated an &amp;#8220;advanced mode&amp;#8221; installer that was more like
Fedora 17 (or even much older versions), but I guess I&amp;#8217;m an edge case,
at least in the Fedora community. However, I did find two things
especially difficult, both related to the fact that my laptop has two
main drives (a &lt;span class="caps"&gt;500GB&lt;/span&gt; hard drive and a &lt;span class="caps"&gt;120GB&lt;/span&gt;&amp;nbsp;&lt;span class="caps"&gt;SSD&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;First, the installer prompted me to select a &amp;#8220;boot disk&amp;#8221;. I guess I
should have read the installation guide, but I assumed that nomenclature
translated to either &amp;#8220;which disk should the automatic partitiioning put
yout &lt;code&gt;/boot&lt;/code&gt; partition on&amp;#8221; or &amp;#8220;which disk should I set the bootable flag
on in the partition table&amp;#8221;. In fact, it means &amp;#8220;which disk should I put
&lt;span class="caps"&gt;GRUB&lt;/span&gt; on the &lt;span class="caps"&gt;EFI&lt;/span&gt; partition of&amp;#8221;. I installed, rebooted, and was shocked -
and somewhat distressed - to boot directly to &lt;span class="caps"&gt;GRUB2&lt;/span&gt; instead of the
rEFInd installation I had setup. The installer didn&amp;#8217;t have any of the
previously-customary &amp;#8220;warning: this will overwrite your &lt;span class="caps"&gt;MBR&lt;/span&gt;/&lt;span class="caps"&gt;EFI&lt;/span&gt; boot
partition&amp;#8221; notices, so I felt safe letting it continue. It turned out
that this was the way I ended up going, and it also turns out that
there&amp;#8217;s a bug in Anaconda that makes it fail installation if you tell it
not to write a bootloader to disk (though it&amp;#8217;s patched by one line of
Python code). But I was deeply distressed that - contrary to the
experience of every, admittedly more complicated, Linux installer I&amp;#8217;d
used before - the Fedora 18 installer overwrote my &lt;span class="caps"&gt;EFI&lt;/span&gt; bootloader
(analogous to overwriting the &lt;span class="caps"&gt;MBR&lt;/span&gt; on a &lt;span class="caps"&gt;BIOS&lt;/span&gt; boot machine) without ever
warning me or asking for a&amp;nbsp;confirmation.&lt;/p&gt;
&lt;p&gt;Secondly, the partitioning tool is clearly designed for only one
destination disk. The overview screen lists configured partitions by
label and mount point, but not by physical device, so figuring out which
partitions are on which physical disks takes a click on each and every
partition to view that information in the detail panel. When you create
a new partition, it&amp;#8217;s automatically put in a &lt;span class="caps"&gt;LVM&lt;/span&gt; volume group spanning
all disks. Changing the target of the automatically created volume group
requires a few clicks, as does changing the physical disks backing any
new volume groups. To assign a newly created partition to a specific
disk, you have to click on an unlabeled &amp;#8220;tool&amp;#8221; icon under the list of
partitions, far away from the information on the partition in question.
It&amp;#8217;s a nice interface for someone who clicks the &amp;#8220;partition
automatically&amp;#8221; button, or who just knows they want to add &amp;#8220;an extra
partition&amp;#8221;, but for anyone who has a specific layout in mind (like
having &lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/boot&lt;/code&gt; and &lt;code&gt;/var&lt;/code&gt;, specifically sized, on the &lt;span class="caps"&gt;SSD&lt;/span&gt; and
&lt;code&gt;/home&lt;/code&gt; on the rotating disk) it takes about 4-5 more clicks and dialogs
to add a partition than the last Fedora installer did. Mainly, it&amp;#8217;s
lacking any sort of Advanced Mode for partitioning that allows the user
to quickly and accurately layout a more complex partitioning&amp;nbsp;scheme.&lt;/p&gt;
&lt;p&gt;Below are some screenshots from the Fedora 17 and Fedora 18 Installation
Guides, which contrast both the overview of all partitions and the
individual partition&amp;nbsp;settings:&lt;/p&gt;
&lt;p&gt;Fedora 18 Overview, from &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/18/html/Installation_Guide/s1-diskpartitioning-x86.html"&gt;9.13. Creating a Custom Partition
Layout&lt;/a&gt;:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="http://www.dedoimedo.com/images/computers_years/2013_1/fedora-18-installer-configure-partitions.jpg"&gt;  &lt;/p&gt;
&lt;p&gt;Fedora 17 Overview, from &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/17/html/Installation_Guide/s1-diskpartitioning-x86.html"&gt;9.14. Creating a Custom Layout or Modifying
the Default
Layout&lt;/a&gt;:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="http://docs.fedoraproject.org/en-US/Fedora/17/html/Installation_Guide/images/diskpartitioning/ddmain.png"&gt;  &lt;/p&gt;
&lt;p&gt;Fedora 18 Partition Creation/Editing, from &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/18/html/Installation_Guide/Create_LVM-x86.html"&gt;9.13.3. Create &lt;span class="caps"&gt;LVM&lt;/span&gt; Logical
Volume&lt;/a&gt;:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="http://docs.fedoraproject.org/en-US/Fedora/18/html/Installation_Guide/images/diskpartitioning/lvm-pv.png"&gt;  &lt;/p&gt;
&lt;p&gt;Fedora 17 Partition Creation/Editing, from &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/17/html/Installation_Guide/Adding_Partitions-x86.html"&gt;9.14.2. Adding
Partitions&lt;/a&gt;:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="http://docs.fedoraproject.org/en-US/Fedora/17/html/Installation_Guide/images/diskpartitioning/part-add.png"&gt;&lt;/p&gt;</content><category term="bootloader"></category><category term="efi"></category><category term="fedora"></category><category term="gpt"></category><category term="grub"></category><category term="installation"></category><category term="laptop"></category><category term="mac"></category><category term="macbook"></category><category term="os x"></category></entry><entry><title>Fedora Init Script SpecificationÂ Summary</title><link href="http://blog.jasonantman.com/2013/01/fedora-init-script-specification-summary/" rel="alternate"></link><published>2013-01-03T11:30:00-05:00</published><updated>2013-01-03T11:30:00-05:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2013-01-03:/2013/01/fedora-init-script-specification-summary/</id><summary type="html">&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; 2014-12-16: I&amp;#8217;m leaving this here for historical reasons, and since
some older &lt;span class="caps"&gt;OS&lt;/span&gt; versions still need properly-written init scripts. Since starting
to use &lt;a href="https://www.archlinux.org/"&gt;Arch Linux&lt;/a&gt; on my laptop, I&amp;#8217;ve become a
convert to the &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/"&gt;systemd&lt;/a&gt;
world. I know this is a &lt;a href="http://en.wikipedia.org/wiki/Systemd#Criticism"&gt;hot topic&lt;/a&gt;
and has sparked a â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; 2014-12-16: I&amp;#8217;m leaving this here for historical reasons, and since
some older &lt;span class="caps"&gt;OS&lt;/span&gt; versions still need properly-written init scripts. Since starting
to use &lt;a href="https://www.archlinux.org/"&gt;Arch Linux&lt;/a&gt; on my laptop, I&amp;#8217;ve become a
convert to the &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/"&gt;systemd&lt;/a&gt;
world. I know this is a &lt;a href="http://en.wikipedia.org/wiki/Systemd#Criticism"&gt;hot topic&lt;/a&gt;
and has sparked a lot of controversy. While I agree with some of the arguments
in principal, I strongly feel that systemd provides the interface that
Linux needs in modern times, and provides a unified solution to many problems
that were previously solved in myriad ways inside init scripts. In short,
if your distro supports systemd, I&amp;#8217;d recommend to skip past this page
and go ahead and write a &lt;a href="http://www.freedesktop.org/software/systemd/man/systemd.unit.html"&gt;unit file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve been deploying some new software lately (specifically
&lt;a href="https://github.com/marisaseal/selenesse"&gt;selenesse&lt;/a&gt;, which combines
&lt;a href="http://seleniumhq.org/"&gt;Selenium&lt;/a&gt; and &lt;a href="http://fitnesse.org/"&gt;fitnesse&lt;/a&gt;,
&lt;a href="http://en.wikipedia.org/wiki/Xvfb"&gt;xvfb&lt;/a&gt;). None of these seem to come
with init scripts to run as daemons, and the quality of the few
Fedora/RedHat/CentOS init scripts I was able to find was quite poor. The
Fedora project has a &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript"&gt;Specification for SysV-style Init Scripts in their
Packaging wiki&lt;/a&gt;,
which specifies what a Fedora/RedHat/CentOS init script should look
like, in excruciating detail. What follows is an overview of the more
important points, which I&amp;#8217;m using to develop or modify the scripts I&amp;#8217;m
currently working&amp;nbsp;on.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scripts must be put in &lt;code&gt;/etc/rc.d/init.d&lt;/code&gt;, not in the &lt;code&gt;/etc/init.d&lt;/code&gt;
    symlink. They should have 0755&amp;nbsp;permissions.&lt;/li&gt;
&lt;li&gt;Scripts must have a Fedora-style &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#Chkconfig_Header"&gt;chkconfig
    header&lt;/a&gt;
    (&amp;#8220;chkconfig:&amp;#8221;, &amp;#8220;description:&amp;#8221; lines), and may have an &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#LSB_Header"&gt;&lt;span class="caps"&gt;LSB&lt;/span&gt;-style
    header&lt;/a&gt;
    (&lt;span class="caps"&gt;BEGIN&lt;/span&gt; &lt;span class="caps"&gt;INIT&lt;/span&gt; &lt;span class="caps"&gt;INFO&lt;/span&gt;/&lt;span class="caps"&gt;END&lt;/span&gt; &lt;span class="caps"&gt;INIT&lt;/span&gt; &lt;span class="caps"&gt;INFO&lt;/span&gt;). See &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#Initscript_template"&gt;Initscript
    template&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Scripts &lt;strong&gt;must&lt;/strong&gt; make use of a lockfile in &lt;code&gt;/var/lock/subsys/&lt;/code&gt;, and
    the name of the lockfile must be the same as the name of the init
    script. (There is a technical reason for this relating to how sysv
    init terminates daemons at shutdown). The lockfile should be touched
    when the daemon successfully starts, and removed when it
    successfully&amp;nbsp;stops.&lt;/li&gt;
&lt;li&gt;Init scripts should not depend on any environment variables set
    outside the script. They should operate gracefully with an
    empty/uninitialized environment (or only &lt;span class="caps"&gt;LANG&lt;/span&gt; and &lt;span class="caps"&gt;TERM&lt;/span&gt; set and a &lt;span class="caps"&gt;CWD&lt;/span&gt;
    of &lt;code&gt;/&lt;/code&gt;, as enforced by &lt;code&gt;service(8)&lt;/code&gt;, or with a full environment if
    they are called directly by a&amp;nbsp;user.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#Required_Actions"&gt;Required&amp;nbsp;actions&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all of the following actions are required, and have specific&amp;nbsp;definitions:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;start&lt;/strong&gt;: starts the&amp;nbsp;service&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;stop&lt;/strong&gt;: stops the&amp;nbsp;service&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;restart&lt;/strong&gt;: stop and restart the service if the service is
    already running, otherwise just start the&amp;nbsp;service&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;condrestart (and try-restart)&lt;/strong&gt;: restart the service if the
    service is already running, if not, do&amp;nbsp;nothing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reload&lt;/strong&gt;: reload the configuration of the service without
    actually stopping and restarting the service (if the service
    does not support this, do&amp;nbsp;nothing)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;force-reload&lt;/strong&gt;: reload the configuration of the service and
    restart it so that it takes&amp;nbsp;effect&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;status&lt;/strong&gt;: print the current status of the&amp;nbsp;service&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;usage&lt;/strong&gt;: by default, if the initscript is run without any
    action, it should list a &amp;#8220;usage message&amp;#8221; that has all actions
    (intended for&amp;nbsp;use)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are specified exit codes for &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#Exit_Codes_for_the_Status_Action"&gt;status
    actions&lt;/a&gt;
    and &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#Exit_Codes_for_non-Status_Actions"&gt;non-status
    actions&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;They must &amp;#8220;behave sensibly&amp;#8221;. I&amp;#8217;ve found this to be one of the
    biggest problems with homegrown init scripts. If &lt;code&gt;servicename start&lt;/code&gt;
    is called while the service is already running, it should simply
    exit 0. Likewise if the service is already stopped. Init scripts
    &lt;strong&gt;must not kill unrelated processes&lt;/strong&gt;. I don&amp;#8217;t know how many times
    I&amp;#8217;ve seen scripts that kill every java or python process on a&amp;nbsp;machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I intend to use this as a quick checklist when developing or evaluating
init scripts for RedHat/Fedora based systems. In my experience, the
biggest problems with most init scripts revolve around poor handling of
&lt;span class="caps"&gt;PID&lt;/span&gt; files and lockfiles,&amp;nbsp;mainly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Killing processes other than the one that the script started (i.e.
    killing all java or python processes), usually because the &lt;span class="caps"&gt;PID&lt;/span&gt; isn&amp;#8217;t
    tracked at&amp;nbsp;start&lt;/li&gt;
&lt;li&gt;Starting a second instance of the subsystem because lockfiles aren&amp;#8217;t
    used, or the status function is&amp;nbsp;broken.&lt;/li&gt;
&lt;li&gt;improper exit&amp;nbsp;codes&lt;/li&gt;
&lt;li&gt;either explicitly relying on environment variables (and therefore
    breaking when called through &lt;code&gt;service(8)&lt;/code&gt;), or conversely, not
    cleaning/resetting environment variables that are used by dependent
    code or&amp;nbsp;processes.&lt;/li&gt;
&lt;/ul&gt;</content><category term="centos"></category><category term="fedora"></category><category term="init"></category><category term="redhat"></category><category term="startup"></category></entry><entry><title>Random Links for Wednesday, OctoberÂ 24th</title><link href="http://blog.jasonantman.com/2012/10/random-links-for-wednesday-october-24th/" rel="alternate"></link><published>2012-10-24T12:01:00-04:00</published><updated>2012-10-24T12:01:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2012-10-24:/2012/10/random-links-for-wednesday-october-24th/</id><summary type="html">&lt;p&gt;Some random interesting links from Slashdot for&amp;nbsp;today:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://news.slashdot.org/story/12/10/23/2038220/the-greatest-battle-of-the-personal-computing-revolution-lies-ahead"&gt;The Greatest Battle of the Personal Computing Revolution Lies Ahead
    -
    Slashdot&lt;/a&gt;.
    A bit of a rant, but makes some good points that are close to my
    heart, and unfortunately far from the thoughts of many&amp;nbsp;non-techies.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tos-dr.info/"&gt;Terms of Service; Didn&amp;#8217;t Read â¦&lt;/a&gt;&lt;/li&gt;&lt;/ul&gt;</summary><content type="html">&lt;p&gt;Some random interesting links from Slashdot for&amp;nbsp;today:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://news.slashdot.org/story/12/10/23/2038220/the-greatest-battle-of-the-personal-computing-revolution-lies-ahead"&gt;The Greatest Battle of the Personal Computing Revolution Lies Ahead
    -
    Slashdot&lt;/a&gt;.
    A bit of a rant, but makes some good points that are close to my
    heart, and unfortunately far from the thoughts of many&amp;nbsp;non-techies.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tos-dr.info/"&gt;Terms of Service; Didn&amp;#8217;t Read&lt;/a&gt; - an
    interesting&amp;nbsp;project&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yro.slashdot.org/story/12/10/21/208206/how-patent-trolls-harm-the-economy"&gt;How Patent Trolls Harm the Economy -
    Slashdot&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;another issue close to my&amp;nbsp;heart&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.datacenterknowledge.com/archives/2012/10/17/how-google-cools-its-armada-of-servers/"&gt;How Google Cools Its Armada of Servers Â» Data Center&amp;nbsp;Knowledge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.google.com/about/datacenters/gallery/#/"&gt;Data centers â Google Data
    centers&lt;/a&gt; - A
    photo tour of Google data centers, by Google, along with a &lt;a href="https://plus.google.com/+google/posts/Gk8ScjPX23n"&gt;Google+
    post&lt;/a&gt; about the
    architecture photographer who did this&amp;nbsp;work.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tech.slashdot.org/story/12/10/22/0518231/darpa-funds-a-300-software-defined-radio-for-hackers"&gt;&lt;span class="caps"&gt;DARPA&lt;/span&gt; Funds a $300 Software-Defined Radio For Hackers -
    Slashdot&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;way&amp;nbsp;cool.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://map.honeynet.org/"&gt;Honeynet Map&lt;/a&gt; - &amp;#8220;realtime&amp;#8221; map of
    cybersecurity incidents, from the Honeynet&amp;nbsp;Project.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://science.slashdot.org/story/12/10/17/1741225/malware-is-rampant-on-medical-devices-in-hospitals"&gt;Malware Is &amp;#8216;Rampant&amp;#8217; On Medical Devices In Hospitals -
    Slashdot&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;a bit scary, but unfortunately not that hard to guess. I&amp;#8217;ve seen
(probably unpatched) Windows 2000 workstations on hospital&amp;nbsp;networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To top off the scary posts: &lt;a href="http://news.slashdot.org/story/12/10/17/0325236/researcher-reverse-engineers-pacemaker-transmitter-to-deliver-deadly-shocks"&gt;Researcher Reverse-Engineers Pacemaker
    Transmitter To Deliver Deadly Shocks -&amp;nbsp;Slashdot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><category term="appliancization"></category><category term="cooling"></category><category term="datacenter"></category><category term="google"></category><category term="healthcare"></category><category term="legal"></category><category term="links"></category><category term="malware"></category><category term="pacemaker"></category><category term="patents"></category><category term="radio"></category><category term="SDR"></category><category term="security"></category></entry><entry><title>Readable Nagios LogÂ Timestamps</title><link href="http://blog.jasonantman.com/2012/10/readable-nagios-log-timestamps/" rel="alternate"></link><published>2012-10-17T05:00:00-04:00</published><updated>2012-10-17T05:00:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2012-10-17:/2012/10/readable-nagios-log-timestamps/</id><summary type="html">&lt;p&gt;If you&amp;#8217;re like me and most humans, the Nagios logfile timestamp (a unix
timestamp) isn&amp;#8217;t terribly useful when trying to grep through the logs
and correlate&amp;nbsp;events:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;#&lt;/span&gt; head -2 nagios.log
&lt;span class="go"&gt;[1350360000] &lt;span class="caps"&gt;LOG&lt;/span&gt; &lt;span class="caps"&gt;ROTATION&lt;/span&gt;: &lt;span class="caps"&gt;DAILY&lt;/span&gt;&lt;/span&gt;
&lt;span class="go"&gt;[1350360000] &lt;span class="caps"&gt;LOG&lt;/span&gt; &lt;span class="caps"&gt;VERSION&lt;/span&gt;: 2.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here&amp;#8217;s a nifty Perl one-liner that you â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;If you&amp;#8217;re like me and most humans, the Nagios logfile timestamp (a unix
timestamp) isn&amp;#8217;t terribly useful when trying to grep through the logs
and correlate&amp;nbsp;events:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gp"&gt;#&lt;/span&gt; head -2 nagios.log
&lt;span class="go"&gt;[1350360000] &lt;span class="caps"&gt;LOG&lt;/span&gt; &lt;span class="caps"&gt;ROTATION&lt;/span&gt;: &lt;span class="caps"&gt;DAILY&lt;/span&gt;&lt;/span&gt;
&lt;span class="go"&gt;[1350360000] &lt;span class="caps"&gt;LOG&lt;/span&gt; &lt;span class="caps"&gt;VERSION&lt;/span&gt;: 2.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here&amp;#8217;s a nifty Perl one-liner that you can pipe your logs&amp;nbsp;through:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;perl&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;pe&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;s/(\\d+)/localtime($1)/e&amp;#39;&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to get nicer output&amp;nbsp;like:  &lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# head -2 nagios.log&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Tue&lt;/span&gt; &lt;span class="n"&gt;Oct&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="mo"&gt;00&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;LOG&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;ROTATION&lt;/span&gt;:&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;DAILY&lt;/span&gt;&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Tue&lt;/span&gt; &lt;span class="n"&gt;Oct&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="mo"&gt;00&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;LOG&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;VERSION&lt;/span&gt;:&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</content><category term="icinga"></category><category term="Nagios"></category><category term="perl"></category><category term="timestamp"></category></entry><entry><title>Custom Tombstone and Road SignÂ Pictures</title><link href="http://blog.jasonantman.com/2012/10/custom-tombstone-and-road-sign-pictures/" rel="alternate"></link><published>2012-10-16T07:02:00-04:00</published><updated>2012-10-16T07:02:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2012-10-16:/2012/10/custom-tombstone-and-road-sign-pictures/</id><summary type="html">&lt;p&gt;On the lighter side, I found a few web sites by &lt;a href="http://www.pixbytom.com/"&gt;Tom
Blackwell&lt;/a&gt; that do some fun stuff with text
overlays on images. seems like a nice little tool for those
end-of-project powerpoints, or to send out the monthly &amp;#8220;most rolled-back
commits&amp;#8221;&amp;nbsp;medal&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.tombstonebuilder.com/index.php"&gt;Custom Tombstone&amp;nbsp;Maker&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Image of tombstone, with 'Your Text Goes Here' carved into it" src="/GFX/my_tombstone.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.customroadsign.com/"&gt;CustomRoadSign.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Highway sign with 'Your Text Goes Here' written on it" src="/GFX/menusign.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.custommotelsign.com/"&gt;CustomMotelSign.com â¦&lt;/a&gt;&lt;/p&gt;</summary><content type="html">&lt;p&gt;On the lighter side, I found a few web sites by &lt;a href="http://www.pixbytom.com/"&gt;Tom
Blackwell&lt;/a&gt; that do some fun stuff with text
overlays on images. seems like a nice little tool for those
end-of-project powerpoints, or to send out the monthly &amp;#8220;most rolled-back
commits&amp;#8221;&amp;nbsp;medal&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.tombstonebuilder.com/index.php"&gt;Custom Tombstone&amp;nbsp;Maker&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Image of tombstone, with 'Your Text Goes Here' carved into it" src="/GFX/my_tombstone.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.customroadsign.com/"&gt;CustomRoadSign.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Highway sign with 'Your Text Goes Here' written on it" src="/GFX/menusign.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.custommotelsign.com/"&gt;CustomMotelSign.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Motel-style sign with 'Your Text Goes Here' written on it" src="/GFX/motelsign.jpg"&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.getamedal.com/"&gt;GetAMedal.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gold medal with 'Your Text Goes Here' written on it" src="/GFX/medal.jpg"&gt;&lt;/p&gt;</content><category term="graphics"></category><category term="humor"></category><category term="road sign"></category><category term="sign"></category><category term="tombstone"></category></entry><entry><title>All-Mechanical Computer InstructionalÂ Video</title><link href="http://blog.jasonantman.com/2012/10/all-mechanical-computer-instructional-video/" rel="alternate"></link><published>2012-10-12T20:54:00-04:00</published><updated>2012-10-12T20:54:00-04:00</updated><author><name>admin</name></author><id>tag:blog.jasonantman.com,2012-10-12:/2012/10/all-mechanical-computer-instructional-video/</id><summary type="html">&lt;p&gt;I saw a link to &lt;a href="http://www.youtube.com/watch?v=s1i-dnAH9Y4"&gt;this YouTube
video&lt;/a&gt; shared on &lt;a href="http://everythingsysadmin.com/2012/10/mechanical-computer-instructio.html"&gt;Tom
Limoncelli&amp;#8217;s
blog&lt;/a&gt;.
It&amp;#8217;s a 1953 &lt;span class="caps"&gt;US&lt;/span&gt; Navy instructional video about an all-mechanical fire
control computer. Yes, I really mean a &lt;em&gt;computer&lt;/em&gt; that can solve
continuously changing 25-variable fire control problems using only
mechanical means (gears, cams, etc â¦&lt;/p&gt;</summary><content type="html">&lt;p&gt;I saw a link to &lt;a href="http://www.youtube.com/watch?v=s1i-dnAH9Y4"&gt;this YouTube
video&lt;/a&gt; shared on &lt;a href="http://everythingsysadmin.com/2012/10/mechanical-computer-instructio.html"&gt;Tom
Limoncelli&amp;#8217;s
blog&lt;/a&gt;.
It&amp;#8217;s a 1953 &lt;span class="caps"&gt;US&lt;/span&gt; Navy instructional video about an all-mechanical fire
control computer. Yes, I really mean a &lt;em&gt;computer&lt;/em&gt; that can solve
continuously changing 25-variable fire control problems using only
mechanical means (gears, cams, etc.). Think about it for a minute - it&amp;#8217;s
truly mind-boggling. And really gives one an amazing appreciation for
the power of a simple pocket calculator, and the amazing engineering
that went into solving these problems before electronic computers. I&amp;#8217;m
usually not much of a math geek, but I watched the whole 40 minute video
and was in awe of both the simple ability to use three arms and a pin to
multiply numbers, and the amazingly precise engineering and machining it
would take to translate various rotation inputs into landing a shell on
a moving ship miles away. It&amp;#8217;s a really good watch, and will probably
leave you astonished by both how far technology has come (and what we
take for granted every day), and by the fact that feats of engineering
like this one worked quite&amp;nbsp;well.&lt;/p&gt;</content><category term="mechanical computer"></category></entry></feed>