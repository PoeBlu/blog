<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Jason Antman's Blog</title><link>http://blog.jasonantman.com/</link><description></description><atom:link href="http://blog.jasonantman.com/feeds/all.rss.xml" rel="self"></atom:link><lastBuildDate>Mon, 05 Oct 2015 16:46:00 -0400</lastBuildDate><item><title>A Rant on Post-Secondary TechÂ Education</title><link>http://blog.jasonantman.com/2015/10/a-rant-on-post-secondary-tech-education/</link><description>&lt;p&gt;&lt;em&gt;(Disclaimer: First, I know there&amp;#8217;s a wide range of curricula in the &lt;span class="caps"&gt;CS&lt;/span&gt;/tech education world,
and some schools are better than others; there are certainly programs that prepare students
much better than what I experienced. Second, these views are somewhat biased to my own
experience in the web, agile and DevOps worlds; for people who want to write Java for banks
their whole career, I&amp;#8217;m sure most schools prepare them&amp;nbsp;well.)&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s been a decade since I&amp;#8217;ve been in the University system, but I&amp;#8217;ve seen precious little to
indicate that much has changed since then. I graduated with a degree in &lt;a href="https://comminfo.rutgers.edu/component/cur,547/option,com_courses/sch,04/task,listing/"&gt;Information Technology
and Informatics (&lt;span class="caps"&gt;ITI&lt;/span&gt;)&lt;/a&gt;;
I switched majors partly because of my deep hatred for calculus, and partly
because I was already working for Rutgers University as a student systems programmer, and it
was painfully obvious how little the &lt;span class="caps"&gt;CS&lt;/span&gt; program would do to prepare me for a career. My time
in &lt;span class="caps"&gt;CS&lt;/span&gt; was spent writing pitifully small Java applications in teams as large as &lt;strong&gt;three&lt;/strong&gt; (any
team work was highly unusual) and playing with linked lists. I switched to &lt;span class="caps"&gt;ITI&lt;/span&gt; so I could take
classes on &lt;span class="caps"&gt;IT&lt;/span&gt; management and policy, information security and &amp;#8220;web application development&amp;#8221; (with
databases and &lt;span class="caps"&gt;JSON&lt;/span&gt;!) Since then, I&amp;#8217;ve had two family members enter &lt;span class="caps"&gt;CS&lt;/span&gt; or &lt;span class="caps"&gt;ECE&lt;/span&gt; programs, and I
tried to give them the best advice I could. I happened to be thinking about it, and figured
I&amp;#8217;d write down some of my&amp;nbsp;thoughts.&lt;/p&gt;
&lt;p&gt;First and foremost, I understand that technology changes very quickly - a lot quicker than
college syllabi. However, my alma mater&amp;#8217;s &lt;a href="https://www.cs.rutgers.edu/undergraduate/courses/"&gt;undergrad course list&lt;/a&gt;
does not appear to have changed &lt;em&gt;at all&lt;/em&gt; since I took some of those classes a decade ago.
Don&amp;#8217;t get me wrong, it&amp;#8217;s very good that we&amp;#8217;re teaching classes in Operating Systems Design
and Compilers. But current curricula seem to focus almost exclusively on fundamentals
and low-level details. When I was in college, almost all the classes were taught in Java,
and that already seemed dated and ignoring the &amp;#8220;Web 2.0&amp;#8221; world; these days, it relegates
most graduates to jobs that I&amp;#8217;d consider boring, in the Enterprise sector. Rutgers&amp;#8217; undergrad
&lt;span class="caps"&gt;CS&lt;/span&gt; catalog doesn&amp;#8217;t list &amp;#8220;Internet Technology&amp;#8221; until &lt;span class="caps"&gt;CS&lt;/span&gt; 352, and I&amp;#8217;d be surprised if any
networking (aside from maybe &amp;#8220;use raw sockets to do something&amp;#8221;) is covered before that class,
which is tentatively designed for third-year students. Very few of the courses that I see
listed (and many that I looked at haven&amp;#8217;t changed their description, or even their instructor,
in ten years) do much to prepare students for the actual tech industry. Even worse, most
of them are the same classes that I found boring, and caused me to switch&amp;nbsp;majors.&lt;/p&gt;
&lt;p&gt;Obviously, I can understand the argument that tech moves too fast for class materials
to keep pace. But introducing the Internet as a third-year topic, and Distributed
Systems as a senior-level class? Writing &lt;em&gt;everything&lt;/em&gt; in Java? This seemed silly
to me a decade ago; now, it seems downright wrong for a department that claims to
prepare students for careers in technology. Once again, I agree that a strong grasp
of fundamentals is overwhelmingly important. However, a balance needs to be struck
between this and (1) providing students with useful, current skills, and (2) keeping
students interested. In a world where more and more (and in many areas, such as my own
subculture, almost all) software runs in the browser, how can education ignore&amp;nbsp;this?&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll take a small moment to dovetail this to a heated topic in tech at the moment:
hiring practices and diversity. The tech education at most universities does little
to prepare students for actual jobs outside of, quite literally, &amp;#8220;entry-level Java
programmer&amp;#8221;. The people who will get good, interesting jobs in tech (yes, that&amp;#8217;s quite
opinionated; I&amp;#8217;m defining that as jobs with web or DevOps shops) are the ones who
either have ample free time and pre-existing interest to hack on their own projects,
and/or are insomniacs and can handle going to class, working, and still writing
a lot of code and experimenting on their own. I suppose this ends up being biased
towards stereotypical white males, even if only because it seems socially acceptable
for us to ignore having a social life in favor of finishing those last lines of&amp;nbsp;code.&lt;/p&gt;
&lt;p&gt;But I digress. There are few other industries that I can think of - and certainly
no professions - where someone leaves the University system with the highest
degree commonly attained in their field, yet has virtually zero real world
hands-on experience. I&amp;#8217;ll leave out the doctor or lawyer analogies, but some
of our closest parallels - engineers, architects, etc. - graduate and are able
to start practicing their profession. Sure, there&amp;#8217;s organization- or domain-specific
on the job training, but for the most part, they can do what they were hired to do.
On the other hand, tech-focused programs are turning out graduates many of whom
have never seen the tools (or even languages) that we use. They don&amp;#8217;t have
any real experience working in teams larger than three or four (at the best),
and have no concept of what goes into developing real software, working in
a large (or distributed) team, or what happens to software after someone
grades it (which in my experience, was usually as simple as &amp;#8220;does this program
produce the right output when it&amp;#8217;s run at the command&amp;nbsp;line).&lt;/p&gt;
&lt;p&gt;So, that&amp;#8217;s my rant. What do I think should be covered in tech/&lt;span class="caps"&gt;CS&lt;/span&gt; curricula that isn&amp;#8217;t?
Here are a&amp;nbsp;few:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Testing&lt;/strong&gt; - I have painful memories of being marked as failing assignments because
I used a tab when the instructors expected some number of spaces in the output of my
program. In most cases, my instructors used a test harness that exec&amp;#8217;ed our Java applications
and inspected the string output. I still have no idea why they didn&amp;#8217;t use JUnit. But that&amp;#8217;s
beyond the point; we&amp;#8217;re turning out programmers who don&amp;#8217;t know what unit or integration tests
are. I still don&amp;#8217;t understand how I got through a four-year degree, partially in &lt;span class="caps"&gt;CS&lt;/span&gt; and
partially in &lt;span class="caps"&gt;IT&lt;/span&gt;, without writing a single test for any program I wrote (aside from a few
courses where we wrote &amp;#8220;test harnesses&amp;#8221;, but never used a proper testing&amp;nbsp;framework).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Software Distribution&lt;/strong&gt; - Here&amp;#8217;s another no-brainer. Why - especially when working in
Java - would students email completed assignments to professors, or upload them to online
courseware, when so many artifact repositories exist? If people can&amp;#8217;t use your software it&amp;#8217;s
pretty pointless. Distribution should be a part of at least some assignments, whether it&amp;#8217;s
an open source model or just uploading an artifact to an internal maven&amp;nbsp;repository.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Maintenance&lt;/strong&gt; - Ok, sure, this is the part that none of us really like. But it&amp;#8217;s also
an inherent part of what we do, and the odds are most entry-level programmers will first
find themselves fixing bugs or adding features to someone else&amp;#8217;s application. Being able
to read and understand existing code is perhaps the most important thing we do, whether it&amp;#8217;s
to fix it or just to learn from it. Nobody I&amp;#8217;ve talked to about education as a programmer
has ever encountered an assignment of &amp;#8220;here&amp;#8217;s an application, here&amp;#8217;s a bug report, find and
fix it&amp;#8221; beyond the most trivial contrived examples. The process of fixing a bug in or adding
a feature to an existing codebase is probably one of the most important lessons in learning
to write code - even if it&amp;#8217;s partially a lesson in what not to&amp;nbsp;do.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Distributed Projects&lt;/strong&gt; - &lt;em&gt;The&lt;/em&gt; &amp;#8220;big&amp;#8221; project in my time as a student was pairing on a Java
&lt;span class="caps"&gt;GUI&lt;/span&gt;/backend program. Perhaps I didn&amp;#8217;t have the best experience, as my partner neglected to write
any code. However, few people are going to enter the workforce and be the sole person touching
a given codebase. I think there should be much more emphasis on working as part of a realistically-sized
team. Sure, the tooling might not be the same, but at least graduates should have some experience
in collaborating with others, and more importantly, in working on code where they don&amp;#8217;t necessarily
understand all of it. If we&amp;#8217;re going to teach Java, we should at least be teaching it realistically
and throwing in some black-box classes or having students code to each others&amp;#8217; (not-yet-complete)&amp;nbsp;APIs.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Web-First&lt;/strong&gt; - With the plummeting cost of cloud computing and containers, and the massive
compute farms available at most universities, there&amp;#8217;s really no excuse for completely ignoring
the Internet. Sure, it doesn&amp;#8217;t play much of a role in the low-level basics, but for the classic
&amp;#8220;hello world&amp;#8221;, calculator app, tic-tack-toe, etc. it&amp;#8217;s not really that much overhead to do
them in &lt;a href="http://spring.io/guides/gs/rest-service/"&gt;Spring&lt;/a&gt; or &lt;a href="http://flask.pocoo.org/"&gt;Flask&lt;/a&gt;
or &lt;a href="http://guides.rubyonrails.org/getting_started.html"&gt;Rails&lt;/a&gt; and also give students exposure
to a modern, web-centric framework that someone might actually use to write a simple application.
Most programmers are probably going to touch the web at some point. I&amp;#8217;d argue that it&amp;#8217;s also a
lot more applicable for people who aren&amp;#8217;t going into a distinctly programming&amp;nbsp;role.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Operable Software&lt;/strong&gt; - Going with the web-first paradigm, we have the virtualization or
container technology to give students a shell and a running web server. Why not show them
how to use it? I once failed a major assignment because the graders used a script that combined
&lt;span class="caps"&gt;STDOUT&lt;/span&gt; and &lt;span class="caps"&gt;STDERR&lt;/span&gt; and evaluated it. They asked me what all these weird lines in my output
were; I told them it was log4j. They asked what that was. Even the classes I took that
included working in teams or something else somewhat realistic, completely ignored the
operations side of software, as far as never discussing logging. If we want the quality
of software that we (as an industry) turn out to increase, one of the best things we can
do is introduce programmers to logging, testing, debugging and the operations side as early
as possible, even if in a quick, superficial way. If students had to actually &lt;em&gt;run&lt;/em&gt; their
app, and let it serve actual requests, they&amp;#8217;d have a lot clearer picture of what software
actually does after the build a &lt;span class="caps"&gt;JAR&lt;/span&gt;. Related to this, an introduction to the concepts
of security and stability would be quite&amp;nbsp;useful.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you&amp;#8217;re currently going to school for something programming-related, here are a few
things that I&amp;#8217;d recommend doing to get ahead of the&amp;nbsp;pack:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Learn some languages. Look through job ads for the type of entry-level/graduate work you
hope to get when you graduate, and see what they&amp;#8217;re asking for. If your school is still the
way mine appears to be, in a &lt;span class="caps"&gt;CS&lt;/span&gt; program you&amp;#8217;ll probably be exposed to Java and C. Learn some
Ruby or Python, or something else that&amp;#8217;s in use on the web. It certainly won&amp;#8217;t hurt you, and
it&amp;#8217;ll also be less intimidating to learn a new language once you already know a few, preferably
that are rather&amp;nbsp;different.&lt;/li&gt;
&lt;li&gt;If you want to work in the web world and are a Windows person, learn Linux. Despite what
some people tell you, it&amp;#8217;s the rule not the&amp;nbsp;exception.&lt;/li&gt;
&lt;li&gt;Read. I understand that not everyone has extensive time to experiment on their own, and I know
a lot of people who didn&amp;#8217;t, especially in college. Read. A lot. It seems that most tech programs
require a lot less work than other engineering disciplines; I remember how envious my &lt;span class="caps"&gt;ECE&lt;/span&gt; and Mech-E
friends were at the &amp;#8220;low&amp;#8221; amount of work we &lt;span class="caps"&gt;CS&lt;/span&gt;/&lt;span class="caps"&gt;IT&lt;/span&gt; majors had to do. Read everything you can, especially
about the industry you want to work in (if you have an idea of what it is). Find out what tools they&amp;#8217;re
using from job ads or company tech blogs, and find out about them. Even if you can&amp;#8217;t use them
yourself, at least knowing a bit about them will help a&amp;nbsp;lot.&lt;/li&gt;
&lt;li&gt;If you can, find an open source project or two to work on. This one comes with a bit of a warning;
the open source world can be quite abrasive, and sometimes downright hurtful. Unfortunately, technology
as a whole seems to attract a lot of very loud, angry, bad people. So do some research; try to find
a project that you&amp;#8217;re interested in and that you have some relevant experience for. But most importantly,
find a project that&amp;#8217;s clearly open to mentoring new contributors; they&amp;#8217;re unfortunately few and
far between, but it will really pay&amp;nbsp;off.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Mon, 05 Oct 2015 16:46:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2015-10-05:2015/10/a-rant-on-post-secondary-tech-education/</guid><category>college</category><category>computer science</category><category>CS</category><category>technology</category><category>education</category></item><item><title>Puppetlabs Beaker SUTs with GUI /Â Non-Headless</title><link>http://blog.jasonantman.com/2015/09/puppetlabs-beaker-suts-with-gui--non-headless/</link><description>&lt;p&gt;&lt;a href="https://github.com/puppetlabs/beaker/"&gt;Beaker&lt;/a&gt; is a puppetlabs tool for automating acceptance testing
of puppet modules; in most common use cases, it uses a &lt;a href="https://www.vagrantup.com/"&gt;Vagrant&lt;/a&gt;/
&lt;a href="https://www.virtualbox.org/"&gt;virtualbox&lt;/a&gt; &lt;span class="caps"&gt;VM&lt;/span&gt; to run the&amp;nbsp;tests.&lt;/p&gt;
&lt;p&gt;This week, I was writing tests for a &lt;a href="https://github.com/jantman/puppet-archlinux-workstation"&gt;module&lt;/a&gt;
that configures my desktop and laptop, including installing and setting up Xorg and &lt;span class="caps"&gt;KDE&lt;/span&gt; and the
&lt;a href="https://github.com/sddm"&gt;&lt;span class="caps"&gt;SDDM&lt;/span&gt;&lt;/a&gt; display manager. I wanted to be able to test that they not only
got installed, but actually ran without dieing - which required a graphincal environment (ideally,
I&amp;#8217;d visually confirm this as&amp;nbsp;well).&lt;/p&gt;
&lt;p&gt;To do this in Vagrant, you&amp;#8217;d just add a &lt;code&gt;gui = true&lt;/code&gt; option to the
&lt;a href="https://docs.vagrantup.com/v2/virtualbox/configuration.html"&gt;virtualbox provider&lt;/a&gt; in your&amp;nbsp;Vagrantfile.&lt;/p&gt;
&lt;p&gt;It isn&amp;#8217;t documented anywhere, but I &lt;a href="https://github.com/jantman/puppet-archlinux-workstation/commit/6ca19a24853681c468eba38735c8d2d7f54cd616"&gt;found&lt;/a&gt;
that Beaker has support for this as well; all you need to do is add &lt;code&gt;vb_gui: true&lt;/code&gt; in your node definition&amp;nbsp;&lt;span class="caps"&gt;YAML&lt;/span&gt;.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="gd"&gt;--- before.yaml 2015-09-19 11:20:47.772523116 -0400&lt;/span&gt;
&lt;span class="gi"&gt;+++ after.yaml  2015-09-19 11:20:20.768867546 -0400&lt;/span&gt;
&lt;span class="gu"&gt;@@ -1,11 +1,12 @@&lt;/span&gt;
 &lt;span class="caps"&gt;HOSTS&lt;/span&gt;:
   arch-x64:
     roles:
       - master
     platform: archlinux-2015.09.01-amd64
     box: jantman/packer-arch-workstation
     hypervisor: vagrant
&lt;span class="gi"&gt;+    vb_gui: true&lt;/span&gt;

 &lt;span class="caps"&gt;CONFIG&lt;/span&gt;:
   log_level: verbose
   type: foss
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Once that&amp;#8217;s done, the VirtualBox &lt;span class="caps"&gt;VM&lt;/span&gt; will run with a graphical display enabled. This is probably only useful on a local
machine or if you&amp;#8217;re running on a remote host have you have access to and have &lt;a href="https://www.virtualbox.org/manual/ch07.html"&gt;vrdp&lt;/a&gt;
enabled, but in some edge cases like my module, it&amp;#8217;s&amp;nbsp;useful.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 19 Sep 2015 11:09:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2015-09-19:2015/09/puppetlabs-beaker-suts-with-gui--non-headless/</guid><category>puppet</category><category>beaker</category><category>sut</category><category>rspec</category><category>virtualbox</category><category>headless</category><category>GUI</category></item><item><title>AwsLimitChecker - Check Your AWS Usage Against ServiceÂ Limits</title><link>http://blog.jasonantman.com/2015/07/awslimitchecker-check-your-aws-usage-against-service-limits/</link><description>&lt;p&gt;Over the past year or so, at my day job, we&amp;#8217;ve been leveraging &lt;span class="caps"&gt;AWS&lt;/span&gt; more and more, specifically
&lt;a href="https://aws.amazon.com/cloudformation/"&gt;CloudFormation&lt;/a&gt; to manage complete application stacks. One
side-effect of this is that we went through a few periods where we were constantly hitting various
&lt;span class="caps"&gt;AWS&lt;/span&gt; Service Limits - subnet groups, ElastiCache clusters, security groups, and a whole slew of others.
In pretty much all these cases, we weren&amp;#8217;t &lt;em&gt;really&lt;/em&gt; aware of the limits; we (the Tooling and
Automation team) had succeeded in our goal of handing our internal customers the tools to spin up
complete application environments, per-developer, on-demand. And it was wonderful until we hit some
magic number of CloudFormation stacks, at which point almost every day for a week or two we had to
open a new &lt;span class="caps"&gt;AWS&lt;/span&gt; Support ticket to have a different limit increased, and deal with completely broken
deploys until that was done (or send out a frantic &amp;#8220;someone please delete a dev stack&amp;#8221;&amp;nbsp;email).&lt;/p&gt;
&lt;p&gt;Early last month we decided that we had to do something about this. As much as I tried, I couldn&amp;#8217;t
find an existing solution that would monitor our limits and alert us when we approached them; there
were some open source scripts that would do so for a handful of limits (generally just &lt;span class="caps"&gt;EC2&lt;/span&gt; usage),
and the proprietary solutions that I was able to find didn&amp;#8217;t seem much better; none of them stated
that they handle &lt;span class="caps"&gt;VPC&lt;/span&gt; or ElastiCache limits, which had been problematic for us. &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217;s own
&lt;a href="https://aws.amazon.com/premiumsupport/trustedadvisor/"&gt;Trusted Advisor&lt;/a&gt; has a Service Limits
check available to Business- and Enterprise-level support accounts, but it only monitors 17 of the
94 Service Limits that we identified as relevant to us, and it sends out &lt;em&gt;weekly&lt;/em&gt; alerts. So,
I decided to write something to solve the problem. My co-workers and I have been trying to get
corporate legal approval to release our work publicly under an &lt;span class="caps"&gt;OSI&lt;/span&gt;-approved license for years,
to no avail. I asked my team if they&amp;#8217;d support waiting a while for this work, so I could do it
entirely in my own time, publicly, under an open source license. Happily, they&amp;nbsp;agreed.&lt;/p&gt;
&lt;p&gt;Today I&amp;#8217;m making the first release of &lt;a href="https://github.com/jantman/awslimitchecker"&gt;awslimitchecker&lt;/a&gt;,
an &lt;span class="caps"&gt;AGPL&lt;/span&gt; 3.0-licensed Python tool to calculate your &lt;span class="caps"&gt;AWS&lt;/span&gt; resource usage for various services bound by
&lt;a href="http://awslimitchecker.readthedocs.org/en/latest/limits.html#current-checks"&gt;service limits&lt;/a&gt;, and tell you which ones exceed a given threshold (actually, warning and critical
thresholds). Effective limits are hard-coded to the &lt;a href="http://docs.aws.amazon.com/general/latest/gr/aws_service_limits.html"&gt;published defaults&lt;/a&gt;,
but can be overridden in cases where you&amp;#8217;ve received limit increases, and will be automatically updated
from Trusted Advisor data for all limits that it monitors (if your account includes the full &lt;span class="caps"&gt;TA&lt;/span&gt; checks).
awslimitchecker provides warning and critical thresholds that can be set globally as a percentage of the
limit (defaults are 80% and 99%, respectively) or overridden on a per-limit basis, as either a percentage
or a fixed integer usage&amp;nbsp;value.&lt;/p&gt;
&lt;p&gt;awslimitchecker is available &lt;a href="https://pypi.python.org/pypi/awslimitchecker/0.1.0"&gt;from pypi&lt;/a&gt;.
It is compatible and tested with Python versions 2.6 through 3.4, though the library it uses to communicate
with &lt;span class="caps"&gt;AWS&lt;/span&gt;, &lt;a href="http://boto.readthedocs.org/en/latest/"&gt;boto&lt;/a&gt;, still has a few &lt;span class="caps"&gt;AWS&lt;/span&gt; services which are not python3-compatible.
awslimitchecker includes both a Python module with a &lt;a href="http://awslimitchecker.readthedocs.org/en/latest/awslimitchecker.checker.html"&gt;documented &lt;span class="caps"&gt;API&lt;/span&gt;&lt;/a&gt; for those who
don&amp;#8217;t mind working with Python, and a command line script for those who&amp;nbsp;do.&lt;/p&gt;
&lt;p&gt;The project is still very young, and only being used by one organization, but it&amp;#8217;s proven
stable for us, and I&amp;#8217;m more than happy to accept questions, comments, criticisms,
&lt;a href="https://github.com/jantman/awslimitchecker/issues"&gt;issues/feature requests&lt;/a&gt; and Pull&amp;nbsp;Requests.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 25 Jul 2015 08:35:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2015-07-25:2015/07/awslimitchecker-check-your-aws-usage-against-service-limits/</guid><category>aws</category><category>ec2</category><category>limits</category><category>python</category><category>awslimitchecker</category><category>cloud</category></item><item><title>Visualization of when Iâm working on personal vs workÂ projects</title><link>http://blog.jasonantman.com/2015/06/visualization-of-when-im-working-on-personal-vs-work-projects/</link><description>&lt;p&gt;I was thinking the other day - as I was pushing out some final code reviews for work at &lt;span class="caps"&gt;11PM&lt;/span&gt; before taking a day off -
about how much work I do outside of &amp;#8220;work hours&amp;#8221;. And the answer is, I don&amp;#8217;t really know, especially when it comes to
projects that I really enjoy and find interesting. So, I decided to have some fun with &lt;a href="https://github.com/gitpython-developers/GitPython"&gt;GitPython&lt;/a&gt;
and find&amp;nbsp;out.&lt;/p&gt;
&lt;p&gt;The result of this was &lt;a href="https://github.com/jantman/misc-scripts/blob/master/whendoiwork.py"&gt;whendoiwork.py&lt;/a&gt;. It&amp;#8217;s a pretty simple script,
and also makes some pretty big assumptions, but I found the results interesting. Given some local directories which contain git clones
of my work repositories, and some which contain clones of my personal repos, it iterates over all* of the commits in them by me
(going by the git author name) in the last N days (default 365); it counts commits to personal repositories as +1 and to work
repositories as -1, and adds them to buckets per hour of day, per day of week. It then uses &lt;a href="http://matplotlib.org/"&gt;matplotlib&lt;/a&gt;
to build a heatmap, with the maximum commits per hour for work repos in blue and the maximum per hour for personal in&amp;nbsp;red.&lt;/p&gt;
&lt;p&gt;I can&amp;#8217;t vouch that it&amp;#8217;s 100% accurate, but the results were interesting to me; while it seems like I tend to do a fair amount
of work in the evenings, compared to work on personal projects, all of my work for my employer is well contained in my normal
7-3 work&amp;nbsp;day.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s an example of the output of this script, for my own work, run&amp;nbsp;with:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;./whendoiwork.py -v -a /home/jantman/GIT -b /home/jantman/work/git -b /home/jantman/work/git/ops -d 365 -t &amp;#39;US/Eastern&amp;#39; --repoAlabel personal --repoBlabel work
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that this iterates over every commit in all of the git repos it finds, possibly multiple times. On my own
system (9G of git repos with a few hundred thousand commits), this took about 2&amp;nbsp;minutes.&lt;/p&gt;
&lt;p&gt;&lt;img alt="heatmap of days of week and hours of day when I commit to work vs personal repos" src="https://raw.githubusercontent.com/jantman/misc-scripts/master/whendoiwork.png" /&gt;&lt;/p&gt;
&lt;p&gt;If you find any bugs/issues with it, please pass them along by &lt;a href="https://github.com/jantman/misc-scripts/issues"&gt;opening an issue&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Fri, 05 Jun 2015 21:20:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2015-06-05:2015/06/visualization-of-when-im-working-on-personal-vs-work-projects/</guid><category>git</category><category>commits</category><category>python</category><category>graphs</category><category>visualization</category></item><item><title>Local S3 Server to Acceptance Test Netflix Ice Installation InÂ Isolation</title><link>http://blog.jasonantman.com/2015/05/local-s3-server-to-acceptance-test-netflix-ice-installation-in-isolation/</link><description>&lt;p&gt;At work, we recently started using &lt;a href="http://netflix.github.io/"&gt;Netflix &lt;span class="caps"&gt;OSS&lt;/span&gt;&lt;/a&gt;&amp;#8216;s &lt;a href="https://github.com/Netflix/ice"&gt;Ice&lt;/a&gt; &lt;span class="caps"&gt;AWS&lt;/span&gt; cost analysis tool.
It provides a Java daemon to read and parse &lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217; &lt;a href="http://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/detailed-billing-reports.html"&gt;detailed billing reports&lt;/a&gt;
and a web interface to the data (&lt;a href="https://github.com/Netflix/ice/blob/master/README.md#screenshots"&gt;screenshots&lt;/a&gt;). The single biggest feature for us
is the ability to do cost breakdowns (by hour/day/week/month) based on Cost Allocation tags in the detailed billing reports. We tag every billable &lt;span class="caps"&gt;AWS&lt;/span&gt;
resource with the Application Name, Service Class (environment; dev/test/prod) and Responsible Party. Ice lets us configure &amp;#8220;Application Groups&amp;#8221;
based on applications as seen from a business/budgetary standpoint and allow up-to-the-hour data on that available to anyone who needs&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;We spun up the development install of Ice for a few weeks to give it a spin, but once people started complaining that my screen session died and took
Ice with it, it was clear we needed a real, permanent installation. While there is &lt;a href="https://github.com/mdsol/ice_cookbook"&gt;chef&lt;/a&gt; and &lt;a href="https://github.com/Answers4AWS/netflixoss-ansible"&gt;ansible&lt;/a&gt;
code to install and configure Ice, we&amp;#8217;re a Puppet shop, and there wasn&amp;#8217;t anything available that I could find for Puppet. So, I set about writing a
module to install and configure Ice, running in Tomcat behind an Nginx proxy. Like any good modern module, I wanted not only &lt;a href="http://rspec-puppet.com/"&gt;rspec-puppet&lt;/a&gt;
unit tests but also &lt;a href="https://github.com/puppetlabs/beaker"&gt;beaker&lt;/a&gt; acceptance tests. For those unfamiliar, Beaker is an acceptance testing framework for Puppet
that&amp;#8217;s similar to Test Kitchen; it spins up Vagrant machines, runs some code in them, and then uses &lt;a href="http://serverspec.org/"&gt;serverspec&lt;/a&gt; to make assertions about
the state of the system (file contents, running processes, command output, etc.) (side note: if you used Beaker prior to the
&lt;a href="https://github.com/puppetlabs/beaker/blob/master/HISTORY.md#beaker2.0.0"&gt;2.0 release&lt;/a&gt; in December 2014, you should really try it again; they&amp;#8217;ve made some great&amp;nbsp;improvements).&lt;/p&gt;
&lt;h2 id="the-problem"&gt;The&amp;nbsp;Problem&lt;/h2&gt;
&lt;p&gt;This posed a bit of a challenge, as Ice (in addition to being pretty poorly documented) is really designed to run in &lt;span class="caps"&gt;AWS&lt;/span&gt;. Firstly, the very reason we started running Ice was
to get a handle on our fast-growing &lt;span class="caps"&gt;AWS&lt;/span&gt; spend; as a result, we&amp;#8217;re trying hard not to use &lt;span class="caps"&gt;AWS&lt;/span&gt; for small-scale projects that could use existing resources. Second, while our
company very unfortunately doesn&amp;#8217;t have an open source policy and isn&amp;#8217;t releasing anything (hopefully this may be changing soon), we try hard to write generic, forge-quality&amp;nbsp;modules.&lt;/p&gt;
&lt;p&gt;As a result, I wanted to use the default Vagrant/VirtualBox provider for Beaker. To make matters worse, in keeping with the spirit of a community module, I didn&amp;#8217;t
want the acceptance tests to require anything specific to my company, such as an S3 bucket preseeded with our billing data. Ice both reads the detailed billing reports
(one of its three inputs; &lt;span class="caps"&gt;EC2&lt;/span&gt; pricing data and your accounts&amp;#8217; reservation pricing/capacity being the others) and writes state from and to S3. So, this was a bit difficult.
As we don&amp;#8217;t plan on upgrading Ice terribly often, and we wanted to install from the &lt;a href="https://netflixoss.ci.cloudbees.com/job/ice-master/"&gt;cloudbees master builds&lt;/a&gt;, we wanted
acceptance testing of not just the provisioning tooling, but also some basic smoke tests for the application&amp;nbsp;itself.&lt;/p&gt;
&lt;h2 id="the-solution"&gt;The&amp;nbsp;Solution&lt;/h2&gt;
&lt;p&gt;I managed to come up with a working, albeit somewhat Rube Goldberg, method of getting isolated acceptance tests to work. What follows is the gist of how I got Ice
working in complete isolation. The majority of this happens in &lt;code&gt;spec/acceptance/0prerequisite_spec.rb&lt;/code&gt; which runs first and both does the prerequisite setup
and validates that everything is setup right and working for the tests. The following solution is based on the amazingly helpful &lt;a href="https://github.com/jubos/fake-s3"&gt;fakes3&lt;/a&gt;
Ruby gem, the &lt;a href="http://www.apsis.ch/pound/"&gt;Pound&lt;/a&gt; reverse proxy, and some &lt;span class="caps"&gt;SSL&lt;/span&gt; certificate trickery. While my code was specific to Beaker, this should be generic
enough to use with any system acceptance testing&amp;nbsp;tool.&lt;/p&gt;
&lt;h2 id="prerequisites"&gt;Prerequisites&lt;/h2&gt;
&lt;p&gt;First, we obtain or create some files that we&amp;#8217;ll need on the test&amp;nbsp;instance:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Grab a relatively recent Detailed Billing With Resources and Tags zipped &lt;span class="caps"&gt;CSV&lt;/span&gt; report from an &lt;span class="caps"&gt;AWS&lt;/span&gt; account of yours (the filename is in the format
    &lt;code&gt;&amp;lt;ACCOUNT NUMBER&amp;gt;-aws-billing-detailed-line-items-with-resources-and-tags-&amp;lt;YYYY&amp;gt;-&amp;lt;MM&amp;gt;.csv&lt;/code&gt;). Manually trim it down to a sufficient sample of data;
    I took a few hours&amp;#8217; worth of data from one day and trimmed it down to just that referencing a few randomly chosen &lt;span class="caps"&gt;RDS&lt;/span&gt; instances, ELBs, on-demand &lt;span class="caps"&gt;EC2&lt;/span&gt;
    instances and reserved &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. I then anonymized the account number, resource IDs, tag values, and anything else identifying. Ice needs billing
    data in order to do anything, so this will serve as our test&amp;nbsp;data.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;When Ice runs, it attempts to retrieve reserved instance pricing. It appears (I&amp;#8217;ve lost the mailing list or GitHub issue reference) that it&amp;#8217;s typical for
    the first Ice run on an empty S3 work directory to die because these files are missing. As a result, grab the &lt;code&gt;reservation_prices.oneyear.*&lt;/code&gt; files from
    the S3 work bucket of a running/working Ice installation. This will prevent a time-consuming shutdown of Ice on the first&amp;nbsp;run.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Generate a self-signed &lt;span class="caps"&gt;SSL&lt;/span&gt; key and certificate for &lt;code&gt;fakebucket.s3.amazonaws.com&lt;/code&gt;. Package them together in a &lt;span class="caps"&gt;PEM&lt;/span&gt; file suitable for use in web servers.
    (Note that most modern S3 &lt;span class="caps"&gt;API&lt;/span&gt; clients accept a full &lt;span class="caps"&gt;URL&lt;/span&gt; to a bucket, as there are now third parties that implement the S3 &lt;span class="caps"&gt;API&lt;/span&gt;. Ice does not; it connects
    to https://&lt;span class="caps"&gt;BUCKETNAME&lt;/span&gt;.s3amazonaws.com. As a result, this &lt;span class="caps"&gt;SSL&lt;/span&gt; foolery is&amp;nbsp;required.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="setup"&gt;Setup&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;a href="https://rubygems.org/gems/fakes3"&gt;fakes3&lt;/a&gt; rubygem; this provides an s3-compliant &lt;span class="caps"&gt;API&lt;/span&gt; backed by local filesystem storage.
    Configure it to run during your tests (I set it up as a systemd service, but there are certainly other ways to do this). Note that
    while fakes3 stores the uploaded data on the local filesystem, it maintains a mapping of known objects in memory; as such, the process
    always starts completely empty, regardless of what&amp;#8217;s in the backing directory on the filesystem. fakes3 allows all &lt;span class="caps"&gt;IAM&lt;/span&gt; credentials,
    so fake ones are fine. It also automatically creates buckets the first time they&amp;#8217;re&amp;nbsp;accessed.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install the &lt;a href="http://www.apsis.ch/pound/"&gt;pound&lt;/a&gt; reverse proxy and configure it to listen on port 443 with the &lt;span class="caps"&gt;PEM&lt;/span&gt; file you generated
    earlier, and proxy to fakes3 (which listens by default on port 10000). The &lt;code&gt;ListenHTTPS&lt;/code&gt;section of &lt;code&gt;pound.cfg&lt;/code&gt; will need the
    &lt;code&gt;xHTTP 1&lt;/code&gt; directive in order to enable &lt;span class="caps"&gt;HTTP&lt;/span&gt; verbs other than&amp;nbsp;&lt;span class="caps"&gt;GET&lt;/span&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Setup a local hosts file entry pointing &lt;code&gt;fakebucket.s3.amazonaws.com&lt;/code&gt; at &lt;code&gt;127.0.0.1&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;After fakes3 starts, upload your sample billing data file and your reserved instance pricing files to the appropriate paths under a
    bucket called &amp;#8220;fakebucket&amp;#8221;. You can use a tool such as &lt;a href="http://s3tools.org/s3cmd"&gt;s3cmd&lt;/a&gt; to manipulate its contents, and other
    supported tools are listed in &lt;a href="https://github.com/jubos/fake-s3/wiki/Supported-Clients"&gt;the documentation&lt;/a&gt;. This step also serves
    to validate your Pound configuration, which should pass &lt;span class="caps"&gt;HTTPS&lt;/span&gt; port 443 traffic through to fakes3 and allow you to store and
    retrieve&amp;nbsp;objects.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Figure out the path to the trusted keystore for the version of Java that you&amp;#8217;re running Ice under. On CentOS 7 with OpenJDK 1.7.0,
    this was (after a lot of symlinks) &lt;code&gt;/usr/lib/jvm/jre/lib/security/cacerts&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Import your self-signed certificate into the Java keystore as a trusted certificate. This will allow &lt;span class="caps"&gt;SSL&lt;/span&gt; verification to succeed even
    with a self-signed&amp;nbsp;certificate:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;/bin/keytool -importcert -alias fakebucket -file fakebucket.s3.amazonaws.com.crt -keystore /usr/lib/jvm/jre/lib/security/cacerts -storepass changeit -trustcacerts -noprompt
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Configure &lt;code&gt;ice.properties&lt;/code&gt; for the above. The important and unintuitive parts that I found&amp;nbsp;are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Going by the above examples, your billing and work S3 bucket names should both be&amp;nbsp;&amp;#8220;fakebucket&amp;#8221;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Unless you want to mock out bigger parts of the &lt;span class="caps"&gt;AWS&lt;/span&gt; metadata service, run Ice with
   &lt;code&gt;-Dice.s3AccessKeyId=NotAValidAccessKeyId -Dice.s3SecretKey=NotAValidAwsSecretKeyXxxxxxxxxxxxxxxxxxx&lt;/code&gt;
   in the &lt;code&gt;JAVA_OPTS&lt;/code&gt;. If Ice can&amp;#8217;t retrieve an instance&amp;#8217;s &lt;span class="caps"&gt;IAM&lt;/span&gt; role from the metadata service
   (http://169.254.169.254/latest/meta-data/iam/security-credentials/) and doesn&amp;#8217;t have the
   access and secret keys defined, it won&amp;#8217;t run. Also note that while the documentation is &lt;strong&gt;very&lt;/strong&gt;
   unclear on this, a number of &lt;a href="https://github.com/Netflix/ice/issues/49#issuecomment-23497701"&gt;github issues&lt;/a&gt;
   clarify that these need to be passed in as Java runtime options; they can&amp;#8217;t be put in the properties&amp;nbsp;file.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Disable the Reservation Capacity Poller (&lt;code&gt;ice.reservationCapacityPoller=false&lt;/code&gt;). This service
   needs to connect to the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt;, and will cause Ice to die if it&amp;nbsp;can&amp;#8217;t.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For testing purposes, it&amp;#8217;s a lot simpler and less error-prone (as well as being a lot faster) to
   test the processor and reader separately - at least in serial instead of simultaneously in the same&amp;nbsp;instance.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Once all this is done, running the Ice Processor should retrieve the billing file, process it, and write the processed data to the
fakes3 bucket. Running the Reader should display the data properly. So far I&amp;#8217;ve been unable to find any features (other than the
Reservation Capacity Poller, noted above) that don&amp;#8217;t work with this&amp;nbsp;setup.&lt;/p&gt;
&lt;p&gt;Whether it&amp;#8217;s related to Ice itself or ideas for acceptance testing isolated applications, I hope this can be of use to&amp;nbsp;someone&amp;#8230;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Tue, 05 May 2015 06:45:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2015-05-05:2015/05/local-s3-server-to-acceptance-test-netflix-ice-installation-in-isolation/</guid><category>netflix</category><category>ice</category><category>puppet</category><category>beaker</category><category>acceptance testing</category><category>aws</category><category>s3</category><category>fakes3</category><category>testing</category></item><item><title>Jira to TrelloÂ Script</title><link>http://blog.jasonantman.com/2015/04/jira-to-trello-script/</link><description>&lt;p&gt;A few weeks ago, I &lt;a href="/2015/03/my-new-found-love-of-trello-and-a-helpful-greasemonkey-script/"&gt;posted about&lt;/a&gt; how I&amp;#8217;ve
started using &lt;a href="https://trello.com/"&gt;Trello&lt;/a&gt; to keep track of my work and keep things flowing smoothly. It&amp;#8217;s been
absolutely wonderful, and I feel more productive and less stressed, and like I have a better idea of what&amp;#8217;s coming
up. The one thing that Trello is missing is time tracking. I don&amp;#8217;t need anything fancy, all I really wanted was to
be able to show a time estimate on my cards. I know I could&amp;#8217;ve just put the estimate right in the title of the cards,
but that seemed like a waste of&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;At the moment we&amp;#8217;re using Jira at work, so I wrote &lt;a href="https://github.com/jantman/misc-scripts/blob/master/jira2trello.py"&gt;jira2trello.py&lt;/a&gt;.
It&amp;#8217;s a pretty simple Python script that uses the &lt;a href="https://pypi.python.org/pypi/trello"&gt;trello&lt;/a&gt; and
&lt;a href="https://pypi.python.org/pypi/jira"&gt;jira&lt;/a&gt; packages from pypi to iterate over all cards on a specified Trello
board, and for each card that matches a configurable regular expression for ticket keys (i.e.
&lt;code&gt;.*((project1|project2|project3)-\d+):.*&lt;/code&gt;), the script&amp;nbsp;will:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Determine if the Jira issue is a subtask, and if so, prefix its title with the issue key of the parent issue,
using the format of &lt;code&gt;PARENT-xxx -&amp;gt; CHILD-xxx&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;Look up the &amp;#8220;Original Estimate&amp;#8221; time tracking field in Jira, and if present, prepend it to the title of
the&amp;nbsp;card.&lt;/li&gt;
&lt;li&gt;Regenerate the title of the card, using the current issue summary from&amp;nbsp;Jira.&lt;/li&gt;
&lt;li&gt;Move the card to a specified &amp;#8220;Done&amp;#8221; list if it&amp;#8217;s closed in&amp;nbsp;Jira.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are a few assumptions in the script about how the titles of cards are formed, namely that they follow the
&lt;code&gt;ISSUE-xxx: Summary Here&lt;/code&gt; format used by my &lt;a href="https://github.com/jantman/userscripts#trellocontextmenu"&gt;TrelloContextMenu&lt;/a&gt;
Firefox userscript. But I hope that this might be of use to someone else as well. Please feel free to open issues
or submit pull requests for any improvements that would be helpful, including any assumptions I&amp;#8217;ve made that aren&amp;#8217;t
valid in your environment. The source can be found on GitHub: &lt;a href="https://github.com/jantman/misc-scripts/blob/master/jira2trello.py"&gt;jira2trello.py&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Fri, 10 Apr 2015 05:58:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2015-04-10:2015/04/jira-to-trello-script/</guid><category>jira</category><category>trello</category><category>python</category><category>ticket</category><category>kanban</category></item><item><title>My New-found Love of Trello and a Helpful GreaseMonkeyÂ Script</title><link>http://blog.jasonantman.com/2015/03/my-new-found-love-of-trello-and-a-helpful-greasemonkey-script/</link><description>&lt;p&gt;When I started at my current job two and a half years ago, we were just really getting into &lt;a href="http://en.wikipedia.org/wiki/Kanban_%28development%29"&gt;Kanban&lt;/a&gt;.
We used a web-based Kanban board (&lt;a href="https://github.com/cmheisel/kardboard"&gt;Kardboard&lt;/a&gt;, written by our director of development at the time), and each team had
their daily stand-up status meetings in the middle of the office in front of a projector screen, with their board filling the entire wall (and remotes on Skype). It took me a while
to get used to it, having always worked in very ticket-and-emergency-driven ops roles. But once it clicked, it was like an epiphany. Suddenly I could see all
of the (non-breakfix or unforeseen) work headed to our team, its priority and due dates, and what everyone (including myself) was working on at a given moment.
Even though work-in-progress (&lt;span class="caps"&gt;WIP&lt;/span&gt;) limits never made it to the ops team, it reduced my stress level amazingly. Instead of dealing with a massive queue of tickets
assigned to me - some of which were such low priority I&amp;#8217;d probably never get around to them - suddenly all I had to concern myself with was my &lt;span class="caps"&gt;WIP&lt;/span&gt; and what was
next up for&amp;nbsp;me.&lt;/p&gt;
&lt;p&gt;Over the past year we&amp;#8217;ve had a major (almost complete) changeover of management, and many of the old ways have disappeared. Some might even say that, as a technical
company, we&amp;#8217;ve regressed quite a bit. Either way, we haven&amp;#8217;t been using Kanban for over six months. Our development teams are moving to &lt;a href="http://en.wikipedia.org/wiki/Scrum_%28software_development%29"&gt;Scrum&lt;/a&gt;,
and our more ops-y teams (I&amp;#8217;m now on our Automation and Tooling team, straddling the awkward line between the two) are trying to figure out what&amp;#8217;s right for us.
And I haven&amp;#8217;t been this stressed since I started work here; my team is both busier than ever and understaffed by almost 50%. We stopped using the Kanban board in our
stand-ups, our new manager stopped referring to it, so (even with mostly-working synchronization with Jira) it stopped being useful, and I stopped using it. Without thinking,
I went back to my old &amp;#8220;page showing &lt;em&gt;everything&lt;/em&gt; assigned to me&amp;#8221; view in our ticketing system, and grew increasingly frustrated by managing my &lt;span class="caps"&gt;WIP&lt;/span&gt; and deciding what
needed to be worked&amp;nbsp;next.&lt;/p&gt;
&lt;p&gt;So, after discussing this with the rest of my team, last week two of us came to the same conclusion, independently, on the same day: use &lt;a href="https://trello.com/"&gt;Trello&lt;/a&gt; to
run our own personal Kanban boards. I&amp;#8217;ve been doing so for about a week now, and all I&amp;#8217;m horribly embarrassed that I didn&amp;#8217;t think of this sooner. It&amp;#8217;s absolutely wonderful -
I can keep managing my own work in a Kanban-like form (albeit without formal &lt;span class="caps"&gt;WIP&lt;/span&gt; limits) without needing management endorsement. Sure, it only works for things that I know
are coming and takes some manual curation time, but so far, it&amp;#8217;s been amazingly refreshing and calming. The best part is being able to (once again) visually see
both my &lt;span class="caps"&gt;WIP&lt;/span&gt;, and my recently-complete&amp;nbsp;work.&lt;/p&gt;
&lt;p&gt;Someone else on my team mentioned that they use Trello for their personal tasks; I created two more boards, one for my personal development work, and another for my general
around-the-house tasks and to-do&amp;#8217;s (my wife connected with her inner manager once I shared my board with her and she figured out that she could re-order by backlog&amp;#8230;).
It&amp;#8217;s 8 &lt;span class="caps"&gt;PM&lt;/span&gt; on Sunday night, and I can confidently say that I&amp;#8217;ve had one of the most productive weekends in ages. And one of the most relaxing. Instead of spending
lots of time trying to figure out what I have to do this weekend and what the priorities are, I just used the same Kanban method that I loved from work. And it
paid&amp;nbsp;off.&lt;/p&gt;
&lt;h2 id="the-greasemonkey-script"&gt;The GreaseMonkey&amp;nbsp;Script&lt;/h2&gt;
&lt;p&gt;The one thing that initially bothered me about Trello was the time it took to add cards. At work every task I have is in either our ticketing system or
a GitHub Issue. Our previous official tool, &lt;a href="https://github.com/cmheisel/kardboard"&gt;Kardboard&lt;/a&gt;, synchronized with Jira so everything was always
up-to-date and on the right board. At first I was adding cards manually, but I figured there had to be a better way. A quick Google search turned up
a &lt;a href="https://github.com/danlec/Trello-Bookmarklet"&gt;Bookmarklet&lt;/a&gt; by &lt;a href="https://github.com/danlec"&gt;Daniel LeCheminant&lt;/a&gt; of Trello, to add a Trello card for the current
page. It does some really cool stuff, like parsing Jira and GitHub issues and setting the card title nicely for them, as well as some other ticketing
systems. I also found a &lt;a href="https://gist.github.com/aggieben/5811685"&gt;GreaseMonkey script&lt;/a&gt; from &lt;a href="https://github.com/aggieben"&gt;Benjamin Collins&lt;/a&gt; of StackExchange
that adds a link to create Trello cards from StackExchange meta&amp;nbsp;posts.&lt;/p&gt;
&lt;p&gt;So, I took things a bit further and whipped up a GreaseMonkey script, &lt;a href="https://github.com/jantman/userscripts#trellocontextmenu"&gt;TrelloContextMenu&lt;/a&gt;. It
uses Daniel&amp;#8217;s card naming code (plus fixing the GitHub format a bit and adding support for &lt;a href="https://www.reviewboard.org/"&gt;ReviewBoard&lt;/a&gt; code reviews) and
the GreaseMonkey/Trello logic from Benjamin&amp;#8217;s script. Once installed and authenticated with Trello, the script retrieves a list of all of your boards
and cards, and adds an &amp;#8220;Add to Trello&amp;#8221; right-click context menu in Firefox, allowing you to add the current page to any list on any of your&amp;nbsp;boards.&lt;/p&gt;
&lt;p&gt;&lt;a href="/GFX/TrelloContextMenu_large.png"&gt;&lt;img alt="screenshot of TrelloContextMenu context menu popup in Firefox" src="/GFX/TrelloContextMenu_sm.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The script is &lt;a href="https://github.com/jantman/userscripts#trellocontextmenu"&gt;available on GitHub&lt;/a&gt; (the link in the &lt;span class="caps"&gt;README&lt;/span&gt; will go to an installable raw
version of the script), and uses GreaseMonkey&amp;#8217;s versioning capabilities. At the moment I&amp;#8217;ve only tested it with GreaseMonkey in Firefox, and I don&amp;#8217;t
expect it to work elsewhere as it uses a few GreaseMonkey-specific features, such as &lt;code&gt;GM_xmlhttpRequest&lt;/code&gt; and GreaseMoneky&amp;#8217;s browser-wide SQLite
persistent storage (to store your boards and lists, and authentication credentials, until you manually refresh). I&amp;#8217;d be happy to accept pull requests
from anyone who can get it working in other&amp;nbsp;browsers.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sun, 22 Mar 2015 19:36:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2015-03-22:2015/03/my-new-found-love-of-trello-and-a-helpful-greasemonkey-script/</guid><category>Trello</category><category>kanban</category><category>tickets</category><category>organization</category><category>work</category></item><item><title>Some Additional ServerspecÂ Types</title><link>http://blog.jasonantman.com/2015/03/some-additional-serverspec-types/</link><description>&lt;p&gt;&lt;a href="http://serverspec.org/"&gt;Serverspec&lt;/a&gt; is an rspec-based framework for testing live machines,
and making assertions about things like the output of commands, installed packages, running
services, file content, etc. However, it has a relatively limited and basic set of
&lt;a href="http://serverspec.org/resource_types.html"&gt;Resource Types&lt;/a&gt; that it can test&amp;nbsp;for.&lt;/p&gt;
&lt;p&gt;Before Serverspec completely disabled their GitHub issue tracker (they now seem to have no
issue tracker at all), I&amp;#8217;d suggested some improvements for more advanced resource types,
such as one that can perform an &lt;span class="caps"&gt;HTTP&lt;/span&gt; &lt;span class="caps"&gt;GET&lt;/span&gt; against an application and check the status code
and/or output. I was told in no uncertain terms that this is a task for application integration
testing, and that it&amp;#8217;s &amp;#8220;not what Serverspec is&amp;nbsp;for.&amp;#8221;&lt;/p&gt;
&lt;p&gt;I humbly disagree. I&amp;#8217;ve begun migrating my &lt;a href="https://www.linode.com/"&gt;Linode&lt;/a&gt; to an &lt;span class="caps"&gt;EC2&lt;/span&gt; machine,
using some technology that I&amp;#8217;ve been using at my day job; specifically, Puppet to configure the
machine and &lt;a href="https://packer.io/"&gt;Packer&lt;/a&gt; to build an &lt;span class="caps"&gt;AMI&lt;/span&gt;. Instead of using &lt;a href="http://aws.amazon.com/cloudformation/"&gt;Cloudformation&lt;/a&gt;
to spin up an entire stack, I just use a Rakefile to spin up a new &lt;span class="caps"&gt;EC2&lt;/span&gt; instance, test it, and
swap an &lt;a href="http://docs.aws.amazon.com/AWSEC2/latest/UserGuide/elastic-ip-addresses-eip.html"&gt;Elastic &lt;span class="caps"&gt;IP&lt;/span&gt;&lt;/a&gt;
if all the tests pass. Of course, this requires that I have relatively complete automated testing
of the &lt;span class="caps"&gt;EC2&lt;/span&gt; instance. Stock Serverspec can handle 95% of what I want to test, but there are a few
other, more complex, things that it can&amp;#8217;t. So, I wrote some code to fix&amp;nbsp;that.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll admit right off the bat that this code doesn&amp;#8217;t really work the way Serverspec is intended to,
but it works and it&amp;#8217;s relatively simple. This largely breaks the abstraction of serverspec using
&lt;a href="https://github.com/serverspec/specinfra"&gt;specinfra&lt;/a&gt; under the hood, but I&amp;#8217;m not sure if that&amp;#8217;s even
a concern (since specinfra seems to be all about testing a running machine via some local command
execution mechanism, and two of the types that I wrote use network &lt;span class="caps"&gt;IO&lt;/span&gt;&amp;nbsp;instead).&lt;/p&gt;
&lt;p&gt;For the time being, I&amp;#8217;ve written three additional &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#Types"&gt;types&lt;/a&gt;
that solve some specific use cases for&amp;nbsp;me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#bitlbee"&gt;bitlbee&lt;/a&gt;
type that connects to a &lt;a href="http://www.bitlbee.org/"&gt;Bitlbee&lt;/a&gt; &lt;span class="caps"&gt;IRC&lt;/span&gt; gateway, authenticates,
and checks the running bitlbee version. It has matchers to check whether or not the connection and
authentication was successful, whether or not it timed out, and the bitlbee version. Parameters for
the type include login nick and password, bitlbee port, and whether or not to connect with&amp;nbsp;&lt;span class="caps"&gt;SSL&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;A &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#http_get"&gt;http_get&lt;/a&gt;
type which connects to the system under test (with a specified port) and issues a
&lt;span class="caps"&gt;HTTP&lt;/span&gt; &lt;span class="caps"&gt;GET&lt;/span&gt; request for a specified path, with a specified &lt;code&gt;Host&lt;/code&gt; header and a timeout (default
10 seconds). Matchers are provided for the response content body (string), response headers
(hash), &lt;span class="caps"&gt;HTTP&lt;/span&gt; status code, and whether or not the request timed out (which also sets a status of&amp;nbsp;0).&lt;/li&gt;
&lt;li&gt;A &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/#virtualenv"&gt;virtualenv&lt;/a&gt; type for testing
python &lt;a href="https://virtualenv.pypa.io/en/latest/"&gt;virtualenv&lt;/a&gt;s. It takes the absolute path to the venv
on the filesystem, and uses serverspec&amp;#8217;s built-in file and command execution features to ensure that
the path &amp;#8220;looks like&amp;#8221; a virtualenv, and has matchers for the pip and python versions used in the venv
as well as the &lt;code&gt;pip freeze&lt;/code&gt; output as a hash of requirements and their&amp;nbsp;versions.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hopefully this will be of use to someone else as well. As I continue using serverspec, I plan on
adding to the&amp;nbsp;types.&lt;/p&gt;
&lt;p&gt;The code for serverspec-extended-types is on &lt;a href="https://github.com/jantman/serverspec-extended-types/tree/master"&gt;GitHub&lt;/a&gt;
(pull requests and issues welcome) and it&amp;#8217;s packaged and hosted as a &lt;a href="https://rubygems.org/gems/serverspec-extended-types"&gt;ruby gem&lt;/a&gt;.
&lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/0.0.2#Installation"&gt;Installation&lt;/a&gt; and usage is as simple
as adding it to your Gemfile and &lt;a href="http://www.rubydoc.info/gems/serverspec-extended-types/0.0.2#Usage"&gt;spec_helper&lt;/a&gt;
and then using the types and matchers in your&amp;nbsp;specs.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 14 Mar 2015 11:58:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2015-03-14:2015/03/some-additional-serverspec-types/</guid><category>serverspec</category><category>specinfra</category><category>testing</category><category>beaker</category><category>ruby</category><category>rspec</category><category>gem</category></item><item><title>RSpec Matcher For Hash ItemÂ Value</title><link>http://blog.jasonantman.com/2015/02/rspec-matcher-for-hash-item-value/</link><description>&lt;p&gt;&lt;strong&gt;Update:&lt;/strong&gt; Well, this is embarassing. &lt;em&gt;After&lt;/em&gt; I posted this, I received a
&lt;a href="http://blog.jasonantman.com/2015/02/rspec-matcher-for-hash-item-value/#comment-1868422853"&gt;comment&lt;/a&gt;
within a few hours from &lt;a href="https://twitter.com/myronmarston"&gt;@myronmarston&lt;/a&gt;. I&amp;#8217;d originally
written this matcher for RSpec2, and then had to convert my project to use
RSpec3. I just blindly converted this matcher over. Myron pointed out that with
RSpec3&amp;#8217;s &lt;a href="http://rspec.info/blog/2014/01/new-in-rspec-3-composable-matchers/"&gt;composable matchers&lt;/a&gt;,
the functionality of this gem is built-in. It can be done as simply&amp;nbsp;as:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;its&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:headers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="n"&gt;should&lt;/span&gt; &lt;span class="kp"&gt;include&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;server&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="sr"&gt;/nginx\/1\./&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;As such, I&amp;#8217;ve yanked them gem and am leaving the code and blog post here just for posterity.&lt;/strong&gt;
This should probably not be&amp;nbsp;used.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve been working on a project to move my &lt;a href="http://linode.com"&gt;Linode&lt;/a&gt; &lt;span class="caps"&gt;VM&lt;/span&gt; to an
Amazon &lt;span class="caps"&gt;EC2&lt;/span&gt; instance; the entire instance is a &amp;#8220;baked&amp;#8221; &lt;span class="caps"&gt;AMI&lt;/span&gt; built by Puppet. Since
I&amp;#8217;d like to be able to rebuild this quickly, I&amp;#8217;m using &lt;a href="http://serverspec.org/"&gt;ServerSpec&lt;/a&gt;
(which I have some non-technical issues with, but that&amp;#8217;s a long story) to run full
integration tests of the whole system - check that packages are installed, services
are running, and even make live &lt;span class="caps"&gt;HTTP&lt;/span&gt; requests agsinst&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;One part of this was making live &lt;span class="caps"&gt;HTTP&lt;/span&gt; requests (from inside ServerSpec / &lt;a href="http://rspec.info/"&gt;rspec&lt;/a&gt;)
and checking &lt;span class="caps"&gt;HTTP&lt;/span&gt; response headers. Unfortunately, RSpec doesn&amp;#8217;t have a nice, clean way to make
assertions about a hash&amp;nbsp;item.&lt;/p&gt;
&lt;p&gt;So, I wrote a little Ruby Gem to do this, &lt;a href="https://github.com/jantman/rspec-matcher-hash-item"&gt;rspec-matcher-hash-item&lt;/a&gt;. At the moment it just
has one matcher, &lt;code&gt;have_hash_item_matching&lt;/code&gt;. This operates on a hash, and takes two arguments,
a key and a regex for the value. It allows me to do simple but useful things&amp;nbsp;like:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;  &lt;span class="n"&gt;describe&lt;/span&gt; &lt;span class="n"&gt;http_get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;80&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;testapp1.jasonantman.com&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/testapp1234&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;do&lt;/span&gt;
    &lt;span class="n"&gt;its&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="ss"&gt;:headers&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="n"&gt;should&lt;/span&gt; &lt;span class="n"&gt;have_hash_item_matching&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;server&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="sr"&gt;/nginx\/1\./&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;end&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(The &lt;code&gt;http_get&lt;/code&gt; serverspec matcher is coming in a future gem and blog&amp;nbsp;post)&lt;/p&gt;
&lt;p&gt;Among other things, it prints diffs on&amp;nbsp;failure:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;  2) privatepuppet::ec2::vhosts::testapp1 Http_get &amp;quot;&amp;quot; headers should include key &amp;#39;server&amp;#39; matching /badvalue/
     On host `54.149.198.147&amp;#39;
     Failure/Error: its(:headers) { should have_hash_item_matching(&amp;#39;server&amp;#39;, /badvalue/) }
       expected that hash[server] would match /badvalue/
       Diff:
       @@ -1,2 +1,6 @@
       -[&amp;quot;server&amp;quot;, /badvalue/]
       +&amp;quot;connection&amp;quot; =&amp;gt; &amp;quot;close&amp;quot;,
       +&amp;quot;content-type&amp;quot; =&amp;gt; &amp;quot;text/plain&amp;quot;,
       +&amp;quot;date&amp;quot; =&amp;gt; &amp;quot;Sat, 21 Feb 2015 16:07:42 GMT&amp;quot;,
       +&amp;quot;server&amp;quot; =&amp;gt; &amp;quot;nginx/1.6.2&amp;quot;,
       +&amp;quot;transfer-encoding&amp;quot; =&amp;gt; &amp;quot;chunked&amp;quot;,
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Using the gem is as simple as including it in your &lt;code&gt;Gemfile&lt;/code&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;gem &amp;quot;rspec-matcher-hash-item&amp;quot;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;And adding a line to your &lt;code&gt;spec_helper.rb&lt;/code&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;require &amp;#39;rspec_matcher_hash_item&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Note that the gem is written for&amp;nbsp;RSpec3.&lt;/p&gt;
&lt;p&gt;This is available at &lt;a href="https://rubygems.org/gems/rspec-matcher-hash-item"&gt;rubygems.org&lt;/a&gt; or from
&lt;a href="https://github.com/jantman/rspec-matcher-hash-item"&gt;GitHub&lt;/a&gt;. See GitHub for the&amp;nbsp;documentation.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 21 Feb 2015 10:33:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2015-02-21:2015/02/rspec-matcher-for-hash-item-value/</guid><category>ruby</category><category>rspec</category><category>spec</category><category>testing</category></item><item><title>AWS CloudFormation and RDSÂ Snapshots</title><link>http://blog.jasonantman.com/2014/12/aws-cloudformation-and-rds-snapshots/</link><description>&lt;p&gt;For the past few weeks, I&amp;#8217;ve been working on spinning up a WordPress stack on Amazon &lt;span class="caps"&gt;AWS&lt;/span&gt;. It&amp;#8217;s intended to be a production application,
so it uses Multi-&lt;span class="caps"&gt;AZ&lt;/span&gt; and a few other tricks to try to achieve relatively high fault tolerance (nothing insane, still in one region). It uses
&lt;span class="caps"&gt;AWS&lt;/span&gt;&amp;#8217;s &lt;a href="https://aws.amazon.com/rds/"&gt;&lt;span class="caps"&gt;RDS&lt;/span&gt;&lt;/a&gt; hosted MySQL service for the database, and the stacks are created with &lt;a href="https://aws.amazon.com/cloudformation/"&gt;CloudFormation&lt;/a&gt;.
Using CloudFormation has been an utterly wonderful experience and being able to spin up an entire stack - multiple autoscaling web server
instances, a database, memcache, etc. with the click of a button in ~20 minutes - is as close to operations nirvana as I&amp;#8217;ve ever&amp;nbsp;gotten.&lt;/p&gt;
&lt;p&gt;One of the last steps for me was to work on database backups and restoration; both restoring the production application&amp;#8217;s database to a
previous snapshot, and restoring a production database snapshot to a test or development stack. This took a few days of testing, and I
wasn&amp;#8217;t able to find much complete information on the nuances of it; there are also some pieces that are not intuitive and (&lt;span class="caps"&gt;IMO&lt;/span&gt;) not
documented well enough in the &lt;span class="caps"&gt;AWS&lt;/span&gt; docs. In short, it&amp;#8217;s horribly easy to blow away your entire database. So, I&amp;#8217;m going to attempt to document
some of what I learned, in the hope that it will benefit&amp;nbsp;others.&lt;/p&gt;
&lt;p&gt;At the bottom of this post I&amp;#8217;ve included some snippets from my CloudFormation template, which I make reference to. It&amp;#8217;s probably worth looking
through that, as I make reference to some of the names used in it. Also, to make sense of this, you should be familiar with the nomenclature used by CloudFormation,
such as the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/template-anatomy.html"&gt;template anatomy&lt;/a&gt; and the difference between
parameters and properties, and resources and&amp;nbsp;instances.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; I&amp;#8217;m writing this in mid-December 2014. I&amp;#8217;ll make every effort to keep this updated as I continue working with &lt;span class="caps"&gt;AWS&lt;/span&gt;, but it&amp;#8217;s possible
that some of the problems described herein will be fixed by &lt;span class="caps"&gt;AWS&lt;/span&gt; in the&amp;nbsp;future.&lt;/p&gt;
&lt;h2 id="deletionpolicy-snapshot"&gt;DeletionPolicy&amp;nbsp;Snapshot&lt;/h2&gt;
&lt;p&gt;CloudFormation resources support a &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-deletionpolicy.html"&gt;DeletionPolicy&lt;/a&gt;
attribute that says what to do to a resource when deleted. For &lt;span class="caps"&gt;RDS&lt;/span&gt; instances, &amp;#8220;Snapshot&amp;#8221; is an option, which takes a manual snapshot when the resource
is deleted (manual snapshots, unlike the automated daily ones, live on even after the instance is deleted). Be warned, this only takes effect when you
delete the &lt;strong&gt;entire stack&lt;/strong&gt;. If you make a change to one of the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html"&gt;DBInstance properties&lt;/a&gt;
that requires a &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks.html#update-replacement"&gt;resource replacement&lt;/a&gt; to
take effect, the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance will be replaced with a new one, and all of the data and automatic snapshots from the old one will be deleted.
That last part deserves repeating: automatic snapshots (the daily ones created by &lt;span class="caps"&gt;RDS&lt;/span&gt;) are tied to the instance; if the instance is replaced
by CloudFormation, you lose all automatic (backup) snapshots with&amp;nbsp;it.&lt;/p&gt;
&lt;h2 id="stack-policy-to-prevent-updates"&gt;Stack Policy to Prevent&amp;nbsp;Updates&lt;/h2&gt;
&lt;p&gt;To prevent &lt;span class="caps"&gt;RDS&lt;/span&gt; data loss from accidentally changing a property of the instance, it&amp;#8217;s wise to add a
&lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html"&gt;stack policy to prevent updates to &lt;span class="caps"&gt;RDS&lt;/span&gt; resources&lt;/a&gt;.
This will prevent CloudFormation from making any changes to the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance at all. Once the stack policy
is in place, in order to make changes to the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance you would either need to set a temporary stack policy
to allow the update (see the &amp;#8220;Updating Protected Resources&amp;#8221; section of the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/protect-stack-resources.html"&gt;stack policy documentation&lt;/a&gt;)
or simply delete and re-create the stack (the recommended method, if it&amp;#8217;s feasible for&amp;nbsp;you).&lt;/p&gt;
&lt;p&gt;Setting a proper stack policy should prevent many of the pitfalls I describe below; however, for completeness,
I&amp;#8217;ve described how &lt;span class="caps"&gt;RDS&lt;/span&gt; resources behave currently without a stack policy protecting them. The
&lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-rds-database-instance.html"&gt;&lt;span class="caps"&gt;AWS&lt;/span&gt;::&lt;span class="caps"&gt;RDS&lt;/span&gt;::DBInstance resource documentation&lt;/a&gt;
describes which properties can be updated in-place (&amp;#8220;Update requires: No interruption&amp;#8221; or &amp;#8220;some interruptions&amp;#8221;)
and which trigger complete replacement of the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance (&amp;#8220;Update requires:&amp;nbsp;replacement&amp;#8221;).&lt;/p&gt;
&lt;p&gt;When you try to update a protected resource through the &lt;code&gt;aws&lt;/code&gt; &lt;span class="caps"&gt;CLI&lt;/span&gt; tools, the update will appear to have worked, but the event
log on the stack will show the update denied and the update will be rolled&amp;nbsp;back.&lt;/p&gt;
&lt;h2 id="restoring-snapshots-and-dbname"&gt;Restoring Snapshots and&amp;nbsp;DBName&lt;/h2&gt;
&lt;p&gt;The DBSnapshotIdentifier property on a MySQL &lt;span class="caps"&gt;RDS&lt;/span&gt; instance specifies a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot to restore into the instance. The DBName
property will create a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with a single blank database of that name. This bears repeating again; if the DBName
property ever changes, your &lt;span class="caps"&gt;RDS&lt;/span&gt; instance will be replaced with one with a new, blank database of that name.
When creating a MySQL &lt;span class="caps"&gt;RDS&lt;/span&gt; instance, you can specify either the &lt;code&gt;DBName&lt;/code&gt; or &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; property, but not both;
if you attempt to specify both, you&amp;#8217;ll get an error, &amp;#8220;DBName must be null when Restoring for this&amp;nbsp;Engine.&amp;#8221;&lt;/p&gt;
&lt;p&gt;If you want to restore a snapshot to a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance, you&amp;#8217;ll need to ensure that &lt;code&gt;DBName&lt;/code&gt; is null (either not specified at all, or the special &lt;code&gt;AWS::NoValue&lt;/code&gt;
&lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/pseudo-parameter-reference.html"&gt;pseudo parameter&lt;/a&gt;). In order
to do this automatically (and since NoValue/null can&amp;#8217;t be passed in as a template parameter), in the template snippet below I&amp;#8217;ve defined a
&lt;code&gt;UseDbSnapshot&lt;/code&gt; &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/conditions-section-structure.html"&gt;condition&lt;/a&gt;
that evaluates to true if the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter is not empty. In my &lt;code&gt;RDS::DBInstance&lt;/code&gt; resource,
I conditionally set (using &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/intrinsic-function-reference-conditions.html#d0e42982"&gt;&lt;code&gt;Fn::If&lt;/code&gt;&lt;/a&gt;)
the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; and &lt;code&gt;DBName&lt;/code&gt; properties depending on the value of &lt;code&gt;UseDbSnapshot&lt;/code&gt;. The end result is that if the
&lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter is not empty, it is passed in as the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; property of the resource and
the &lt;code&gt;DBName&lt;/code&gt; property is set to &lt;code&gt;AWS::NoValue&lt;/code&gt;. Otherwise, the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; property is set to &lt;code&gt;AWS::NoValue&lt;/code&gt;
and the &lt;code&gt;DBName&lt;/code&gt; parameter is passed in to the corresponding property on the resource (indicating to create a new blank database
of that&amp;nbsp;name).&lt;/p&gt;
&lt;p&gt;To explain this a bit more, CloudFormation seems to have no introspection into &lt;span class="caps"&gt;RDS&lt;/span&gt; instances. The &lt;code&gt;DBName&lt;/code&gt; parameter
exists only in CloudFormation itself, and is only evaluated as a diff from the previous template; if it changes,
CloudFormation spins up a completely new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with a single blank database of that name. Whether or not
the value of &lt;code&gt;DBName&lt;/code&gt; matches the database currently in the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance (say, restored from a snapshot)
is not known by CloudFormation. In short, if you create an &lt;span class="caps"&gt;RDS&lt;/span&gt; instance from a snapshot of a &amp;#8220;foo&amp;#8221; database
and then change the template to have a &lt;code&gt;DBName&lt;/code&gt; of &amp;#8220;foo&amp;#8221;, CloudFormation will spin up a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance
with an empty &amp;#8220;foo&amp;#8221;&amp;nbsp;database.&lt;/p&gt;
&lt;h2 id="restoring-to-a-new-stack"&gt;Restoring to a New&amp;nbsp;Stack&lt;/h2&gt;
&lt;p&gt;When restoring to a new stack (stack creation), specify the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; and make sure &lt;code&gt;DBName&lt;/code&gt; is set
to &lt;code&gt;AWS::NoValue&lt;/code&gt; per the previous paragraph (condition in the template). Note that for the life of the stack, you
must continue specifying these parameters (or the &amp;#8220;use previous value&amp;#8221; option for them). Using my example template
below, if you restored into a new stack using the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter and then later updated the stack
and omitted that parameter (which, because of the condition, would set it to &lt;code&gt;NoValue&lt;/code&gt; and set the &lt;code&gt;DBName&lt;/code&gt; parameter
to its default value) the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance would be replaced with a new one with a blank&amp;nbsp;database.&lt;/p&gt;
&lt;p&gt;Because of this, stack updates should always use the previous value for the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter; this can
be done through the &lt;span class="caps"&gt;AWS&lt;/span&gt; Console, or using the &lt;code&gt;aws&lt;/code&gt; command line tools and a parameter like: &lt;code&gt;--parameters ParameterKey=DBSnapshotIdentifier,UsePreviousValue=true&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="restoring-to-an-existing-stack"&gt;Restoring to an Existing&amp;nbsp;Stack&lt;/h2&gt;
&lt;p&gt;Restoring a snapshot to an existing stack is a bit more nuanced. You can&amp;#8217;t restore a snapshot to an existing &lt;span class="caps"&gt;RDS&lt;/span&gt; instance,
you can only restore to a new instance. If you do this through the &lt;span class="caps"&gt;AWS&lt;/span&gt; Console, you&amp;#8217;ll end up with an &lt;span class="caps"&gt;RDS&lt;/span&gt; instance disconnected
from your CloudFormation stack. So the way to do this is more or less the same as restoring to a new stack - specify
the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter for your template, and it will create a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with the snapshot. The same
rules about using previous values for the parameters hold true. If you used a stack policy to prevent updates to the &lt;span class="caps"&gt;RDS&lt;/span&gt;
instance, you&amp;#8217;ll need to override that with a temporary policy when doing the&amp;nbsp;restore.&lt;/p&gt;
&lt;p&gt;There are a few caveats to keep in mind with this procedure. The first, obviously, is that there may be some application downtime
when the existing database is replaced with the new (restored) one, and any writes will obviously be lost. Also, this only
works on &lt;span class="caps"&gt;RDS&lt;/span&gt; instances that were created with DBName or a &lt;strong&gt;different&lt;/strong&gt; snapshot. In order to restore the same snapshot to
an &lt;span class="caps"&gt;RDS&lt;/span&gt; resource a second time, you need to first update with the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; parameter removed and have the &lt;span class="caps"&gt;RDS&lt;/span&gt;
instance re-created with an empty database, and then update again with the &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt; in order to do the restore.
This is because CloudFormation doesn&amp;#8217;t reconcile the current state of instances to determine which actions to take, it only diffs
the updated template against the existing one. If the existing template and the updated one have the same value for the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance&amp;#8217;s
properties (specifically &lt;code&gt;DBSnapshotIdentifier&lt;/code&gt;), CloudFormation determines there are no changes, and does&amp;nbsp;nothing.&lt;/p&gt;
&lt;h2 id="launchconfig-metadata-issues"&gt;LaunchConfig Metadata&amp;nbsp;Issues&lt;/h2&gt;
&lt;p&gt;The &lt;span class="caps"&gt;EC2&lt;/span&gt; instances I&amp;#8217;m using for this project are &amp;#8220;baked&amp;#8221; AMIs (built with &lt;a href="https://packer.io/"&gt;packer.io&lt;/a&gt;) in an Auto-Scaling Group (&lt;span class="caps"&gt;ASG&lt;/span&gt;).
They use a &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-properties-as-launchconfig.html"&gt;LaunchConfig&lt;/a&gt; to write
out a file on disk with the database connection information for the application. In addition, my &lt;span class="caps"&gt;ASG&lt;/span&gt; has an &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attribute-updatepolicy.html"&gt;UpdatePolicy&lt;/a&gt;
designed to perform rolling updates (termination and replacement) of &lt;span class="caps"&gt;EC2&lt;/span&gt; instances when their properties&amp;nbsp;change.&lt;/p&gt;
&lt;p&gt;In my testing, I noticed a number of times where updates to the &lt;span class="caps"&gt;RDS&lt;/span&gt; resource that triggered creation of a new &lt;span class="caps"&gt;RDS&lt;/span&gt; instance - such as restoring from
a snapshot in an existing stack, or changing the DBName - properly triggered an update of the LaunchConfig, but failed to trigger
the rolling update of the &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. This left the application in a state where one or more (sometimes all) of the &lt;span class="caps"&gt;EC2&lt;/span&gt;
instances couldn&amp;#8217;t connect to the database, because the file written out by the LaunchConfig still contained the old &lt;span class="caps"&gt;DB&lt;/span&gt; connection
information. For non-production stacks where the entire stack can be deleted and recreated instead of updating the &lt;span class="caps"&gt;RDS&lt;/span&gt; resource,
this shouldn&amp;#8217;t be an issue. Otherwise, if changes are made that replace the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance, I&amp;#8217;d recommend watching for the
LaunchConfig update completion, and manually terminating instances (or increasing the size of the &lt;span class="caps"&gt;ASG&lt;/span&gt; to add instances)
to ensure that the running &lt;span class="caps"&gt;EC2&lt;/span&gt; instances have the updated&amp;nbsp;LaunchConfig.&lt;/p&gt;
&lt;p&gt;Another option would be to use the &lt;a href="http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/cfn-hup.html"&gt;cfn-hup daemon&lt;/a&gt; to
listen for stack updates that cause changes in resource metadata, and perform the required actions without needing the rolling update
to replace the&amp;nbsp;instances.&lt;/p&gt;
&lt;h2 id="how-to-do-things-using-the-template-below"&gt;How to Do Things Using the Template&amp;nbsp;Below&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;m currently using the &lt;code&gt;aws&lt;/code&gt; command line tools to perform stack creation and updates,
wrapped in a Rakefile (I plan on changing this to use &lt;a href="https://github.com/boto/boto"&gt;boto&lt;/a&gt;
inside a &lt;a href="http://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; job). What follows is a quick high-level guide
on how to accomplish various &lt;span class="caps"&gt;RDS&lt;/span&gt;-related tasks, using the template snippet&amp;nbsp;below.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Build a new stack using a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot and a stack policy to prevent updates&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;cat /tmp/stack_policy.json
&lt;span class="o"&gt;{&lt;/span&gt;
  &lt;span class="s2"&gt;&amp;quot;Statement&amp;quot;&lt;/span&gt; : &lt;span class="o"&gt;[&lt;/span&gt;
    &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;Effect&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Deny&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Update:*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Principal&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Resource&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;LogicalResourceId/DBInstance&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;,
    &lt;span class="o"&gt;{&lt;/span&gt;
      &lt;span class="s2"&gt;&amp;quot;Effect&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Allow&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;Update:*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Principal&amp;quot;&lt;/span&gt;: &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;,
      &lt;span class="s2"&gt;&amp;quot;Resource&amp;quot;&lt;/span&gt; : &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;
    &lt;span class="o"&gt;}&lt;/span&gt;
  &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;aws cloudformation create-stack --stack-name mystack --stack-policy-body file:///tmp/stack_policy.json --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;my-snapshot-identifier&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Temporarily override stack policy to allow updates&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Create a file with the following contents (we&amp;#8217;ll assume it&amp;#8217;s at &lt;code&gt;/home/myuser/allow_all_updates.json&lt;/code&gt;):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Statement&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Effect&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Allow&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Action&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Update:*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Principal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Resource&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;*&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;In the following &lt;code&gt;aws&lt;/code&gt; commands, append &lt;code&gt;--stack-policy-during-update-body file:///home/myuser/allow_all_updates.json&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Update a stack (built using a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot), without losing data&lt;/strong&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,UsePreviousValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nb"&gt;true&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Load a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot into an existing stack&lt;/strong&gt; (that isn&amp;#8217;t already using this&amp;nbsp;snapshot):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;my-snapshot-identifier&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Load a &lt;span class="caps"&gt;RDS&lt;/span&gt; snapshot into an existing stack again&lt;/strong&gt; (i.e. restore from the same snapshot a second time; this one is a&amp;nbsp;kludge):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="c"&gt;# re-create the &lt;span class="caps"&gt;RDS&lt;/span&gt; instance with a blank &lt;span class="caps"&gt;DB&lt;/span&gt; (DBName)&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;&lt;span class="c"&gt;# then load the snapshot again&lt;/span&gt;
&lt;span class="nv"&gt;$ &lt;/span&gt;aws cloudformation update-stack --stack-name mystack --template-body file:///home/myuser/cloudformation_template.json --parameters &lt;span class="nv"&gt;ParameterKey&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;DBSnapshotIdentifier,ParameterValue&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;my-snapshot-identifier&amp;#39;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="cloudformation-template-snippet"&gt;CloudFormation Template&amp;nbsp;Snippet&lt;/h2&gt;
&lt;p&gt;This is by no means complete, but just includes the parameters, conditions, and resources which I make reference&amp;nbsp;to.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nt"&gt;&amp;quot;Parameters&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Default&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;wordpress&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Description&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;The WordPress database name&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;String&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;MinLength&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;MaxLength&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;64&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;AllowedPattern&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;[a-zA-Z][a-zA-Z0-9]*&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;ConstraintDescription&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;must begin with a letter and contain only alphanumeric characters.&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Description&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot; The &lt;span class="caps"&gt;RDS&lt;/span&gt; MySQL snapshot name to restore to the new &lt;span class="caps"&gt;DB&lt;/span&gt; instance.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;String&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Default&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;quot;Conditions&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;UseDbSnapshot&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Fn::Not&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;Fn::Equals&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
          &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
          &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;]&lt;/span&gt;
      &lt;span class="p"&gt;}]&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;},&lt;/span&gt;

  &lt;span class="nt"&gt;&amp;quot;Resources&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;DBInstance&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::&lt;span class="caps"&gt;RDS&lt;/span&gt;::DBInstance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Properties&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt;            &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Fn::If&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;UseDbSnapshot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::NoValue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;Engine&amp;quot;&lt;/span&gt;            &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;MySQL&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;MasterUsername&amp;quot;&lt;/span&gt;    &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBUsername&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBInstanceClass&amp;quot;&lt;/span&gt;   &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBClass&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBSecurityGroups&amp;quot;&lt;/span&gt;  &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSecurityGroup&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;}],&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBSubnetGroupName&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSubnetGroup&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;AllocatedStorage&amp;quot;&lt;/span&gt;  &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBAllocatedStorage&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;MasterUserPassword&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBPassword&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Fn::If&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;UseDbSnapshot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBSnapshotIdentifier&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
            &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::NoValue&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;MultiAZ&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;DeletionPolicy&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Snapshot&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;WebServerGroup&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::AutoScaling::AutoScalingGroup&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Properties&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;LaunchConfigurationName&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;LaunchConfig&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;UpdatePolicy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;AutoScalingRollingUpdate&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;MinInstancesInService&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;MaxBatchSize&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;WaitOnResourceSignals&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;true&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;PauseTime&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;PT10M&lt;/span&gt;&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;},&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;AutoScalingScheduledAction&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;IgnoreUnmodifiedGroupSizeProperties&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="kc"&gt;true&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;},&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;CreationPolicy&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;ResourceSignal&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Timeout&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;PT10M&lt;/span&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;Count&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;2&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;quot;LaunchConfig&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Type&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::AutoScaling::LaunchConfiguration&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
      &lt;span class="nt"&gt;&amp;quot;Metadata&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="nt"&gt;&amp;quot;&lt;span class="caps"&gt;AWS&lt;/span&gt;::CloudFormation::Init&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
          &lt;span class="nt"&gt;&amp;quot;config&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
            &lt;span class="nt"&gt;&amp;quot;files&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
              &lt;span class="nt"&gt;&amp;quot;/opt/wordpress/cloudformation_db.php&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
                &lt;span class="nt"&gt;&amp;quot;content&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="nt"&gt;&amp;quot;Fn::Join&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;&amp;lt;?php\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_NAME&amp;#39;,          &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBName&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_USER&amp;#39;,          &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBUsername&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_PASSWORD&amp;#39;,      &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Ref&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;DBPassword&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                  &lt;span class="s2"&gt;&amp;quot;define(&amp;#39;DB_HOST&amp;#39;,          &amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="nt"&gt;&amp;quot;Fn::GetAtt&amp;quot;&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;DBInstance&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Endpoint.Address&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]},&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;#39;);\n&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="p"&gt;]]&lt;/span&gt; &lt;span class="p"&gt;},&lt;/span&gt;
              &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="p"&gt;}&lt;/span&gt;
          &lt;span class="p"&gt;}&lt;/span&gt;
        &lt;span class="p"&gt;}&lt;/span&gt;
      &lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Mon, 15 Dec 2014 09:29:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2014-12-15:2014/12/aws-cloudformation-and-rds-snapshots/</guid><category>aws</category><category>cloudformation</category><category>rds</category><category>mysql</category><category>snapshot</category></item><item><title>Watching Jenkins Jobs and CloudFormation Updates with PushoverÂ Notification</title><link>http://blog.jasonantman.com/2014/12/watching-jenkins-jobs-and-cloudformation-updates-with-pushover-notification/</link><description>&lt;p&gt;A few months ago I &lt;a href="http://blog.jasonantman.com/2014/09/pushover-notifications-for-shell-command-completion-and-status/"&gt;posted&lt;/a&gt;
about a script I wrote to send &lt;a href="https://pushover.net/"&gt;Pushover&lt;/a&gt; notifications for shell command&amp;nbsp;completion.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve been doing quite a bit of work lately both with testing some &lt;a href="http://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; jobs, and spinning up
&lt;span class="caps"&gt;AWS&lt;/span&gt; stacks using &lt;a href="https://aws.amazon.com/cloudformation/"&gt;CloudFormation&lt;/a&gt;. Last week I wrote two python scripts to aid in&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jantman/misc-scripts/blob/master/watch_cloudformation.py"&gt;watch_cloudformation.py&lt;/a&gt; uses the popular &lt;a href="https://github.com/boto/boto"&gt;boto&lt;/a&gt;
Python &lt;span class="caps"&gt;AWS&lt;/span&gt; interface to list (and display) the events on a specified CloudFormation stack, and exit 0 or 1 when it finds a (&lt;span class="caps"&gt;CREATE&lt;/span&gt;|&lt;span class="caps"&gt;UPDATE&lt;/span&gt;)_(&lt;span class="caps"&gt;FAILED&lt;/span&gt;|&lt;span class="caps"&gt;COMPLETE&lt;/span&gt;) event.
It also optionally uses &lt;a href="https://pypi.python.org/pypi/python-pushover"&gt;python-pushover&lt;/a&gt; to send the notification to your devices via&amp;nbsp;Pushover.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jantman/misc-scripts/blob/master/watch_jenkins.py"&gt;watch_jenkins.py&lt;/a&gt; takes the &lt;span class="caps"&gt;URL&lt;/span&gt; to a Jenkins job or build, and uses
&lt;a href="https://pypi.python.org/pypi/python-jenkins"&gt;python-jenkins&lt;/a&gt; to poll the status of the build (or the latest build, if given a Job url)
and display the result when the build finishes, also optionally using python-pushover to send notifications to your&amp;nbsp;device.&lt;/p&gt;
&lt;p&gt;They&amp;#8217;re really quick-and-dirty scripts and might not be suitable for everyone&amp;#8217;s use case, but I took the time to write them,
so hopefully they&amp;#8217;ll be useful to someone&amp;nbsp;else.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sun, 14 Dec 2014 19:22:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2014-12-14:2014/12/watching-jenkins-jobs-and-cloudformation-updates-with-pushover-notification/</guid><category>script</category><category>pushover</category><category>jenkins</category><category>hudson</category><category>aws</category><category>cloudformation</category></item><item><title>Idea for a Generic Method to Communicate Repository/ProjectÂ Status</title><link>http://blog.jasonantman.com/2014/12/idea-for-a-generic-method-to-communicate-repositoryproject-status/</link><description>&lt;p&gt;Update 2014-12-24: I actually did something with this. See &lt;a href="http://www.repostatus.org"&gt;repostatus.org&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;First, something funny, before my possibly-hair-brained&amp;nbsp;scheme:&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.commitstrip.com/en/2014/11/25/west-side-project-story/"&gt;&lt;img alt="commitstrip.com &amp;quot;side project&amp;quot; comic strip" src="http://www.commitstrip.com/wp-content/uploads/2014/11/Strip-Side-project-650-finalenglish.jpg" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I know I&amp;#8217;m not alone in having a mess of &lt;a href="https://github.com/jantman?tab=repositories"&gt;repositories on GitHub&lt;/a&gt;; I personally have over 90, and they&amp;#8217;re
all in various states of &amp;#8220;doneness.&amp;#8221; Some are working and undergoing active development. Some should be
working, but I no longer use them (and sometimes lack &amp;#8220;things&amp;#8221; needed to use them, especially the case
with projects linked to specific hardware). Some of them were ideas that never took off; some of these
I intend on finishing, and some I never want to touch&amp;nbsp;again.&lt;/p&gt;
&lt;p&gt;While GitHub has a &lt;a href="https://help.github.com/articles/about-releases/"&gt;Releases&lt;/a&gt; feature, at best (where everyone
understands and follows &lt;a href="http://semver.org/"&gt;semantic versioning&lt;/a&gt;), it can only differentiate &amp;#8220;initial development&amp;#8221;
(prior to stable public release) versions from those after them. It may be an indication of the usability or completeness
of the software, but not of its current state of&amp;nbsp;maintenance.&lt;/p&gt;
&lt;p&gt;The questions that I&amp;#8217;d really like to be able to answer about a given project or repository&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is the &amp;#8220;completeness&amp;#8221; of the code? Should it be usable, or is it functionally&amp;nbsp;incomplete?&lt;/li&gt;
&lt;li&gt;What is the status of development efforts? Is this actively developed, or supported (even if bugfix-only), or totally&amp;nbsp;abandoned?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I&amp;#8217;d like to be able to easily communicate this to people who come across my work, and also
track it for my own needs - I have enough repositories with barely-started concepts that I
occasionally forget about them. I&amp;#8217;d also, of course, like to be able to know this information
about other peoples&amp;#8217; work as&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;Ideally, I thought that this should be a GitHub feature, exposed via the &lt;span class="caps"&gt;API&lt;/span&gt; and the &lt;span class="caps"&gt;UI&lt;/span&gt;. However,
there are a number of problems with&amp;nbsp;that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It would require GitHub to implement the feature. Quite ironically, GitHub is &lt;a href="https://github.com/isaacs/github/issues/6"&gt;not very open&lt;/a&gt;
about issues and feature requests for their platform itself, and the only good way to suggest something is &lt;a href="https://github.com/isaacs/github"&gt;unofficial&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;It would be tied to GitHub directly. When the next big thing comes along, or for projects using other services (like Gitorious, or even non-git hosting),
it would be rendered&amp;nbsp;useless.&lt;/li&gt;
&lt;li&gt;The status really describes the code/project itself, not the GitHub repository per se, so it should live with the&amp;nbsp;code.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;So, I&amp;#8217;m brainstorming a straightforward semi-standardized way of communicating this information. Assuming
it&amp;#8217;s not implemented in GitHub itself, but rather becomes part of the repository content, that poses some
interesting questions for both what information is communicated and how to communicate it. What follows is
really my brainstorming and initial ideas. I&amp;#8217;d very much appreciate it if anyone who&amp;#8217;s interested submits
their ideas and comments. I fully intend to start using something like this for my own projects but, not to
be too arrogant, I think it&amp;#8217;s a useful idea and could benefit from some accepted&amp;nbsp;standard.&lt;/p&gt;
&lt;h2 id="what-to-communicate"&gt;What to&amp;nbsp;Communicate&lt;/h2&gt;
&lt;p&gt;The first question is what data to communicate. Ideally, this would be one of a standardized set of
repository/project status identifiers, along with a textual description that could be provided by
the author, for additional clarity. My humble suggestion (very much a &lt;span class="caps"&gt;WIP&lt;/span&gt;) of the possible statuses,
along with the suggested (canonical) description of their&amp;nbsp;meanings:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Concept&lt;/strong&gt; - Minimal or no implementation has been done&amp;nbsp;yet.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;&lt;span class="caps"&gt;WIP&lt;/span&gt;&lt;/strong&gt; - Initial development is in progress, but there has not yet been a stable, usable release suitable for the&amp;nbsp;public.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Suspended&lt;/strong&gt; - A &lt;span class="caps"&gt;WIP&lt;/span&gt; project that has had work stopped for the time being; the author(s) intend on resuming&amp;nbsp;work.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Abandoned&lt;/strong&gt; - A &lt;span class="caps"&gt;WIP&lt;/span&gt; project that has been abandoned; the author(s) do not intend on continuing&amp;nbsp;development.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Active&lt;/strong&gt; - The project has reached a stable, usable state and is being actively&amp;nbsp;developed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inactive&lt;/strong&gt; - The project has reached a stable, usable state but is no longer being actively developed; support/maintenance will be provided as time&amp;nbsp;allows.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unsupported&lt;/strong&gt; - The project has reached a stable, usable state but the author(s) have ceased all work on it. A new maintainer may be&amp;nbsp;desired.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I assume that there might be some dissenting opinions on whether this list of statuses is complete, or perhaps too long.
However I feel that it&amp;#8217;s the minimum set required to describe a project along the two axes which I consider important:
usability (is the code here complete enough to &amp;#8220;work&amp;#8221; for something) and support/development status (is it being worked on,
or are there plans to do so in the future). I&amp;#8217;m certainly open to opinions on&amp;nbsp;this.&lt;/p&gt;
&lt;h2 id="how-to-communicate-it"&gt;How to Communicate&amp;nbsp;It&lt;/h2&gt;
&lt;p&gt;I view this as a more complex question technically, as doing this within the repository content (instead of in a GitHub &lt;span class="caps"&gt;API&lt;/span&gt;)
necessarily involves polluting that repository. My main two technical requirements (at least with my own intended use in mind)
are that the status be readable both by human and machine, and that the status should be available in one place within the
repository (i.e. in only one place for both humans and machines, and not requiring any&amp;nbsp;transformation).&lt;/p&gt;
&lt;p&gt;The best I&amp;#8217;ve been able to come up with so far is either including the status in a special file (likely a specially-named dotfile),
or including it in the &lt;span class="caps"&gt;README&lt;/span&gt;. The dotfile method is optimized for machine-reading - it would be a single file, likely named
&amp;#8220;.repostatus.org&amp;#8221;, with a simple specified format. It&amp;#8217;s easy and cheap for a machine to find and parse, and shouldn&amp;#8217;t be too cumbersome
to add. But it pollutes the repository with another file, and worse, it would be quite unlikely to be found by a human who isn&amp;#8217;t
familiar with this practice, so it loses a lot in terms of human readability and&amp;nbsp;intuitiveness.&lt;/p&gt;
&lt;p&gt;On the other hand, adding something special to the &lt;span class="caps"&gt;README&lt;/span&gt; file is much more human-centric. The &amp;#8220;something&amp;#8221; could be a simple
string or link, or even better, a &lt;a href="http://shields.io/"&gt;badge&lt;/a&gt;. It would appear clearly when rendered on GitHub, and should also appear anywhere
else the readme is rendered (i.e. in online documentation or in packages of the project). However, this poses a few&amp;nbsp;challenges:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;It wouldn&amp;#8217;t necessarily be possible to have a status that&amp;#8217;s machine-readable but not rendered to the human observer. Sure, this
sort of goes against half of the purpose of this idea, but some people probably wouldn&amp;#8217;t want this extra piece of information
cluttering up their &lt;span class="caps"&gt;README&lt;/span&gt;. It&amp;#8217;s possible to put comments in &lt;a href="http://docutils.sourceforge.net/docs/ref/rst/restructuredtext.html#comments"&gt;rST&lt;/a&gt;,
but &lt;a href="http://stackoverflow.com/questions/4823468/store-comments-in-markdown-syntax"&gt;markdown support&lt;/a&gt; isn&amp;#8217;t nearly as reliable,
being a bit of a&amp;nbsp;hack.&lt;/li&gt;
&lt;li&gt;This is optimized for human readers. In order to be detected by machine, the repository would need to be searched for
a readme file (even assuming the convention of &amp;#8220;^&lt;span class="caps"&gt;README&lt;/span&gt;*&amp;#8221;, there&amp;#8217;s a myriad of possible file extensions that could be used),
which isn&amp;#8217;t necessarily a cheap operation (especially since it would require access to the file listing within the&amp;nbsp;repository).&lt;/li&gt;
&lt;li&gt;Furthermore, machine detection would need to be able to either parse the markup (if any), or do string search on the file
contents. Once again, a more expensive&amp;nbsp;operation.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="current-theory"&gt;Current&amp;nbsp;Theory&lt;/h2&gt;
&lt;p&gt;At the moment, I&amp;#8217;m leaning towards this theory of&amp;nbsp;implementation:&lt;/p&gt;
&lt;p&gt;Badges are placed in the project&amp;#8217;s &lt;span class="caps"&gt;README&lt;/span&gt; indicating the status. The badges would be sourced from specified URLs, served
by &lt;a href="http://repostatus.org"&gt;repostatus.org&lt;/a&gt; and linked to specified URLs describing the status (likely of the form
http://repostatus.org/1.0/#active). Machine determination of status would be made by a string match for one of
the specified status URLs - nothing more is needed. It would be simple enough to simply specify that, for machine
determination, the first file in the repository (sorted in lexicographical order) beginning with &amp;#8220;readme&amp;#8221; (case-insensitive) and containing
a matching &lt;span class="caps"&gt;URL&lt;/span&gt; determines the status. For human users, the badge image could be combined with descriptive alt-text, and
possibly followed by a more descriptive explanation, if the author chose so. This would eliminate the need for a fixed
set of possible readme file names, and the need for machine identification to be able to parse all possible&amp;nbsp;markups.&lt;/p&gt;
&lt;p&gt;The visual impact to the readme document (assuming it&amp;#8217;s rendered) would be minimal. Here are some quick takes on
a first set of badges, along with the alt text set on them (which could be changed by the user, or also included
in plain text next to the&amp;nbsp;badge).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;img alt="Repo Status: Concept - Minimal or no implementation has been done yet." src="http://img.shields.io/badge/repo%20status-Concept-ffffff.svg" /&gt; Repo Status: Concept - Minimal or no implementation has been done&amp;nbsp;yet.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: WIP - Initial development is in progress, but there has not yet been a stable, usable release suitable for the public." src="http://img.shields.io/badge/repo%20status-WIP-yellow.svg" /&gt; Repo Status: &lt;span class="caps"&gt;WIP&lt;/span&gt; - Initial development is in progress, but there has not yet been a stable, usable release suitable for the&amp;nbsp;public.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: Suspended - A WIP project that has had work stopped for the time being; the author(s) intend on resuming work." src="http://img.shields.io/badge/repo%20status-Suspended-orange.svg" /&gt; Repo Status: Suspended - A &lt;span class="caps"&gt;WIP&lt;/span&gt; project that has had work stopped for the time being; the author(s) intend on resuming&amp;nbsp;work.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: Abandoned - A WIP project that has been abandoned; the author(s) do not intend on continuing development." src="http://img.shields.io/badge/repo%20status-Abandoned-000000.svg" /&gt; Repo Status: Abandoned - A &lt;span class="caps"&gt;WIP&lt;/span&gt; project that has been abandoned; the author(s) do not intend on continuing&amp;nbsp;development.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: Active - The project has reached a stable, usable state and is being actively developed." src="http://img.shields.io/badge/repo%20status-Active-brightgreen.svg" /&gt; Repo Status: Active - The project has reached a stable, usable state and is being actively&amp;nbsp;developed.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: Inactive - The project has reached a stable, usable state and is no longer being actively developed; support/maintenance will be done as time allows." src="http://img.shields.io/badge/repo%20status-Inactive-yellowgreen.svg" /&gt; Repo Status: Inactive - The project has reached a stable, usable state and is no longer being actively developed; support/maintenance will be done as time&amp;nbsp;allows.&lt;/li&gt;
&lt;li&gt;&lt;img alt="Repo Status: Unsupported - The project has reached a stable, usable state but the author(s) have ceased all work on it. A new maintainer may be desired." src="http://img.shields.io/badge/repo%20status-Unsupported-lightgrey.svg" /&gt; Repo Status: Unsupported - The project has reached a stable, usable state but the author(s) have ceased all work on it. A new maintainer may be&amp;nbsp;desired.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If the readme is (for some strange reason) primarily intended for a non-rendered view, it would be acceptable to
include just the &lt;span class="caps"&gt;URL&lt;/span&gt; to the status description, optionally with some human-readable&amp;nbsp;text.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll probably start using something like this for my personal projects. I intend on even writing up a spec
for the &lt;span class="caps"&gt;README&lt;/span&gt;-based variant, along with some formatting and parsing/machine identification rules. Any and
all comments are welcome. This is the result of a few hours&amp;#8217; sporadic thought one afternoon, so I&amp;#8217;m sure there
are some major issues I haven&amp;#8217;t realized yet. Please pass them along, or tell me if this is of any interest to&amp;nbsp;you.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sun, 07 Dec 2014 18:24:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2014-12-07:2014/12/idea-for-a-generic-method-to-communicate-repositoryproject-status/</guid><category>repository</category><category>project</category><category>git</category><category>github</category></item><item><title>Managing EC2 SSH Keys - AnÂ Idea</title><link>http://blog.jasonantman.com/2014/10/managing-ec2-ssh-keys-an-idea/</link><description>&lt;p&gt;At work, we have a bunch of &lt;span class="caps"&gt;EC2&lt;/span&gt; instances (currently hundreds, and growing quickly). We also have a bunch
(probably now around 100, counting contractors) of users. Some users - mainly engineers - need &lt;span class="caps"&gt;SSH&lt;/span&gt; access to all
of the &lt;span class="caps"&gt;EC2&lt;/span&gt; instances; many others only need access to their team&amp;#8217;s instances. While I usually advocate sanity checks
and training over access control for employees, many teams have expressed legitimate concern that they don&amp;#8217;t want
others on their instances; commands that are safe to run in dev/test (like loading test data) might be disastrous
on production instances. So, as part of our automation and tooling team, I&amp;#8217;ve been trying to come up with a way to manage
access to all these instances. Right now we have a single &amp;#8220;bastion&amp;#8221; (a.k.a. jump box / ssh gateway / keyhole) instance, with a single
shared used keyed to access every &lt;span class="caps"&gt;EC2&lt;/span&gt; instance; that doesn&amp;#8217;t scale and doesn&amp;#8217;t meet the security&amp;nbsp;requirements.&lt;/p&gt;
&lt;p&gt;What follows is one theory of mine on how to solve this problem. I&amp;#8217;ve been thinking about this for the past
day; this might not be the Right answer, and it&amp;#8217;s just a theory at this point, but I think it&amp;nbsp;works.&lt;/p&gt;
&lt;h1 id="requirements-and-assumptions"&gt;Requirements and&amp;nbsp;Assumptions&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;We have Active Directory as the one source of authentication/authorization truth, but it&amp;#8217;s only in the corporate
  network. For various reasons both technical and political, accessing it from &lt;span class="caps"&gt;AWS&lt;/span&gt; (whether directly, over &lt;span class="caps"&gt;VPN&lt;/span&gt;,
  via replication, or via data feeds to a separate &lt;span class="caps"&gt;LDAP&lt;/span&gt; infrastructure in &lt;span class="caps"&gt;EC2&lt;/span&gt;) is simply not&amp;nbsp;possible.&lt;/li&gt;
&lt;li&gt;We want to control &lt;span class="caps"&gt;SSH&lt;/span&gt; access to a bunch of instances. Some of them are persistent and some are ephemeral. Some
  are pre-baked AMIs in auto-scaling groups, with &lt;em&gt;no&lt;/em&gt; changes made outside the &lt;span class="caps"&gt;AMI&lt;/span&gt;. Some of them are persistent
  or semi-persistent instances that run Puppet every 30 minutes. Some of them are somewhat special, and can&amp;#8217;t be
  trivially torn&amp;nbsp;down.&lt;/li&gt;
&lt;li&gt;Most of our instances are in a &lt;span class="caps"&gt;VPC&lt;/span&gt;, and have proper security controls which include &lt;span class="caps"&gt;SSH&lt;/span&gt; access from only a specifically
  white-listed range of IPs. However, some instances are in &amp;#8220;&lt;span class="caps"&gt;EC2&lt;/span&gt; Classic&amp;#8221; and have &lt;span class="caps"&gt;SSH&lt;/span&gt; open to the world. We want a
  solution that also protects these&amp;nbsp;instances.&lt;/li&gt;
&lt;li&gt;We&amp;#8217;re mainly concerned with securing access from (a) users inadvertently accessing an instance they shouldn&amp;#8217;t be
  on, (b) outside/untrusted parties, and (c) former employees. We trust our employees within reason, and accept that,
  within our security stance, if an employee &lt;em&gt;really&lt;/em&gt; wants privilege escalation, they&amp;#8217;re going to get it. We&amp;#8217;re not
  overly concerned with protecting against determined, malicious users who already have some access but want&amp;nbsp;more.&lt;/li&gt;
&lt;li&gt;Our current process for security cleanup for former employees is largely based on corporate &lt;span class="caps"&gt;IT&lt;/span&gt; (or is it &lt;span class="caps"&gt;HR&lt;/span&gt;?) turning
  off their &lt;span class="caps"&gt;AD&lt;/span&gt; account. We want to minimize additional steps that need to be completed when someone has access&amp;nbsp;revoked.&lt;/li&gt;
&lt;li&gt;Any solution that we choose needs to be usable with self-service &lt;span class="caps"&gt;AWS&lt;/span&gt;; i.e. any user can spin up their own instances
  or stacks, provided that they use an &lt;span class="caps"&gt;AMI&lt;/span&gt; that is either built by our automation team, or follows guidelines on what
  must be included in all&amp;nbsp;AMIs.&lt;/li&gt;
&lt;li&gt;We have some administrative accounts (Jenkins, as well as some shared privileged accounts on select machines) that need
  unrestricted access to&amp;nbsp;everything.&lt;/li&gt;
&lt;li&gt;Local user accounts aren&amp;#8217;t an option. This would mean running Puppet constantly on every image and/or rebuilding
  every image each time we gain or lose an employee. That would be especially difficult when we occasionally have
  project-based&amp;nbsp;contractors.&lt;/li&gt;
&lt;li&gt;We&amp;#8217;re &lt;span class="caps"&gt;OK&lt;/span&gt; with having a bastion/keyhole server in &lt;span class="caps"&gt;AWS&lt;/span&gt;, we just don&amp;#8217;t want everyone to be able to access&amp;nbsp;everything.&lt;/li&gt;
&lt;li&gt;Our intended network security stance is to have bastion/keyhole servers in &lt;span class="caps"&gt;AWS&lt;/span&gt; (ideally one per &lt;span class="caps"&gt;AZ&lt;/span&gt;), which are only
  reachable via &lt;span class="caps"&gt;SSH&lt;/span&gt; from selected public addresses on our corporate network (which can only be reached by current
  employees with valid, working access). All other instances should only allow &lt;span class="caps"&gt;SSH&lt;/span&gt; from these selected&amp;nbsp;hosts.&lt;/li&gt;
&lt;li&gt;Despite the above, we don&amp;#8217;t want to rely on an instance being properly configured as our only security measure;
  if an instance is incorrectly configured to accept &lt;span class="caps"&gt;SSH&lt;/span&gt; from 0.0.0.0/0, we still want to prevent users whose
  access has been revoked from logging in to the&amp;nbsp;instance.&lt;/li&gt;
&lt;li&gt;We don&amp;#8217;t need access to be granted and revoked immediately. We&amp;#8217;ll assume that in normal operating conditions,
  thirty (30) minutes is a reasonable amount of time to either grant or revoke a user&amp;#8217;s&amp;nbsp;access.&lt;/li&gt;
&lt;li&gt;We want to minimize reliance on our existing corporate infrastructure, so that &lt;span class="caps"&gt;AWS&lt;/span&gt; can be used for business
  continuity&amp;nbsp;purposes.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="main-goals"&gt;Main&amp;nbsp;Goals&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Provide users with &lt;span class="caps"&gt;SSH&lt;/span&gt; access to &lt;span class="caps"&gt;EC2&lt;/span&gt; servers; privilege should be able to be granted to a subset of users and/or groups
  for each &amp;#8220;application&amp;#8221;. Users should not be able to access other&amp;nbsp;instances.&lt;/li&gt;
&lt;li&gt;Allow a fixed list of users access to every&amp;nbsp;instance.&lt;/li&gt;
&lt;li&gt;Be able to revoke a user&amp;#8217;s access without rebuilding instances or ssh-in-a-loop&amp;#8217;ing to all of&amp;nbsp;them.&lt;/li&gt;
&lt;li&gt;Many instances are not going to be running Puppet after initial provisioning/&lt;span class="caps"&gt;AMI&lt;/span&gt; creation, so as much as we love Puppet,
  it&amp;#8217;s not an option to solve this&amp;nbsp;problem.&lt;/li&gt;
&lt;li&gt;This should involve a minimum of administrative overhead when a user leaves the&amp;nbsp;company.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="proposed-solution"&gt;Proposed&amp;nbsp;Solution&lt;/h1&gt;
&lt;p&gt;My solution relies on &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding and the &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt; introduced in OpenSSH 6.2 (see &amp;#8220;Limitations&amp;#8221;, below, for more information),
most likely inspired by (or maybe literally the same code)
as the patch formerly used by GitHub. This allows sshd to execute an arbitrary command, passing it the login username, which returns output identical to what would
be in the &lt;code&gt;authorized_keys&lt;/code&gt; file. If none of the keys successfully authenticate the user, authentication continues using the usual &lt;code&gt;AuthorizedKeysFile&lt;/code&gt;. We take
advantage of this feature, in addition to &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding, to provide our granular access control. Public keys are pulled from a central location &lt;em&gt;at login time&lt;/em&gt;
(and cached for a set amount of time); each user has control over their own public keys, and a central process builds sets of public keys authorized to access a given
group of&amp;nbsp;instances.&lt;/p&gt;
&lt;h2 id="infrastructure"&gt;Infrastructure&lt;/h2&gt;
&lt;p&gt;Each &lt;span class="caps"&gt;EC2&lt;/span&gt; instance will be a member of an Access Group, which is a unique identifier for the set of users authorized to access instances
in the group. In implementation, Access Groups will likely just be a tag on &lt;span class="caps"&gt;EC2&lt;/span&gt; instances that maps to a set of predefined values
(see below for&amp;nbsp;more).&lt;/p&gt;
&lt;p&gt;We will have a number of &amp;#8220;bastion&amp;#8221; (keyhole/jump box/&lt;span class="caps"&gt;SSH&lt;/span&gt; gateway) hosts, ideally one in each Availability Zone where we have instances.
These bastion hosts will only be reachable from within our corporate network (or our &lt;span class="caps"&gt;VPN&lt;/span&gt;); therefore, users must have
current access to our corporate network (where we can rely on Active Directory and other systems to handle authorization) in order to
gain access to &lt;span class="caps"&gt;AWS&lt;/span&gt;. All other &lt;span class="caps"&gt;EC2&lt;/span&gt; instances will only be reachable over &lt;span class="caps"&gt;SSH&lt;/span&gt; from one of these bastion hosts. The bastion hosts themselves
will not have &lt;span class="caps"&gt;SSH&lt;/span&gt; keys to access other instances; they will, however, have &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding&amp;nbsp;enabled.&lt;/p&gt;
&lt;p&gt;Users reach &lt;span class="caps"&gt;AWS&lt;/span&gt; instances by SSHing from a host attached to our corporate network (including &lt;span class="caps"&gt;VPN&lt;/span&gt; hosts) to a bastion host in &lt;span class="caps"&gt;EC2&lt;/span&gt;. From there,
they &lt;span class="caps"&gt;SSH&lt;/span&gt; to the destination instance, making use of &lt;span class="caps"&gt;SSH&lt;/span&gt; agent forwarding to use their local key to authenticate to the instance. We get both
a restricted entry point to &lt;span class="caps"&gt;AWS&lt;/span&gt; (the bastion host, which can enforce further security and logging methods) and the ability to authenticate users
using their own personal public keys on the destination&amp;nbsp;instances.&lt;/p&gt;
&lt;p&gt;To make it easier for end-users, we could develop a wrapper script like &lt;a href="https://pypi.python.org/pypi/ec2-ssh"&gt;Instagram&amp;#8217;s ec2-ssh&lt;/a&gt; that
checks for a valid, running ssh agent with keys in it, and then crafts the correct &lt;span class="caps"&gt;SSH&lt;/span&gt; command to land the user on the desired end
host - i.e. something like &lt;code&gt;ec2ssh instance_id&lt;/code&gt; would generate and execute a command like &lt;code&gt;ssh -At bastion_hostname 'ssh instance_ip'&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="on-the-servers-instances"&gt;On the Servers&amp;nbsp;(Instances)&lt;/h2&gt;
&lt;p&gt;Each instance, when initially built/provisioned, is given a &lt;code&gt;get_authorized_keys&lt;/code&gt; script, which is configured to be run by sshd as the
&lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;. This script uses one of the following three public key distribution services to retrieve the authorized public keys
for that instance, which are then echoed on &lt;span class="caps"&gt;STDOUT&lt;/span&gt; and used to authenticate the user. For the sake of simplicity, we&amp;#8217;ll assume (which is
currently the case in our infrastructure) that this script will only run for a single non-root user that is used for logins; it will exit
without returning any output for any other users on the system, effectively preventing logins to&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;The script will first check for authorized keys cached locally (either on disk or in memory, to be determined). If they&amp;#8217;re found and less
than some age threshold (we&amp;#8217;ll say five minutes), the cached version is returned. This is intended to both reduce latency when performing
multiple sequential logins, and to allow logins to continue functioning through short periods of degraded network connectivity. If no recent
keys are found cached on disk, the script will retrieve them from the configured public key distribution service. If the service does not
return an appropriate response within an acceptable time limit, or is unreachable, the script will exit with no output. This will prevent
logins from users authorized with this method, but will fall through to the standard &lt;code&gt;AuthorizedKeysFile&lt;/code&gt; method. A number of permanent
authorized public keys will be included in each instance, to allow emergency administrative access in the event that the key distribution
service&amp;nbsp;fails.&lt;/p&gt;
&lt;p&gt;If we&amp;#8217;re willing to assume that the instances themselves are trusted (which I think is a valid assumption), the key retrieval script on
each instance will determine the Access Group that the instance belongs to, and then request the authorized keys for that Access Group.
Determination of Access Group will likely be made via user data passed into the instance at provisioning time, or via retrieval of a
tag value for the&amp;nbsp;instance.&lt;/p&gt;
&lt;p&gt;If assuming trust locally on the instance is not sufficient, then the burden of identifying the instance&amp;#8217;s access group is shifted
to the key distribution service (likely by identifying the &lt;span class="caps"&gt;IP&lt;/span&gt; address of the requesting instance, and then using the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; to
determine which group that instance belongs to). With this solution, only the second alternative key distribution service is&amp;nbsp;feasible.&lt;/p&gt;
&lt;p&gt;If a shorter delay to authorization changes is needed, it would be feasible for instances to also run a separate process
(cronjob, daemon, etc.) that polls the key distribution service at a regular interval to check for updates (i.e.
&lt;span class="caps"&gt;HTTP&lt;/span&gt; &lt;span class="caps"&gt;HEAD&lt;/span&gt;, something &lt;span class="caps"&gt;SQS&lt;/span&gt;-based, etc.) and updates the local cache when they&amp;nbsp;occur.&lt;/p&gt;
&lt;h1 id="public-key-distribution-service"&gt;Public Key Distribution&amp;nbsp;Service&lt;/h1&gt;
&lt;p&gt;Instances will retrieve their authorized public keys from a key distribution service. Three examples&amp;nbsp;follow:&lt;/p&gt;
&lt;h2 id="alternative-1-scalable-architecture-aws-and-local"&gt;Alternative 1 - Scalable Architecture - &lt;span class="caps"&gt;AWS&lt;/span&gt; and&amp;nbsp;Local&lt;/h2&gt;
&lt;p&gt;Keys will be managed by a web-based application (with a complete and documented &lt;span class="caps"&gt;API&lt;/span&gt;) living in the corporate data center.
The application will provide facilities for authorized users (managers, operations) to define new Access Groups and modify
the list of users allowed to access them. Individual end-users will be able to manage their public keys. At a set interval,
a standalone script will retrieve a list of all users defined in the application and check the status of their corporate Active
Directory accounts. Any users whose accounts have been deactivated or locked will be flagged as such in the application. Whenever
a change is made in the application (including a user being flagged as deactivated), all Access Groups that include that user
will have their authorized_keys file (composed of the authorized_keys files of all users with access) written to an S3 bucket
that&amp;#8217;s only writable by the privileged
user running the application. All instances will have &lt;span class="caps"&gt;IAM&lt;/span&gt; roles that allow them to read the&amp;nbsp;bucket.&lt;/p&gt;
&lt;p&gt;This method allows us to provide self-service to users and application administrators, and keeps all data about users within
the corporate network. It provides automatic revocation of access for disabled Active Directory accounts. It does introduce
a delay in revocation of access for disabled &lt;span class="caps"&gt;AD&lt;/span&gt; accounts, but a delay of ~10 minutes is certainly not a concern in our&amp;nbsp;environment.&lt;/p&gt;
&lt;h2 id="alternative-2-scalable-architecture-entirely-in-aws"&gt;Alternative 2 - Scalable Architecture Entirely in&amp;nbsp;&lt;span class="caps"&gt;AWS&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;A similar application exists, but lives entirely in &lt;span class="caps"&gt;AWS&lt;/span&gt;, utilizing its native high availability technologies (i.e. multi-&lt;span class="caps"&gt;AZ&lt;/span&gt;
&lt;span class="caps"&gt;RDS&lt;/span&gt; as a data store). A script still runs in the corporate data center, but all it does is query the &lt;span class="caps"&gt;API&lt;/span&gt; for a list of all
active users, check &lt;span class="caps"&gt;AD&lt;/span&gt; account status, and deactivate any users that no longer have a valid account. Instead of writing the
authorized key files to an S3 bucket, the application serves them directly in real-time. The application could
store keys and data in a &lt;span class="caps"&gt;RDBMS&lt;/span&gt;, or perhaps something like OpenLDAP, depending on which technologies are best known and
what the performance requirements&amp;nbsp;are.&lt;/p&gt;
&lt;p&gt;This is more of an infrastructure challenge and introduces additional points for failure; if the application above (1)
fails, it will only impact &lt;em&gt;changes&lt;/em&gt; to access, whereas if this application fails, all user access (aside from the static
emergency keys) will break. However, this method allows us to control access at a level finer than Access Groups; rules
could be developed based on any attributes of the requesting instance, including (if the latency was allowable) queries
to the &lt;span class="caps"&gt;EC2&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; for instance-specific&amp;nbsp;data.&lt;/p&gt;
&lt;h2 id="alternative-3-simple-architecture"&gt;Alternative 3 - Simple&amp;nbsp;Architecture&lt;/h2&gt;
&lt;p&gt;A text file stores mappings of Access Groups to the Active Directory users and groups authorized for them. The text file
is manually maintained, stored in version control, and all changes must comply with an access policy and be peer-reviewed.
A script runs at a set interval (let&amp;#8217;s say cron every 5-10 minutes) that reads the user/group mapping, translates groups
to their membership list, and checks the &lt;span class="caps"&gt;AD&lt;/span&gt; account status of every listed user. Users without valid/current/enabled accounts
are removed from the lists in memory. For the remaining (active) users for each Access Group, their &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt;
file is read. All user&amp;#8217;s authorized_keys files are concatenated together per Access Group, and the result is written to
an S3&amp;nbsp;bucket.&lt;/p&gt;
&lt;p&gt;This is by far the simplest method, and relies on our &lt;span class="caps"&gt;NFS&lt;/span&gt; shared home directories to allow users to manage their public
keys by simply using the standard file. This keeps all user-related data in our corporate data center, and means that we
have only one script and its&amp;#8217; cron job to maintain, rather than a whole application. The text-file-based method of access
control isn&amp;#8217;t terribly scalable, but it should work for the ~100 users that we have to deal with. Checking &lt;span class="caps"&gt;AD&lt;/span&gt; account status
when generating the file should provide a feasible safeguard for users whose corporate accounts are locked/revoked without
requiring someone to remember to also remove them from the &lt;span class="caps"&gt;AWS&lt;/span&gt; user&amp;nbsp;list.&lt;/p&gt;
&lt;h2 id="advantages-over-other-solutions"&gt;Advantages Over Other&amp;nbsp;Solutions&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Self-service for users and for managers/administrators of&amp;nbsp;applications.&lt;/li&gt;
&lt;li&gt;No manual intervention when a user leaves the company; users automatically deactivated when their &lt;span class="caps"&gt;AD&lt;/span&gt; account&amp;nbsp;is.&lt;/li&gt;
&lt;li&gt;No cron job or daemon to run on instances, and no centralized process to break key distribution; each instance
  automatically pulls the current authorized keys when a login is&amp;nbsp;attempted.&lt;/li&gt;
&lt;li&gt;Doesn&amp;#8217;t depend on Puppet, so it allows individual applications to use Puppet as they desire, without complication
  or&amp;nbsp;confusion.&lt;/li&gt;
&lt;li&gt;Only depends on centralized (corporate data center) infrastructure for key updates (at most). Failure of connectivity
  between &lt;span class="caps"&gt;AWS&lt;/span&gt; and the corporate data center can be worked around assuming there is an alternate path of access (such as
  a bastion host that allows logins from engineers/managers from a trusted outside&amp;nbsp;host).&lt;/li&gt;
&lt;li&gt;Management of access can be delegated to application owners/managers, while still allowing engineers full&amp;nbsp;access.&lt;/li&gt;
&lt;li&gt;Uses the strength of public key authentication; no passwords to&amp;nbsp;change.&lt;/li&gt;
&lt;li&gt;Ensures that select static trusted keys always have access to instances, even during a failure of the key distribution&amp;nbsp;system.&lt;/li&gt;
&lt;li&gt;In emergencies, keys could be distributed directly to the authorized_keys file, bypassing the distribution system,
  or key file cache lifetime could be&amp;nbsp;increased.&lt;/li&gt;
&lt;li&gt;Can be easily audited by having a scheduled job add a key for all instances, wait ~15 minutes, and then attempt &lt;span class="caps"&gt;SSH&lt;/span&gt;
  connections to all&amp;nbsp;instances.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="trade-offs"&gt;Trade-Offs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Delay between user access addition/removal and updates (though this can be minimized by a shorter cache&amp;nbsp;time).&lt;/li&gt;
&lt;li&gt;Latency during initial login with a cold&amp;nbsp;cache.&lt;/li&gt;
&lt;li&gt;Addition of another system that could&amp;nbsp;break.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="limitations"&gt;Limitations&lt;/h2&gt;
&lt;p&gt;My company is a CentOS shop. The &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt; feature of OpenSSH itself was only released in &lt;a href="http://www.openssh.com/txt/release-6.2"&gt;OpenSSH 6.2&lt;/a&gt;,
on March 22, 2013. A patch for it was backported to the 5.3p1 version of openssh-server in &lt;span class="caps"&gt;RHEL&lt;/span&gt; and CentOS 6. However,
this method will certainly not work on CentOS 5, which is still running OpenSSH 4.3. Be aware that when the new &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;
feature was backported, the man page was not updated; &lt;code&gt;man sshd_config&lt;/code&gt; is still conspicuously missing these options, and I couldn&amp;#8217;t
find anything in the &lt;span class="caps"&gt;RPM&lt;/span&gt; changelog about it, but the &lt;code&gt;openssh-5.3p1-authorized-keys-command.patch&lt;/code&gt; file is clearly there in the
5.3p1 &lt;span class="caps"&gt;SRPM&lt;/span&gt;, and the options are there but commented out in the &lt;code&gt;sshd_config&lt;/code&gt; it provides. I actually thought this would be near-impossible
to do on CentOS 6 until I found the &lt;code&gt;openssh-ldap&lt;/code&gt; package (in the default repos) and discovered that it uses this&amp;nbsp;feature.&lt;/p&gt;
&lt;p&gt;Also, this solution requires (depending on which alternative is chosen) working access to either S3 or instances serving an application.
Assuming proper configuration (and distribution across AZs) this should be a&amp;nbsp;non-issue.&lt;/p&gt;
&lt;h2 id="accountability"&gt;Accountability&lt;/h2&gt;
&lt;p&gt;If accountability is a concern, we will handle this through detailed logging in every step of the key creation, authorization, distribution
and retrieval process. In addition, all instances will run sshd with &lt;code&gt;LogLevel VERBOSE&lt;/code&gt;, which will log the fingerprint of all public keys
used to connect to the instance. Logs will be written to a secure, append-only&amp;nbsp;medium.&lt;/p&gt;
&lt;h1 id="references-and-further-details"&gt;References and Further&amp;nbsp;Details&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;There is an existing &lt;code&gt;openssh-ldap&lt;/code&gt; package in CentOS that provides instructions on setting up public key storage in an &lt;span class="caps"&gt;LDAP&lt;/span&gt; backend,
  using &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://andriigrytsenko.net/2013/05/authorizedkeyscommand-support-and-centosrhel-5-x/"&gt;Someone said&lt;/a&gt; they successfully built the current
  6.2 OpenSSH for &lt;span class="caps"&gt;RHEL&lt;/span&gt;/Cent&amp;nbsp;5.&lt;/li&gt;
&lt;li&gt;An &lt;span class="caps"&gt;EC2&lt;/span&gt; instance can retrieve its own tags using tools such as &lt;code&gt;awscli&lt;/code&gt; or &lt;code&gt;ec2-api-tools&lt;/code&gt; and an appropriate &lt;span class="caps"&gt;IAM&lt;/span&gt; role set on the&amp;nbsp;instance.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id="rejected-ideas"&gt;Rejected&amp;nbsp;Ideas&lt;/h1&gt;
&lt;p&gt;While thinking through this I considered and rejected a number of alternate methods. Here are some of&amp;nbsp;them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;While &lt;span class="caps"&gt;SSH&lt;/span&gt;&amp;#8217;s relatively new Certificate support (&lt;span class="caps"&gt;CA&lt;/span&gt;-based) sounds nice, it doesn&amp;#8217;t solve the problem; according to
  &lt;a href="http://neocri.me/documentation/using-ssh-certificate-authentication/"&gt;this blog post&lt;/a&gt; it uses a &lt;span class="caps"&gt;CA&lt;/span&gt; to sign keys,
  but doesn&amp;#8217;t do a &lt;span class="caps"&gt;CRL&lt;/span&gt; lookup, it relies on a RevokedKeys file manually sync&amp;#8217;ed to all servers. So, this poses the
  same problem as managing authorized_keys as a file distributed to&amp;nbsp;instances.&lt;/li&gt;
&lt;li&gt;Managing per-application users or groups on the &lt;span class="caps"&gt;AWS&lt;/span&gt; bastion hosts requires a lot of administrative overhead, and isn&amp;#8217;t really an option for us.
  Though this would be a simple implementation using either groups for each application with private keys group-readable,
  or using per-application users and the proper sudo&amp;nbsp;configuration.&lt;/li&gt;
&lt;li&gt;Prior to finding out about &lt;code&gt;AuthorizedKeysCommand&lt;/code&gt;, my top idea was essentially this same implementation on the
  key distribution server side, but writing it to an S3 bucket, and running a cronjob on each &lt;span class="caps"&gt;EC2&lt;/span&gt; instance to pull
  down the authorized_keys&amp;nbsp;file.&lt;/li&gt;
&lt;li&gt;Just Don&amp;#8217;t - See &lt;a href="https://wblinks.com/notes/aws-tips-i-wish-id-known-before-i-started/"&gt;this blog post&lt;/a&gt;
  as a reference. But the gist is, &amp;#8220;If you have to &lt;span class="caps"&gt;SSH&lt;/span&gt; into your servers, then your automation has failed&amp;#8221;.
  Sure, development and test stacks will be spun up, probably with either a single user&amp;#8217;s key, or a shared
  key. But after that (i.e. in prod), instances are cattle. Logs should be shipped to a central store, CloudWatch
  and/or other monitoring technologies (i.e. NewRelic, Diamond to graphite) should get most of the data that&amp;#8217;s
  needed. I&amp;#8217;m not seriously agreeing to &lt;strong&gt;disable&lt;/strong&gt; &lt;span class="caps"&gt;SSH&lt;/span&gt; access, but to put in place the tools that it&amp;#8217;s needed
  so rarely (on non-dev instances) that it&amp;#8217;s feasible to ask one of a small group of privileged people to
  perform the&amp;nbsp;task.&lt;/li&gt;
&lt;li&gt;Trust our users - If someone can push to master, full control of our systems is just a backtick (or popen) away.
  Recognize that if someone wasn&amp;#8217;t trustworthy, we wouldn&amp;#8217;t hire them. Let everyone access a single bastion host.
  Discourage unauthorized use via strong password policies and other standard security measures
  (perhaps &lt;span class="caps"&gt;OTP&lt;/span&gt;-based two-factor authentication). Discourage malicious use via detailed audit logging, with logs
  shipped to an append-only secure storage&amp;nbsp;location.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;SUID&lt;/span&gt; wrapper script - All users have &lt;span class="caps"&gt;SSH&lt;/span&gt; access to a bastion host as their normal
  active directory user. They run a &lt;span class="caps"&gt;SUID&lt;/span&gt; wrapper script that has a list of which users are allowed to access
  which &lt;span class="caps"&gt;EC2&lt;/span&gt; instances (or security groups, subnets, etc). When the user calls this script, it checks if the
  specified host is in a group they&amp;#8217;re allowed to access, and if so, SSHes to that host using a key only readable
  by the owner of the script. This is somewhat complex; there&amp;#8217;s a good possibility of security issues with the
  script itself, and it means that we&amp;#8217;re probably only allowing interactive logins - we&amp;#8217;re limited by the
  capabilities of the wrapper script, it&amp;#8217;s not just a normal &lt;span class="caps"&gt;SSH&lt;/span&gt;&amp;nbsp;client.&lt;/li&gt;
&lt;li&gt;Key Pushing- A script runs in one central location. It has a mapping of which users/groups are allowed
  to access which &lt;span class="caps"&gt;EC2&lt;/span&gt; instances. Every X minutes the script runs. It grabs &lt;code&gt;~/.ssh/authorized_keys&lt;/code&gt; for all
  users that are allowed &lt;span class="caps"&gt;EC2&lt;/span&gt; access, and then generates an authorized_keys file for each group of instances.
  The script checks a cache, and if the file has changed for a group of instances since the last run, it queries
  the &lt;span class="caps"&gt;AWS&lt;/span&gt; &lt;span class="caps"&gt;API&lt;/span&gt; to determine which instances are in that group, and distributes the authorized_keys file to them.
  The &amp;#8220;distributes&amp;#8221; part would, unfortunately, probably have to be&amp;nbsp;scp.&lt;/li&gt;
&lt;li&gt;Bastion host per application. Users are allowed access to this host either via authorized_keys managed by Puppet,
  or via sudoers rules on a bastion host in the corporate network. But yeah, we&amp;#8217;d end up with a &lt;strong&gt;lot&lt;/strong&gt; of&amp;nbsp;these.&lt;/li&gt;
&lt;li&gt;Various thoughts around &lt;span class="caps"&gt;AD&lt;/span&gt; in the cloud, replicated &lt;span class="caps"&gt;AD&lt;/span&gt; in the cloud, OpenLDAP in the cloud pulling from &lt;span class="caps"&gt;AD&lt;/span&gt;, or
  &lt;span class="caps"&gt;AD&lt;/span&gt; over &lt;span class="caps"&gt;VPN&lt;/span&gt;. These were all rejected either because of corporate security policies, or because relying on internal
  &lt;span class="caps"&gt;AD&lt;/span&gt; for authentication would mean that a data center or connectivity failure also affects&amp;nbsp;&lt;span class="caps"&gt;AWS&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;Puppet - We actually &lt;em&gt;run&lt;/em&gt; puppet on every instance. Maybe against our master, maybe masterless with a script
  to deploy some modules before every run. At a minimum, it manages ssh authorized keys for ec2_user. We implement
  some method where each user has a manifest with their own public keys, that they can maintain. Managers can add users
  to the group(s) for their applications, and that users&amp;#8217; keys are automatically deployed. Revoking keys, on the other
  hand, is a bigger problem. This requires some sort of &amp;#8220;this person is going away&amp;#8221; procedure, which currently doesn&amp;#8217;t
  exist (or involve the groups who maintain &lt;span class="caps"&gt;AWS&lt;/span&gt; infrastructure), and would be one more thing for a human to forget.
  There are also instances that have &amp;#8220;special stuff&amp;#8221; going on with Puppet that would complicate&amp;nbsp;this.&lt;/li&gt;
&lt;li&gt;Generate a list of authorized keys, turn it into a manifest, and run puppet masterless on it via a cronjob (pulling
  the manifest from S3). This involves most of the same problems as above, plus means that we have Puppet running
  in two different ways on some instances (triggered via mco against a master, and cron&amp;#8217;ed in apply&amp;nbsp;mode).&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 04 Oct 2014 11:59:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2014-10-04:2014/10/managing-ec2-ssh-keys-an-idea/</guid><category>ssh</category><category>ec2</category><category>aws</category><category>keys</category><category>public key</category><category>pubkey</category></item><item><title>Pushover Notifications for Shell Command Completion andÂ Status</title><link>http://blog.jasonantman.com/2014/09/pushover-notifications-for-shell-command-completion-and-status/</link><description>&lt;p&gt;Lately I&amp;#8217;ve been doing a bunch of work with &lt;a href="http://www.packer.io/"&gt;packer&lt;/a&gt; building &lt;a href="http://www.vagrantup.com/"&gt;Vagrant&lt;/a&gt;
machine images, and using &lt;a href="http://serverspec.org/"&gt;serverspec&lt;/a&gt; to run automated acceptance tests on the images. Unfortunately,
this ends up being a ~40-minute cycle time for the full image to provision and test. So, lots of watching text slowly scroll
down a screen, and finding something else to do. It&amp;#8217;s the weekend; I want to get this project finished, but I&amp;#8217;ve got other
things to&amp;nbsp;do.&lt;/p&gt;
&lt;p&gt;So, I wrote a little bash wrapper around &lt;a href="https://github.com/jnwatts"&gt;jnwatts&amp;#8217;&lt;/a&gt;
&lt;a href="https://raw.githubusercontent.com/jnwatts/pushover.sh/master/pushover.sh"&gt;pushover.sh&lt;/a&gt;. Assuming wherever you put this
is in your path, simply prefix any command with &lt;code&gt;pushover&lt;/code&gt;, and you&amp;#8217;ll get a handy &lt;a href="https://pushover.net/"&gt;Pushover&lt;/a&gt;
notification when it completes, along with the exit status and some other useful&amp;nbsp;information.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="c"&gt;#&lt;/span&gt;
&lt;span class="c"&gt;# Notify command completion and exit status via pushover&lt;/span&gt;
&lt;span class="c"&gt;# uses pushover.sh from https://raw.githubusercontent.com/jnwatts/pushover.sh/master/pushover.sh&lt;/span&gt;
&lt;span class="c"&gt;#&lt;/span&gt;

&lt;span class="nv"&gt;&lt;span class="caps"&gt;APIKEY&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Your Pushover &lt;span class="caps"&gt;API&lt;/span&gt; Key Here&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;&lt;span class="caps"&gt;USERKEY&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Your Pushover User Key Here&amp;quot;&lt;/span&gt;

&lt;span class="nv"&gt;stime&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;date &lt;span class="s1"&gt;&amp;#39;+%s&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;$@&lt;/span&gt;
&lt;span class="nv"&gt;exitcode&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$?&lt;/span&gt;
&lt;span class="c"&gt;# timer&lt;/span&gt;
&lt;span class="nv"&gt;etime&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;date &lt;span class="s1"&gt;&amp;#39;+%s&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nv"&gt;dt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;etime &lt;span class="o"&gt;-&lt;/span&gt; stime&lt;span class="k"&gt;))&lt;/span&gt;
&lt;span class="nv"&gt;ds&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;dt &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="m"&gt;60&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt;
&lt;span class="nv"&gt;dm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;dt &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="m"&gt;60&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="m"&gt;60&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt;
&lt;span class="nv"&gt;dh&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$((&lt;/span&gt;dt &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="m"&gt;3600&lt;/span&gt;&lt;span class="k"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;times&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;printf&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;%d:%02d:%02d&amp;#39;&lt;/span&gt; &lt;span class="nv"&gt;$dh&lt;/span&gt; &lt;span class="nv"&gt;$dm&lt;/span&gt; &lt;span class="nv"&gt;$ds&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="c"&gt;# end timer&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$exitcode&amp;quot;&lt;/span&gt; -eq 0 &lt;span class="o"&gt;]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;pushover.sh -p 0 -t &lt;span class="s2"&gt;&amp;quot;Command Succeeded&amp;quot;&lt;/span&gt; -T &lt;span class="s2"&gt;&amp;quot;$&lt;span class="caps"&gt;APIKEY&lt;/span&gt;&amp;quot;&lt;/span&gt; -U &lt;span class="s2"&gt;&amp;quot;$&lt;span class="caps"&gt;USERKEY&lt;/span&gt;&amp;quot;&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;succeeded in ${times} on $(hostname): $@ (in $(pwd))&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;(sent pushover success notification)&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;pushover.sh -p 0 -s falling -t &lt;span class="s2"&gt;&amp;quot;Command Failed&amp;quot;&lt;/span&gt; -T &lt;span class="s2"&gt;&amp;quot;$&lt;span class="caps"&gt;APIKEY&lt;/span&gt;&amp;quot;&lt;/span&gt; -U &lt;span class="s2"&gt;&amp;quot;$&lt;span class="caps"&gt;USERKEY&lt;/span&gt;&amp;quot;&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;failed in ${times} (exit $exitcode) on $(hostname): $@ (in $(pwd))&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;(sent pushover failure notification)&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, for example, a failing spec&amp;nbsp;test:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;jantman@phoenix:pts/4:~/CMG/git/puppet-cm (AUTO-415=)$ pushover bundle exec rake spec
&amp;lt;lots of failing spec output that exits non-0 after 1 minute 10 seconds&amp;gt;
(sent pushover failure notification)
jantman@phoenix:pts/4:~/CMG/git/puppet-cm (AUTO-415=)$
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Would send me a handy pushover message when it&amp;nbsp;finishes:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;Command Failed
failed in 0:01:10 (exit 1) on phoenix: bundle exec rake spec (in /home/jantman/CMG/git/puppet-cm)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hopefully this is useful to someone else as&amp;nbsp;well&amp;#8230;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 27 Sep 2014 21:20:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2014-09-27:2014/09/pushover-notifications-for-shell-command-completion-and-status/</guid><category>pushover</category><category>shell</category><category>notifications</category></item><item><title>Session Save and Restore with Bash and GNUÂ Screen</title><link>http://blog.jasonantman.com/2014/07/session-save-and-restore-with-bash-and-gnu-screen/</link><description>&lt;p&gt;I&amp;#8217;ve been using &lt;a href="http://www.gnu.org/software/screen/"&gt;&lt;span class="caps"&gt;GNU&lt;/span&gt; Screen&lt;/a&gt; for a very long time; I pretty much do &lt;em&gt;all&lt;/em&gt; of my
daily work in it. I have long-lived screen sessions pretty much everywhere; at any given time, I&amp;#8217;ve got a session running
on my desktop (that probably has 19 windows open and active) and a few on various remote hosts. I also have a really
bad habit of using screen windows to hold work in progress, things that I need to revisit, and what I want to do
next. This isn&amp;#8217;t as big of a deal on boxes in a datacenter that rarely go down, but my home desktop ends up getting
rebooted every few weeks (and not always at planned&amp;nbsp;times).&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Warning&lt;/strong&gt; - what I&amp;#8217;m about to describe is, really, a fragile and somewhat ugly hack. I&amp;#8217;m pretty sure that if I took
the time to learn and switch to zsh (or another, more modern shell) and tmux, I could probably do this easier. But my
shell environment is something I&amp;#8217;m pretty stuck in. So, if this is useful to anyone else, cool. But caveat&amp;nbsp;emptor.&lt;/p&gt;
&lt;p&gt;screen 4.2.0 introduced some extensions to the &lt;code&gt;-Q&lt;/code&gt; remote querying capabilities, including the ability to retrieve a
list of current windows and their titles via &lt;code&gt;screen -Q windows&lt;/code&gt;. A few months ago, I wrapped a python script around
this that reads the currently open windows along with their title and window number, and writes out &lt;code&gt;~/.screenrc.save&lt;/code&gt;
that&amp;#8217;s &lt;code&gt;~/.screenrc&lt;/code&gt; with &lt;code&gt;screen -t&lt;/code&gt; lines to recreate my currently open windows with their titles. After a system
crash or reboot, I could &lt;code&gt;screen -c ~/.screenrc.save&lt;/code&gt; and get all of my windows and their titles back. So, that&amp;#8217;s
a slightly better reminder of what I was working on assuming I keep my titles relevant. But each window just dumped
me into &lt;code&gt;~/&lt;/code&gt; like usual, so I&amp;#8217;d just have the window title to remind me what I was working&amp;nbsp;on. &lt;/p&gt;
&lt;p&gt;I ran this script for a few months; you can see the original version &lt;a href="https://github.com/jantman/misc-scripts/blob/ab6a14774d5dd6250aac98f804c33d3dc26a32eb/savescreen.py"&gt;here&lt;/a&gt;.
However, this still really isn&amp;#8217;t what I&amp;#8217;d call &amp;#8220;session restore&amp;#8221;. I had window titles as &amp;#8220;hints&amp;#8221; to what I was doing,
but everything else was left to my&amp;nbsp;memory.&lt;/p&gt;
&lt;p&gt;Enter some awful &lt;code&gt;bashrc&lt;/code&gt; hackery. Please note that my bashrc is a bit complicated, mainly due to git completion
and getting a proper prompt for python virtualenvs, but here&amp;#8217;s the magic&amp;nbsp;portion:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="c"&gt;# git prompt - make it work everywhere&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -e /usr/share/git/completion/git-prompt.sh &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;source&lt;/span&gt; /usr/share/git/completion/git-prompt.sh
&lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -e /usr/share/git-core/contrib/completion/git-prompt.sh &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;source&lt;/span&gt; /usr/share/git-core/contrib/completion/git-prompt.sh
&lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -e ~/bin/git-prompt.sh &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;source&lt;/span&gt; ~/bin/git-prompt.sh
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="c"&gt;#set the &lt;span class="caps"&gt;PROMPT&lt;/span&gt;&lt;/span&gt;
&lt;span class="nv"&gt;cur_tty&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nv"&gt;temp&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;tty&lt;span class="k"&gt;)&lt;/span&gt; ; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="k"&gt;${&lt;/span&gt;&lt;span class="nv"&gt;temp&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="nv"&gt;5&lt;/span&gt;&lt;span class="k"&gt;})&lt;/span&gt;;
&lt;span class="c"&gt;# git prompt configutation&lt;/span&gt;
&lt;span class="nv"&gt;GIT_PS1_SHOWDIRTYSTATE&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1
&lt;span class="nv"&gt;GIT_PS1_SHOWUNTRACKEDFILES&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1
&lt;span class="nv"&gt;GIT_PS1_SHOWUPSTREAM&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;auto&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;GIT_PS1_SHOWCOLORHINTS&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;1

&lt;span class="c"&gt;# for screen session-saving hack, set per-window history file if in screen&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;$&lt;span class="caps"&gt;STY&lt;/span&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;$&lt;span class="caps"&gt;WINDOW&lt;/span&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;&lt;span class="caps"&gt;HISTFILE&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;readlink -f ~/.screenhist/&lt;span class="nv"&gt;$&lt;span class="caps"&gt;WINDOW&lt;/span&gt;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;shopt&lt;/span&gt; -s histappend

&lt;span class="c"&gt;# make sure our screen session-saving hack directories exist&lt;/span&gt;
&lt;span class="o"&gt;[[&lt;/span&gt; -d ~/.screenhist &lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; mkdir ~/.screenhist
&lt;span class="o"&gt;[[&lt;/span&gt; -d ~/.screendirs &lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; mkdir ~/.screendirs

__wrap_git_ps1 &lt;span class="o"&gt;()&lt;/span&gt;
&lt;span class="o"&gt;{&lt;/span&gt;
    &lt;span class="c"&gt;# commands here now get executed every time bash constructs a prompt&lt;/span&gt;
    &lt;span class="c"&gt;# for screen pwd saving&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;$&lt;span class="caps"&gt;STY&lt;/span&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;$&lt;span class="caps"&gt;WINDOW&lt;/span&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;
    &lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;        &lt;/span&gt;&lt;span class="nv"&gt;&lt;span class="caps"&gt;SCREENLINKDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;readlink -f ~/.screendirs&lt;span class="k"&gt;)&lt;/span&gt;
        rm -f &lt;span class="nv"&gt;$&lt;span class="caps"&gt;SCREENLINKDIR&lt;/span&gt;&lt;/span&gt;/&lt;span class="nv"&gt;$&lt;span class="caps"&gt;WINDOW&lt;/span&gt;&lt;/span&gt;
        ln -sf &lt;span class="k"&gt;$(&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;/ &lt;span class="nv"&gt;$&lt;span class="caps"&gt;SCREENLINKDIR&lt;/span&gt;&lt;/span&gt;/&lt;span class="nv"&gt;$&lt;span class="caps"&gt;WINDOW&lt;/span&gt;&lt;/span&gt;
    &lt;span class="k"&gt;fi&lt;/span&gt;
    &lt;span class="c"&gt;# virtualenv stuff for prompt&lt;/span&gt;
    &lt;span class="nv"&gt;venv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&amp;#39;&lt;/span&gt;
    &lt;span class="o"&gt;[[&lt;/span&gt; &lt;span class="nv"&gt;$VIRTUAL_ENV&lt;/span&gt; !&lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nv"&gt;venv&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;\[\033[31m\](${VIRTUAL_ENV##*/})\e[0m&amp;quot;&lt;/span&gt;
    __git_ps1 &lt;span class="s2"&gt;&amp;quot;$venv\u@\h:$cur_tty:\w&amp;quot;&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;\\\$ &amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;history&lt;/span&gt; -a
&lt;span class="o"&gt;}&lt;/span&gt;
&lt;span class="nv"&gt;PROMPT_COMMAND&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;__wrap_git_ps1&amp;#39;&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;&lt;span class="caps"&gt;PS2&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;gt; &amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So&amp;#8230; the hack. First we source the git prompt scripts that come with git (trying the
locations they should be at on all of the machines I commonly use, and if it can&amp;#8217;t find
any of them, falling back to a copy in my homedir) and set some configuration variables
for them (as well as capturing the current tty). We then (conditionally on being inside
a screen window) set our history file to a per-screen-window path, and have history append.
At this point we also make sure some directories we&amp;#8217;ll use&amp;nbsp;exist.&lt;/p&gt;
&lt;p&gt;Now the real fun. &lt;code&gt;PROMPT_COMMAND&lt;/code&gt; specifies a function for bash to execute to build the
prompt string; this is called every time bash needs to display the prompt (so, effectively,
every time a command completes in the shell). We set it to &lt;code&gt;__wrap_git_ps1&lt;/code&gt;, a function we
just defined. The magic happens in this function. Screen sets some environment variables
inside each window, including &lt;code&gt;STY&lt;/code&gt; (the name of the screen session you&amp;#8217;re in) and
&lt;code&gt;WINDOW&lt;/code&gt;, the current window number. If both of these are set, we symlink our current
&lt;code&gt;pwd&lt;/code&gt; to &lt;code&gt;~/.screendirs/$WINDOW&lt;/code&gt; (note some hackery, explicitly removing the link if it
already exists, to get this to work correctly). We then throw in some python virtualenv-specific
prompt settings, and pass on the strings we&amp;#8217;ve constructed to &lt;code&gt;__git_ps1&lt;/code&gt; which adds the
git-specific information, and then sets &lt;code&gt;PS1&lt;/code&gt; correctly. Finally, we explicitly append to
current history, to make sure the history on disk is always accurate and&amp;nbsp;up-to-date.&lt;/p&gt;
&lt;p&gt;This works in combination with the &lt;a href="https://github.com/jantman/misc-scripts/blob/master/savescreen.py"&gt;latest version&lt;/a&gt;
of savescreen.py, which has some minor changes. The line to create each window,&amp;nbsp;formerly:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;screen -t &lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;{name}&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s"&gt; {num}&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;windows&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;becomes:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;fh&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;screen -t &lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;{name}&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s"&gt; {num} sh -c &lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;&lt;/span&gt;&lt;span class="s"&gt;cd $(readlink -fn {dirpath}/{num}); bash&lt;/span&gt;&lt;span class="se"&gt;\&amp;quot;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;format&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;windows&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;num&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;n&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dirpath&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;dirpath&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When each window is created at startup, we &lt;code&gt;cd&lt;/code&gt; into the previous &lt;code&gt;pwd&lt;/code&gt; (the path
that the &lt;code&gt;~/.screendirs/$WINDOW&lt;/code&gt; symlink, created by bashrc, points to) and then
call our shell. When this is combined with the &lt;code&gt;HISTFILE&lt;/code&gt; change, the effect is that
&lt;code&gt;screen -c ~/.screenrc.save&lt;/code&gt; brings us back into a screen session that has not only
all of our previous windows and their titles, but also a shell in each window&amp;#8217;s previous
working directory, and that window&amp;#8217;s&amp;nbsp;history.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Limitations&lt;/strong&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I should&amp;#8217;ve also used &lt;code&gt;$STY&lt;/code&gt; in each of the paths, so this would be multi-session-safe.
   I didn&amp;#8217;t, so this has undefined behavior if more than one screen session is running as
   your&amp;nbsp;user.&lt;/li&gt;
&lt;li&gt;A lot of this is lost, obviously, if you &lt;code&gt;sudo su&lt;/code&gt; or &lt;code&gt;ssh&lt;/code&gt;, or in any other way end up
   as a different&amp;nbsp;user.&lt;/li&gt;
&lt;li&gt;I&amp;#8217;m thinking about rolling in some method of automatic &lt;code&gt;virtualenv&lt;/code&gt; activation (since it,
   unfortunately, doesn&amp;#8217;t have anything like &lt;code&gt;.rvmrc&lt;/code&gt;). Maybe in the next&amp;nbsp;version.&lt;/li&gt;
&lt;/ol&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Fri, 25 Jul 2014 10:09:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2014-07-25:2014/07/session-save-and-restore-with-bash-and-gnu-screen/</guid><category>bash</category><category>screen</category><category>restore</category><category>bashrc</category></item><item><title>How Yum and RPM CompareÂ Versions</title><link>http://blog.jasonantman.com/2014/07/how-yum-and-rpm-compare-versions/</link><description>&lt;p&gt;I was recently tripped up by a bug in Puppet, &lt;a href="https://tickets.puppetlabs.com/browse/PUP-1244"&gt;&lt;span class="caps"&gt;PUP&lt;/span&gt;-1244&lt;/a&gt;,
dealing with how it compares package versions. All of Puppet&amp;#8217;s Package types assumed
&lt;a href="http://semver.org/"&gt;semantic versioning&lt;/a&gt;, and that&amp;#8217;s far from the case for RPMs and therefore Yum. This
manifested itself in how Puppet validates package installations - if a version was explicitly specified
and Yum/&lt;span class="caps"&gt;RPM&lt;/span&gt; could install it, puppet would shell out to them and install the package, but then report
a failure in its post-install validation, as the &lt;em&gt;exact&lt;/em&gt; version string specified isn&amp;#8217;t present.
For example, many RedHat/CentOS packages (such as those from &lt;span class="caps"&gt;EPEL&lt;/span&gt;) include a release string with the major
version of the distribution they were packged for - i.e. &amp;#8220;.el5&amp;#8221; or &amp;#8220;.el6&amp;#8221;. If Puppet was instructed to
install package &amp;#8220;foo&amp;#8221; version &amp;#8220;1.2.3&amp;#8221;, but the actual package in the repositories was &amp;#8220;foo-1.2.3-el5&amp;#8221;,
Puppet would cause the package to be installed, but then report&amp;nbsp;failure.&lt;/p&gt;
&lt;p&gt;I cut a &lt;a href="https://github.com/puppetlabs/puppet/pull/2866"&gt;pull request&lt;/a&gt; against the Puppet4 branch to
fix these issues, essentially re-implementing yum and rpm&amp;#8217;s version comparison logic in Ruby. It took
me a few days of research and sorting through source code (and in the process I found that &lt;code&gt;yum&lt;/code&gt;, despite
its use in so many distributions, has no unit tests at all) but I finally got it finished. In the process,
I found out exactly how&amp;#8230; weird&amp;#8230; &lt;span class="caps"&gt;RPM&lt;/span&gt;&amp;#8217;s version comparison rules&amp;nbsp;are.&lt;/p&gt;
&lt;h3 id="package-naming-and-parsing"&gt;Package Naming and&amp;nbsp;Parsing&lt;/h3&gt;
&lt;p&gt;&lt;span class="caps"&gt;RPM&lt;/span&gt; package names are made up of five parts; the package name, epoch, version, release, and architecture.
This format is commonly referred to as the acronym &lt;span class="caps"&gt;NEVRA&lt;/span&gt;. The epoch is not always included; it is assumed
to be zero (0) on any packages that lack it explicitly. The format for the whole string is &lt;code&gt;n-e:v-r.a&lt;/code&gt;.
For my purposes, I was only really concerned with comparing the &lt;span class="caps"&gt;EVR&lt;/span&gt; portion; Puppet knows about package names
and the bug herein was with what Puppet calls the &amp;#8220;version&amp;#8221; (&lt;span class="caps"&gt;EVR&lt;/span&gt; in yum/rpm parlance). Parsing is pretty&amp;nbsp;simple:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If there is a &lt;code&gt;:&lt;/code&gt; in the string, everything before it is the epoch. If not, the epoch is&amp;nbsp;zero.&lt;/li&gt;
&lt;li&gt;If there is a &lt;code&gt;-&lt;/code&gt; in the &lt;em&gt;remaining&lt;/em&gt; string, everything before the first &lt;code&gt;-&lt;/code&gt; is the version,
  and everything after it is the release. If there isn&amp;#8217;t one, the release is considered&amp;nbsp;null/nill/None/whatever.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="how-yum-compares-evr"&gt;How Yum Compares&amp;nbsp;&lt;span class="caps"&gt;EVR&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Once the package string is parsed into its &lt;span class="caps"&gt;EVR&lt;/span&gt; components, yum calls &lt;code&gt;rpmUtils.miscutils.compareEVR()&lt;/code&gt;,
which does some data type massaging for the inputs, and then calls out to &lt;code&gt;rpm.labelCompare()&lt;/code&gt;
(found in &lt;code&gt;rpm.git/python/header-py.c&lt;/code&gt;). &lt;code&gt;labelCompare()&lt;/code&gt; sets each epoch
to &amp;#8220;0&amp;#8221; if it was null/Nonem, and then uses &lt;code&gt;compare_values()&lt;/code&gt; to compare each &lt;span class="caps"&gt;EVR&lt;/span&gt; portion, which in turn falls through
to a function called &lt;code&gt;rpmvercmp()&lt;/code&gt; (see below). The algorithm for &lt;code&gt;labelCompare()&lt;/code&gt; is as&amp;nbsp;follows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Set each epoch value to 0 if it&amp;#8217;s&amp;nbsp;null/None.&lt;/li&gt;
&lt;li&gt;Compare the epoch values using &lt;code&gt;compare_values()&lt;/code&gt;. If they&amp;#8217;re not equal, return that result, else
   move on to the next portion (version). The logic within &lt;code&gt;compare_values()&lt;/code&gt; is that if one is empty/null
   and the other is not, the non-empty one is greater, and that ends the comparison. If neither of
   them is empty/not present, compare them using &lt;code&gt;rpmvercmp()&lt;/code&gt; and follow the same logic; if one
   is &amp;#8220;greater&amp;#8221; (newer) than the other, that&amp;#8217;s the end result of the comparison. Otherwise, move
   on to the next component&amp;nbsp;(version).&lt;/li&gt;
&lt;li&gt;Compare the versions using the same&amp;nbsp;logic.&lt;/li&gt;
&lt;li&gt;Compare the releases using the same&amp;nbsp;logic.&lt;/li&gt;
&lt;li&gt;If all of the components are &amp;#8220;equal&amp;#8221;, the packages are the&amp;nbsp;same.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The real magic, obviously, happens in &lt;code&gt;rpmvercmp()&lt;/code&gt;, the rpm library function to compare two
versions (or epochs, or releases). That&amp;#8217;s also where the madness&amp;nbsp;happens.&lt;/p&gt;
&lt;h3 id="how-rpm-compares-version-parts"&gt;How &lt;span class="caps"&gt;RPM&lt;/span&gt; Compares Version&amp;nbsp;Parts&lt;/h3&gt;
&lt;p&gt;&lt;span class="caps"&gt;RPM&lt;/span&gt; is written in C. Converting all of the buffer and pointer processing for these strings
over to Ruby was quite a pain. That being said, I didn&amp;#8217;t make this up, this is actually the
algorithm that &lt;code&gt;rpmvercmp()&lt;/code&gt; (&lt;code&gt;lib/rpmvercmp.c&lt;/code&gt;) uses to compare version &amp;#8220;parts&amp;#8221;
(epoch, version, release). This function returns 0 if the strings are equal, 1 if &lt;code&gt;a&lt;/code&gt; (the
first string argument) is newer than &lt;code&gt;b&lt;/code&gt; (the second string argument), or -1 if
&lt;code&gt;a&lt;/code&gt; is older than &lt;code&gt;b&lt;/code&gt;. Also keep in mind that this uses pointers in C, so it works by removing
a sequence of 0 or more characters from the front of each string, comparing them, and then repeating
for the remaining characters in each string until something is unequal, or a string reaches its&amp;nbsp;end.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;If the strings are binary equal (&lt;code&gt;a == b&lt;/code&gt;), they&amp;#8217;re equal, return&amp;nbsp;0.&lt;/li&gt;
&lt;li&gt;Loop over the strings, left-to-right.&lt;ol&gt;
&lt;li&gt;Trim anything that&amp;#8217;s not &lt;code&gt;[A-Za-z0-9]&lt;/code&gt; or tilde (&lt;code&gt;~&lt;/code&gt;) from the front of both&amp;nbsp;strings.&lt;/li&gt;
&lt;li&gt;If both strings start with a tilde, discard it and move on to the next&amp;nbsp;character.&lt;/li&gt;
&lt;li&gt;If string &lt;code&gt;a&lt;/code&gt; starts with a tilde and string &lt;code&gt;b&lt;/code&gt; does not, return -1 (string &lt;code&gt;a&lt;/code&gt; is older);
   and the inverse if string &lt;code&gt;b&lt;/code&gt; starts with a tilde and string &lt;code&gt;a&lt;/code&gt; does&amp;nbsp;not.&lt;/li&gt;
&lt;li&gt;End the loop if either string has reached zero&amp;nbsp;length.&lt;/li&gt;
&lt;li&gt;If the first character of &lt;code&gt;a&lt;/code&gt; is a digit, pop the leading chunk of continuous digits from
   each string (which may be &amp;#8221; for &lt;code&gt;b&lt;/code&gt; if only one &lt;code&gt;a&lt;/code&gt; starts with digits). If &lt;code&gt;a&lt;/code&gt; begins
   with a letter, do the same for leading&amp;nbsp;letters.&lt;/li&gt;
&lt;li&gt;If the segement from &lt;code&gt;b&lt;/code&gt; had 0 length, return ` if the segment from &lt;code&gt;a&lt;/code&gt; was numeric, or
   &lt;code&gt;b&lt;/code&gt; if it was alphabetic. The logical result of this is that if &lt;code&gt;a&lt;/code&gt; begins with numbers
   and &lt;code&gt;b&lt;/code&gt; does not, &lt;code&gt;a&lt;/code&gt; is newer (return 1). If &lt;code&gt;a&lt;/code&gt; begins with letters and &lt;code&gt;b&lt;/code&gt; does not,
   then &lt;code&gt;a&lt;/code&gt; is older (return -1). If the leading character(s) from &lt;code&gt;a&lt;/code&gt; and &lt;code&gt;b&lt;/code&gt; were both
   numbers or both letters, continue&amp;nbsp;on.&lt;/li&gt;
&lt;li&gt;If the leading segments were both numeric, discard any leading zeros and &lt;em&gt;whichever one is longer
   wins&lt;/em&gt;. If &lt;code&gt;a&lt;/code&gt; is longer than &lt;code&gt;b&lt;/code&gt; (without leading zeroes), return 1, and vice-versa. If
   they&amp;#8217;re of the same length, continue&amp;nbsp;on.&lt;/li&gt;
&lt;li&gt;Compare the leading segments with &lt;code&gt;strcmp()&lt;/code&gt; (or &lt;code&gt;&amp;lt;=&amp;gt;&lt;/code&gt; in Ruby). If that returns a non-zero
   value, then return that value. Else continue to the next iteration of the&amp;nbsp;loop.&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;If the loop ended (nothing has been returned yet, either both strings are totally the same or they&amp;#8217;re
   the same up to the end of one of them, like with &amp;#8220;1.2.3&amp;#8221; and &amp;#8220;1.2.3b&amp;#8221;), then the longest wins -
   if what&amp;#8217;s left of &lt;code&gt;a&lt;/code&gt; is longer than what&amp;#8217;s left of &lt;code&gt;b&lt;/code&gt;, return 1. Vice-versa for if what&amp;#8217;s
   left of &lt;code&gt;b&lt;/code&gt; is longer than what&amp;#8217;s left of &lt;code&gt;a&lt;/code&gt;. And finally, if what&amp;#8217;s left of them is the same
   length, return&amp;nbsp;0.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Well there you have it. Quite convoluted. And full of things like the &amp;#8220;~&amp;#8221; magic character (&amp;#8220;~1&amp;#8221; is always
older than&amp;nbsp;&amp;#8220;9999zzzz&amp;#8221;).&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Fri, 11 Jul 2014 23:31:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2014-07-11:2014/07/how-yum-and-rpm-compare-versions/</guid><category>rpm</category><category>yum</category><category>puppet</category><category>versions</category></item><item><title>bashrc Vagrant / VirtualBoxÂ reminder</title><link>http://blog.jasonantman.com/2014/07/bashrc-vagrant-virtualbox-reminder/</link><description>&lt;p&gt;Lately I&amp;#8217;ve been using VirtualBox VMs, both managed by Vagrant and otherwise, quite a lot.
I&amp;#8217;ve also been doing a bunch of development work with them. And inevitably, I close a screen
window and fo on with my work and end up with a few &amp;#8220;orphaned&amp;#8221; virtualbox VMs running that
I&amp;#8217;ve forgotten&amp;nbsp;about.&lt;/p&gt;
&lt;p&gt;Below is the snippet I&amp;#8217;ve added to my &lt;code&gt;~/.bashrc&lt;/code&gt; to keep me aware of this situation. Unfortunately
the &lt;code&gt;vagrant global-status&lt;/code&gt; command is relatively slow, so this adds (on my machine) about
1.5 seconds of wall-clock time to my &lt;code&gt;.bashrc&lt;/code&gt; (hence the process check&amp;nbsp;first).&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="c"&gt;# Vagrant/VirtualBox reminder&lt;/span&gt;
&lt;span class="k"&gt;if &lt;/span&gt;pgrep VBoxHeadless &amp;amp;&amp;gt;/dev/null; &lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nv"&gt;vblist&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;VBoxManage list runningvms&lt;span class="k"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;[&lt;/span&gt; -n &lt;span class="s2"&gt;&amp;quot;${vblist}&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;\e[1;31mRunning VirtualBox VMs:\e[0m\n${vblist}\n&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if &lt;/span&gt;which vagrant &amp;amp;&amp;gt; /dev/null &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; vagrant &lt;span class="nb"&gt;help&lt;/span&gt; | grep -q global-status; &lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;        &lt;/span&gt;&lt;span class="nv"&gt;vagrantstatus&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="k"&gt;$(&lt;/span&gt;vagrant global-status | sed &lt;span class="s1"&gt;&amp;#39;/^\s*$/q&amp;#39;&lt;/span&gt;&lt;span class="k"&gt;)&lt;/span&gt;
        &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$vagrantstatus&amp;quot;&lt;/span&gt; | grep -q running &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;{&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; -e &lt;span class="s2"&gt;&amp;quot;\e[1;31mRunning Vagrant Machines:\e[0m&amp;quot;&lt;/span&gt; ; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$vagrantstatus&amp;quot;&lt;/span&gt; | head -2; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$vagrantstatus&amp;quot;&lt;/span&gt; | grep running; &lt;span class="o"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;fi&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Thu, 10 Jul 2014 06:45:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2014-07-10:2014/07/bashrc-vagrant-virtualbox-reminder/</guid><category>vagrant</category><category>bashrc</category><category>profile</category><category>virtualbox</category></item><item><title>Remotely-controlled deck.js slideÂ presentations</title><link>http://blog.jasonantman.com/2014/05/remotely-controlled-deckjs-slide-presentations/</link><description>&lt;p&gt;I&amp;#8217;ve been struggling to find a good, cross-platform remote meeting solution. We&amp;#8217;re using &lt;a href="http://www.imeet.com"&gt;iMeet&lt;/a&gt;
at work at the moment, but there&amp;#8217;s no way to present or screen share from a Linux machine. For most of our ops and
automation team daily and weekly meetings, we use &lt;a href="http://www.teamspeak.com/"&gt;TeamSpeak&lt;/a&gt; - sure it&amp;#8217;s not open source,
but it&amp;#8217;s simple, supports all OSes that matter to us (Mac, Linux, Windows, Android and iOS), can be self-hosted,
and has the holy grail, functional push-to-talk. But it&amp;#8217;s audio&amp;nbsp;only.&lt;/p&gt;
&lt;p&gt;On Friday I was running two short elaboration meetings, and had quick little slide decks done up in &lt;a href="http://imakewebthings.com/deck.js/"&gt;deck.js&lt;/a&gt;
to keep us on track. I couldn&amp;#8217;t help but think, gee, it sure would be nice if instead of switching to Mac or a &lt;span class="caps"&gt;VM&lt;/span&gt; and sharing my screen,
we could just use the audio communication mediums that we already do, and I could simply control the slides in a&amp;nbsp;browser.&lt;/p&gt;
&lt;p&gt;Well this morning I stumbled on &lt;a href="http://cleverchris.com/"&gt;Chris Jaure&lt;/a&gt;&amp;#8216;s &lt;a href="https://github.com/chrisjaure/deckjs-remote"&gt;deckjs-remote&lt;/a&gt;
project that does exactly that. It&amp;#8217;s a nodejs npm module that runs a websocket server, and allows people to join a session and follow
along as the presenter changes&amp;nbsp;slides.&lt;/p&gt;
&lt;p&gt;I did have a few hiccups getting it working - mainly some issues with &lt;span class="caps"&gt;CORS&lt;/span&gt;. The &lt;span class="caps"&gt;README&lt;/span&gt;.md has a large block of markup to be added to
the slide deck html to support &amp;#8220;older browsers that don&amp;#8217;t support &lt;span class="caps"&gt;CORS&lt;/span&gt;.&amp;#8221; I&amp;#8217;m running Firefox 28.0 (Firefox has supported &lt;span class="caps"&gt;CORS&lt;/span&gt; since
3.0, quite a few years back) and still needed to add this to get everything working. I also needed to manually add a script tag
for socket.io coming from the nodejs server in order to get everything&amp;nbsp;working.&lt;/p&gt;
&lt;p&gt;There&amp;#8217;s a bit of a delay for the socket connection to come up after initially loading the page, but once that&amp;#8217;s done, the presenter
(&amp;#8220;master&amp;#8221; session) should get the password prompt, and any guests should get a prompt asking if they want to join the current
session. Perhaps the best part is that the nodejs server interally stores each deck by &lt;span class="caps"&gt;URL&lt;/span&gt;, so it seems to work perfectly fine
when running one instance for N presenters (i.e. a single instance running persistently on a shared&amp;nbsp;server).&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Mon, 12 May 2014 09:34:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2014-05-12:2014/05/remotely-controlled-deckjs-slide-presentations/</guid><category>slide</category><category>presentation</category><category>deck.js</category><category>deckjs</category><category>javascript</category></item><item><title>dashsnap.py - A Script to Snapshot a GraphiteÂ Dashboard</title><link>http://blog.jasonantman.com/2014/05/dashsnap-a-script-to-snapshot-a-graphite-dashboard/</link><description>&lt;p&gt;As we push more and more and more metrics into &lt;a href="http://graphite.wikidot.com/"&gt;Graphite&lt;/a&gt;
at work, we&amp;#8217;ve found the need to preserve data from an incident or outage to be quite
important. Especially now that we&amp;#8217;re feeding a &lt;em&gt;lot&lt;/em&gt; of our data at 10-second intervals,
and our storage schemas generally start aggregating that past 24 hours (God only knows
how many spikes are gone if you look a week later), it&amp;#8217;s important to capture as much
data as we think we&amp;#8217;ll need as soon after the incident as&amp;nbsp;possible.&lt;/p&gt;
&lt;p&gt;To this end, a few days (and nights) into a relatively major crisis, I wrote a little
python script, &lt;a href="https://github.com/jantman/misc-scripts/blob/master/dashsnap.py"&gt;dashsnap.py&lt;/a&gt;.
It&amp;#8217;s horribly simple; pass it the hostname to your graphite server (if &amp;#8220;graphite&amp;#8221; doesn&amp;#8217;t
resolve to what you want), the name of a dashboard, optionally a height and width for images
(the default is currently 1024x768), and either a from and to date/time or a list of graphite
&lt;span class="caps"&gt;URL&lt;/span&gt;-style intervals (the default is a ginormous &amp;#8220;-10minutes,-30minutes,-1hours,-2hours,-4hours,-6hours,-12hours,-24hours,-36hours&amp;#8221;).
It will find all graphs on your dashboard, and locally save (in a horribly named directory)
both PNGs of all the graphs, as well as the &lt;em&gt;raw &lt;span class="caps"&gt;JSON&lt;/span&gt; data&lt;/em&gt; for them. It&amp;#8217;ll also write
(2 &lt;span class="caps"&gt;AM&lt;/span&gt;-simple) &lt;span class="caps"&gt;HTML&lt;/span&gt; index files to all of the intervals and graphs within&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s a view of the index page using the default&amp;nbsp;intervals:&lt;/p&gt;
&lt;p&gt;&lt;a href="/GFX/dashsnap_index.png"&gt;&lt;img alt="screenshot of rendered index page" src="/GFX/dashsnap_index_sm.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;And here&amp;#8217;s the page showing graphs and &lt;span class="caps"&gt;JSON&lt;/span&gt; links for an individual dashboard for one&amp;nbsp;interval:&lt;/p&gt;
&lt;p&gt;&lt;a href="/GFX/dashsnap_page.png"&gt;&lt;img alt="screenshot of one interval page" src="/GFX/dashsnap_page_sm.png" /&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ll quickly admit right now that this is alpha software, if you can even call it that.
I guess in reality it&amp;#8217;s a late-night fix to a problem that deserves more. But, if it can
save someone else a few hours late at night, it&amp;#8217;s worth mentioning. PRs are welcome, as
are issues and suggestions on GitHub for bugs, or for where I should take this; I like
the handy little &lt;span class="caps"&gt;CLI&lt;/span&gt; script (though the output could use quite a bit of visual work),
but I&amp;#8217;m also toying around with the idea of creating a service to take the snapshots
and store them, mostly thinking about it being part of something like
&lt;a href="https://github.com/etsy/morgue"&gt;Etsy&amp;#8217;s Morgue&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The latest version of the source for dashsnap will (within the forseeable future)
be available&amp;nbsp;at:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/jantman/misc-scripts/blob/master/dashsnap.py"&gt;https://github.com/jantman/misc-scripts/blob/master/dashsnap.py&lt;/a&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Wed, 07 May 2014 21:58:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2014-05-07:2014/05/dashsnap-a-script-to-snapshot-a-graphite-dashboard/</guid><category>graphite</category><category>monitoring</category></item><item><title>NSA TargetingÂ SysAdmins</title><link>http://blog.jasonantman.com/2014/03/nsa-targeting-sysadmins/</link><description>&lt;p&gt;I try not to rant too much, but I feel that this one was needed. If you&amp;#8217;re looking for
objective, technical information, skip this&amp;nbsp;one.&lt;/p&gt;
&lt;p&gt;When I was younger and more naive, I had near complete trust in the &lt;span class="caps"&gt;US&lt;/span&gt; government. There&amp;#8217;s a Constitution,
and they abide by it, and so do I. Since 2001, that feeling has eroded a bit. Since 2008, it&amp;#8217;s eroded even
more. But the documents leaked by &lt;a href="http://en.wikipedia.org/wiki/Edward_Snowden"&gt;Edward Snowden&lt;/a&gt; have been
the &amp;#8216;icing on the cake&amp;#8217;. I know there&amp;#8217;s disagreement about whether what he did was right or not - I&amp;#8217;m pretty
decided on how I feel, but I know many others who feel that he should be taken out and shot - but one thing
that&amp;#8217;s no longer deniable is, since he released those documents, it&amp;#8217;s come to light that the &lt;span class="caps"&gt;US&lt;/span&gt; government
is routinely performing unconstitutional acts in the furtherance of &amp;#8220;national&amp;nbsp;security.&amp;#8221;&lt;/p&gt;
&lt;p&gt;I feel like many people, who said over and over again that it could never get to this point, that the
Government might cut some corners but they&amp;#8217;d never blatantly do things like &lt;a href="http://www.washingtonpost.com/world/national-security/agencies-collected-data-on-americans-cellphone-use-in-thousands-of-tower-dumps/2013/12/08/20549190-5e80-11e3-be07-006c776266ed_story.html"&gt;mass collection of cellular data&lt;/a&gt;,
&lt;a href="http://www.washingtonpost.com/world/national-security/nsa-tracking-cellphone-locations-worldwide-snowden-documents-show/2013/12/04/5492873a-5cf2-11e3-bc56-c6ca94801fac_story.html"&gt;tracking the physical location of cell phones worldwide&lt;/a&gt;, or &lt;a href="http://www.washingtonpost.com/world/national-security/nsa-infiltrates-links-to-yahoo-google-data-centers-worldwide-snowden-documents-say/2013/10/30/e51d661e-4166-11e3-8b74-d89d714ca4dd_story.html"&gt;tapping into the networks of private companies like Google and Yahoo&lt;/a&gt;. It seems that
every new document leaked is worse than the last. I refuse to accept the arguments that this is all &amp;#8220;legal&amp;#8221;,
they simply don&amp;#8217;t keep up with the times. I couldn&amp;#8217;t care less if the government reads my postal mail (which
they still need an actual warrant for, &lt;span class="caps"&gt;AFAIK&lt;/span&gt;) - they probably get the same credit card and siding offers anyway.
I do, however, care that the government - out of their own self-interest and excitement at near-effortless surveillance -
refuses to extend the same protections that wireline phone calls and postal mail get to their electronic and
wireless&amp;nbsp;equivalents.&lt;/p&gt;
&lt;p&gt;When it came to light that the &lt;span class="caps"&gt;NSA&lt;/span&gt; &lt;a href="http://www.reuters.com/article/2013/12/20/us-usa-security-rsa-idUSBRE9BJ1C220131220"&gt;paid $10M to put a backdoor in &lt;span class="caps"&gt;RSA&lt;/span&gt; encryption&lt;/a&gt;
I was astonished. Late last year a number of other deals came to light, where the &lt;span class="caps"&gt;NSA&lt;/span&gt; paid or extorted software
and hardware manufacturers to intentionally introduce flaws in security to make it easier for the &lt;span class="caps"&gt;NSA&lt;/span&gt;
to gain access. The worst part is these weren&amp;#8217;t &amp;#8220;master passwords&amp;#8221; available only to the &lt;span class="caps"&gt;NSA&lt;/span&gt;; for the most
part, they appear to be mathematical flaws known to the &lt;span class="caps"&gt;NSA&lt;/span&gt;, but just as easily discovered by our enemies.
This part seems to have been glossed over by the media&amp;#8230; the consequences of some mathematician
or security researcher discovering those flaws and selling them to a national enemy or terrorist
would be flat-out devastating to the country and economy. Our government deliberately put a flaw in
an encryption standard, and then used its&amp;#8217; influence via &lt;span class="caps"&gt;NIST&lt;/span&gt;, the National Institute of Standards
and Technology, to &lt;a href="http://arstechnica.com/security/2013/09/the-nsas-work-to-make-crypto-worse-and-better/"&gt;recommend that standard for use&lt;/a&gt;
including by banks, e-commerce sites and financial&amp;nbsp;institutions.&lt;/p&gt;
&lt;p&gt;What has me even more upset, though, is the recent revelation that the &lt;span class="caps"&gt;NSA&lt;/span&gt; is &lt;a href="https://firstlook.org/theintercept/article/2014/03/20/inside-nsa-secret-efforts-hunt-hack-system-administrators/"&gt;systematically targeting
the private, personal accounts of system administrators to gain access to their employers&amp;#8217; networks&lt;/a&gt;. The bulk of the information came from an internal classified blog entry of an &lt;span class="caps"&gt;NSA&lt;/span&gt; employee,
&lt;a href="https://s3.amazonaws.com/s3.documentcloud.org/documents/1094387/i-hunt-sys-admins.pdf"&gt;available as a &lt;span class="caps"&gt;PDF&lt;/span&gt;&lt;/a&gt;
(or &lt;a href="/GFX/i-hunt-sys-admins.pdf"&gt;local copy&lt;/a&gt;). It&amp;#8217;s pretty technical, but it&amp;#8217;s also a startling view into
both the mindsets of &lt;em&gt;individuals&lt;/em&gt; within the &lt;span class="caps"&gt;NSA&lt;/span&gt;, and the organization&amp;#8217;s overall goals. Just two of the many
worthy&amp;nbsp;excerpts:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;(S/&lt;span class="caps"&gt;SI&lt;/span&gt;//&lt;span class="caps"&gt;REL&lt;/span&gt;) One of the coolest things about it is &lt;strong&gt;how much&lt;/strong&gt; data we have at our fingertips. If we
&lt;em&gt;only&lt;/em&gt; collected the data we knew we wanted&amp;#8230; yeah, we&amp;#8217;d fill some of our requirements, but it is
a whole world of possibilities we&amp;#8217;d be missing! It would be like going on a road-trip, but wearing a
blindfold the entire time, and only removing it when you&amp;#8217;re at one of your destinations&amp;#8230; yeah,
you&amp;#8217;ll still see stuff, but you&amp;#8217;ll be missing out on the entire&amp;nbsp;journey!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So&amp;#8230; ok, they&amp;#8217;re admitting that they collect more data than they want (or legally can?). This person&amp;#8217;s
blog series is about &amp;#8220;using passive collect to identify/enable &lt;span class="caps"&gt;CNE&lt;/span&gt; efforts&amp;#8221; (&lt;span class="caps"&gt;CNE&lt;/span&gt; being Computer Network
Exploitation), which is also implying that they have access to massive amounts of data from non-target
persons, including American&amp;nbsp;citizens.&lt;/p&gt;
&lt;p&gt;Within the document, multiple references are made to the &lt;a href="https://firstlook.org/theintercept/article/2014/03/12/nsa-plans-infect-millions-computers-malware/"&gt;&lt;span class="caps"&gt;QUANTUM&lt;/span&gt;&lt;/a&gt;
program which seems to be viewed as, in short, a tool that lets the &lt;span class="caps"&gt;NSA&lt;/span&gt; input someone&amp;#8217;s Facebook,
webmail, or other online service account, and take control of the computers they use to access&amp;nbsp;it&amp;#8230;&lt;/p&gt;
&lt;p&gt;Now, for people in my line of work, the more troubling&amp;nbsp;part:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Now, fade off with me into dream-land. Pretend that we had some master list. This master list
contained tons of networks around the world, and the personal accounts of admins of each of
those networks. And any time you wanted to target a new network, you could just find the admin
associated with it, queue his accounts up for &lt;span class="caps"&gt;QUANTUM&lt;/span&gt;, get access to his box and proceed to pwn
the network. Wouldn&amp;#8217;t that be&amp;nbsp;swell?&lt;/p&gt;
&lt;p&gt;(S/&lt;span class="caps"&gt;SI&lt;/span&gt;//&lt;span class="caps"&gt;REL&lt;/span&gt;) Well, you can stop dreaming my friends, I think it&amp;#8217;s possible (at least kinda&amp;nbsp;partially).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So&amp;#8230; we&amp;#8217;re talking about deliberately targeting the personal accounts of innocent third parties,
in order to compromise their credentials to also innocent networks, in order to eventually
gain access to the information of a target. This seems so horribly illegal I can barely
explain. It&amp;#8217;s also a direct affront to the people in my industry who have an ethical obligation
to protect the data of their employers, customers and users against illegal disclosure. And,
maybe even more troubling, it&amp;#8217;s being perpetrated by other people &amp;#8220;in our industry&amp;#8221;, other
technical people, who obviously have a very clear picture of exactly what they&amp;#8217;re&amp;nbsp;doing.&lt;/p&gt;
&lt;p&gt;My first thought was to make an analogy between our resposibility as those &amp;#8220;with the keys to
the kingdom&amp;#8221; to a more legally entrenched privacy, like that between a doctor and their patient,
or between a lawyer and their client, that would be much more obviously illegal for the government
to breach. But, apparently that&amp;#8217;s an all-too-correct analogy, since it came to light last month
that the &lt;a href="http://www.nytimes.com/2014/02/16/us/eavesdropping-ensnared-american-law-firm.html"&gt;&lt;span class="caps"&gt;NSA&lt;/span&gt; was intercepting privileged lawyer-client communications through the use of a
foreign intermediary, namely Australia&lt;/a&gt;.
Given the intelligence allicances between the &lt;span class="caps"&gt;US&lt;/span&gt;, the &lt;span class="caps"&gt;UK&lt;/span&gt; and Australia, and the scope of what
has been already disclosed, I find it entirely probable that in all of these leaked documents
that discuss doing this to &amp;#8220;foreign&amp;#8221; entities only, the reality is that to do the same to &lt;span class="caps"&gt;US&lt;/span&gt;
citizens, it&amp;#8217;s as simple as logging in to the Australian or &lt;span class="caps"&gt;UK&lt;/span&gt; equivalent&amp;nbsp;system.&lt;/p&gt;
&lt;p&gt;This is truly disturbing to me. I continue to feel that (1) if the &lt;span class="caps"&gt;NSA&lt;/span&gt; had provable cause to
collect this information, they&amp;#8217;d obtain a warrant like the Constitution says they have to,
and (2) their electronic data collection is the equivalent of wiretapping every phone in the
country and hoping for something useful - which has been continually held to be unconstitutional,
but because of the nature (already digital) of electronic communications, it&amp;#8217;s actually feasible
to&amp;nbsp;do.&lt;/p&gt;
&lt;p&gt;On a related note, a while ago I enabled &lt;a href="http://en.wikipedia.org/wiki/Time-based_One-time_Password_Algorithm"&gt;&lt;span class="caps"&gt;TOTP&lt;/span&gt;&lt;/a&gt;-based
two factor authentication on a number of my accounts (Google, GitHub, &lt;span class="caps"&gt;AWS&lt;/span&gt;, etc.) to try
and keep them more secure against the possibility of a password compromise. Yes, that still works
to keep an unscrupulous person out of them. However, if &lt;a href="http://www.washingtonpost.com/blogs/the-switch/wp/2013/08/24/loveint-when-nsa-officers-use-their-spying-power-on-love-interests/"&gt;&lt;span class="caps"&gt;NSA&lt;/span&gt; officials use classified systems to
spy on love interests&lt;/a&gt;,
it&amp;#8217;s entirely possible that an unscrupulous &lt;span class="caps"&gt;NSA&lt;/span&gt; employee (or even worse, someone who manages to
compromise the &lt;span class="caps"&gt;NSA&lt;/span&gt;&amp;#8217;s systems? Though I imagine they only use in-house-developed security for now-obvious reasons)
could decide to compromise an individual&amp;#8217;s accounts for personal gain. Given all this news, I
find it highly unlikely that such an event would ever be reported to the proper oversight authorities,
let alone become public knowledge or known to the&amp;nbsp;victim.&lt;/p&gt;
&lt;p&gt;However, if we look at some of the information about what the &lt;span class="caps"&gt;NSA&lt;/span&gt; is doing, like their
&lt;a href="http://www.wired.com/threatlevel/2012/03/ff_nsadatacenter/all/"&gt;&lt;span class="caps"&gt;NSA&lt;/span&gt;&amp;#8217;s Utah Data Center&lt;/a&gt;
and their plans for an &lt;a href="http://www.theregister.co.uk/2014/01/03/snowden_docs_show_nsa_building_encryptioncracking_quantum_system/"&gt;encryption-cracking quantum computer&lt;/a&gt;,
and assume that they probably have a datacenter full of FPGAs, it&amp;#8217;s entirely conceivable
that they can calculate this faster than most people think possible. On the other hand,
if they have passive taps on backbone providers, it&amp;#8217;s also possible they can just hijack
a session with the click of a&amp;nbsp;mouse.&lt;/p&gt;
&lt;p&gt;I don&amp;#8217;t want to sound like too much of a nut. I&amp;#8217;ve never been terribly concerned about the
government snooping on my data because, well, I&amp;#8217;m not doing anything illegal. And aside from
my stance in favor of tighter controls and more electronic freedom, I&amp;#8217;m not in many groups
that I think would be targeted. However, I do feel very strongly about what&amp;#8217;s going on in general
(we already &lt;a href="http://www.marquette.edu/library/archives/Mss/JRM/JRM-main.shtml"&gt;learned&lt;/a&gt; that
government records on individuals&amp;#8217; activities can be horribly misused). Even more so, I&amp;#8217;m
deeply disturbed that &lt;em&gt;my&lt;/em&gt; personal data and accounts could be compromised simply as a way
for a government employee to gain access to my employer&amp;#8217;s computer systems, and then to
those of our employees and&amp;nbsp;customers.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 22 Mar 2014 09:38:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2014-03-22:2014/03/nsa-targeting-sysadmins/</guid><category>nsa</category><category>government</category><category>privacy</category><category>security</category></item><item><title>Python script to backup DisqusÂ comments</title><link>http://blog.jasonantman.com/2014/03/python-script-to-backup-disqus-comments/</link><description>&lt;p&gt;Since I just &lt;a href="/2014/03/wordpress-to-pelican-with-disqus-comments"&gt;switched this blog to using Disqus for commenting&lt;/a&gt;,
I wanted a way to back up comments in case something goes wrong (like,
Disqus going the way of del.icio.us&amp;nbsp;bookmarking).&lt;/p&gt;
&lt;p&gt;I whipped up a quick &lt;a href="https://github.com/jantman/misc-scripts/blob/master/disqus_backup.py"&gt;Python script&lt;/a&gt;
using the official &lt;a href="https://github.com/disqus/disqus-python"&gt;Disqus Python &lt;span class="caps"&gt;API&lt;/span&gt; client&lt;/a&gt;. It grabs the forum details,
threads list and posts (comments) list, and writes them out to a &lt;span class="caps"&gt;JSON&lt;/span&gt;&amp;nbsp;file.&lt;/p&gt;
&lt;p&gt;It doesn&amp;#8217;t have any restore feature, but it captures all of the&amp;nbsp;data.&lt;/p&gt;
&lt;p&gt;My first test made it look like there &lt;em&gt;may&lt;/em&gt; be some posts and theads missing (my import from
wordpress showed 56 threads and 146 comments, but this script only grabbed 52 and 125 respectively),
so exercise some caution until I verify what the problem is. If you happen to figure it out,
please submit a&amp;nbsp;&lt;span class="caps"&gt;PR&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The script is available on GitHub at &lt;a href="https://github.com/jantman/misc-scripts/blob/master/disqus_backup.py"&gt;https://github.com/jantman/misc-scripts/blob/master/disqus_backup.py&lt;/a&gt;.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 01 Mar 2014 19:01:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2014-03-01:2014/03/python-script-to-backup-disqus-comments/</guid><category>pelican</category><category>disqus</category><category>python</category></item><item><title>Blog Moved from Self-hosted WordPress to Pelican on GitHubÂ Pages</title><link>http://blog.jasonantman.com/2014/03/blog-moved-from-self-hosted-wordpress-to-pelican-on-github-pages/</link><description>&lt;p&gt;I just finally finished my migration from self-hosted WordPress to &lt;a href="http://getpelican.com"&gt;Pelican&lt;/a&gt;, a Python-based
static site generator, hosted on &lt;a href="http://pages.github.com/"&gt;GitHub Pages&lt;/a&gt;. It&amp;#8217;s not only easier and free, but also the
first step in my plan to migrate off of my &lt;a href="http://linode.com"&gt;Linode&lt;/a&gt; &lt;span class="caps"&gt;VM&lt;/span&gt; and onto a mix of &lt;span class="caps"&gt;EC2&lt;/span&gt; and free&amp;nbsp;services.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m sure this post will be &lt;a href="/2009/02/wordpress-installation-finished/"&gt;around in five years&lt;/a&gt; when there&amp;#8217;s a smarter way
to do all this, but until then&amp;#8230;&amp;nbsp;yay!&lt;/p&gt;
&lt;p&gt;I hit a number of bumps during the migration, mainly around &lt;a href="/2014/02/converting-wordpress-posts-to-pelican-markdown/"&gt;Migrating &lt;span class="caps"&gt;HTML&lt;/span&gt; posts from WordPress to Markdown in Pelican&lt;/a&gt;
and migrating &lt;a href="/2014/03/wordpress-to-pelican-with-disqus-comments/"&gt;WordPress comments to Disqus&lt;/a&gt;, but in the end everything
seems to be working. Hopefully someone will find this and save a few hours or days of work if they try the same&amp;nbsp;thing.&lt;/p&gt;
&lt;p&gt;Post-go-live I still had some issues - Disqus was displaying an&amp;nbsp;error:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We were unable to load Disqus. If you are a moderator please see our troubleshooting&amp;nbsp;guide.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;on all posts created after the WordPress migration, and FeedBurner rejected my attempts to change the &lt;span class="caps"&gt;RSS&lt;/span&gt; feed &lt;span class="caps"&gt;URL&lt;/span&gt; to
its new value (though I&amp;#8217;m pretty sure that&amp;#8217;s because I neglected to drop the &lt;span class="caps"&gt;TTL&lt;/span&gt; on the &lt;span class="caps"&gt;DNS&lt;/span&gt; record, and I can&amp;#8217;t
find a way to tell Feedburner to purge it from&amp;nbsp;cache).&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 01 Mar 2014 14:14:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2014-03-01:2014/03/blog-moved-from-self-hosted-wordpress-to-pelican-on-github-pages/</guid><category>blog</category><category>wordpress</category><category>pelican</category><category>github</category></item><item><title>Wordpress to Pelican with DisqusÂ comments</title><link>http://blog.jasonantman.com/2014/03/wordpress-to-pelican-with-disqus-comments/</link><description>&lt;p&gt;This is the second part of my WordPress to &lt;a href="http://getpelican.com"&gt;Pelican&lt;/a&gt; conversion saga.
In the &lt;a href="/2014/03/converting-wordpress-posts-to-pelican-markdown/"&gt;last post&lt;/a&gt; I ran through some
of the issues that I faced when converting the posts themselves, setting up my theme and settings,
etc. In this post, I&amp;#8217;ll discuss the saga of moving from WordPress comments to Disqus&amp;nbsp;comments.&lt;/p&gt;
&lt;p&gt;Be sure to read &lt;strong&gt;all&lt;/strong&gt; of this before trying it yourself, as I had some serious problems with my
first&amp;nbsp;attempt.&lt;/p&gt;
&lt;h2 id="initial-import-to-disqus"&gt;Initial Import to&amp;nbsp;Disqus&lt;/h2&gt;
&lt;p&gt;Initially, I installed the Disqus WordPress plugin as instructed in
&lt;a href="http://help.disqus.com/customer/portal/articles/466255-importing-comments-from-wordpress"&gt;Disqus&amp;#8217; Import from WordPress documentation&lt;/a&gt;.
The automatic import imported three of 134 comments, and froze there,
even though the status said it was 100% complete. I emailed Disqus&amp;#8217; support,
and was told that this meant the import failed (even though there was no explicit
notification, their admin &lt;span class="caps"&gt;UI&lt;/span&gt; said the import was successful) and I had to manually
import my comments. I did this, as instructed in the same docs, by disabling all
plugins except for Disqus and generating an &lt;span class="caps"&gt;XML&lt;/span&gt; export from WordPress, then re-enabling
the plugins, and uploading the export to Disqus. This time, I ended up with all 134
comments in Disqus, so I assumed that all went&amp;nbsp;well.&lt;/p&gt;
&lt;h2 id="previewing-comments-in-pelican"&gt;Previewing Comments in&amp;nbsp;Pelican&lt;/h2&gt;
&lt;p&gt;I added my Disqus &lt;code&gt;shortname&lt;/code&gt; to the &lt;code&gt;DISQUS_SITENAME&lt;/code&gt; field in &lt;code&gt;pelicanconf.py&lt;/code&gt; and
re-built. I ended up having an issue with &lt;code&gt;SITE_URL&lt;/code&gt; being set incorrectly for some testing
that I did, so that killed 10 minutes. I rebuilt locally with &lt;code&gt;SITE_URL&lt;/code&gt; not defined, and
then used &lt;code&gt;fab serve&lt;/code&gt; to serve locally. I was using my
&lt;a href="/2014/01/planning-migration-from-wordpress-to-static-site/"&gt;Planning Migration from Wordpress to Static Site&lt;/a&gt;
post to test, as it was both the most recent post, and had a five comments in WordPress, which imported
correctly into Disqus and were visible both in the Disqus moderation tool and on the now-Disqus-powered
WordPress&amp;nbsp;blog.&lt;/p&gt;
&lt;p&gt;Once I rebuilt with &lt;code&gt;DISQUS_SITENAME&lt;/code&gt; set and served locally with SimpleHTTPServer (&lt;code&gt;fab serve&lt;/code&gt;),
I checked the post and saw only a &amp;#8220;We were unable to load Disqus&amp;#8221; message below the post. It contained
a &lt;a href="http://help.disqus.com/customer/portal/articles/472007-i-m-receiving-the-message-%22we-were-unable-to-load-disqus-%22"&gt;link to their help article for that problem&lt;/a&gt;,
which pointed me at a domain mismatch/different origin problem. As indicated on that page,
I went to Settings -&amp;gt; Advanced in the Disqus admin, found the &amp;#8220;Trusted Domains&amp;#8221; box, and added
both my test domain (newblog.jasonantman.com - pointing at GitHub pages until I was ready to
shut WordPress down and actually move the live site) and &amp;#8220;localhost&amp;#8221; for testing, and&amp;nbsp;saved.&lt;/p&gt;
&lt;p&gt;I refreshed the page I was looking at, and now could see the Disqus commenting below my post,
but it wasn&amp;#8217;t showing any of the comments&amp;nbsp;yet.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Disqus commenting with no comments" src="/GFX/disqus_wrong_url.png" /&gt;&lt;/p&gt;
&lt;p&gt;I pulled up the source of the page, and saw in the Disqus javascript just below the post&amp;nbsp;content:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;            &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_shortname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;jasonantman&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// required: replace example with your forum shortname&lt;/span&gt;
            &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_identifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;planning-migration-from-wordpress-to-static-site&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
            &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;../../../2014/01/planning-migration-from-wordpress-to-static-site/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Everything looked &lt;span class="caps"&gt;OK&lt;/span&gt; to me except for &lt;code&gt;disqus_url&lt;/code&gt;, which I&amp;#8217;d seen mention of on the
&lt;a href="http://help.disqus.com/customer/portal/articles/472007-i-m-receiving-the-message-%22we-were-unable-to-load-disqus-%22"&gt;help page&lt;/a&gt;
I&amp;#8217;d just been looking at. Sure enough, it indicated that the &lt;code&gt;disqus_url&lt;/code&gt; var must be
an absolute &lt;span class="caps"&gt;URL&lt;/span&gt; to the post, not a relative path. I assume this was because I&amp;#8217;d generated
the content without having &lt;code&gt;SITE_URL&lt;/code&gt; set, so I hand-edited the generated page to change this
to the correct &lt;span class="caps"&gt;URL&lt;/span&gt;, http://blog.jasonantman.com/2014/01/planning-migration-from-wordpress-to-static-site/,
and tested again. Unfortunately, still zero&amp;nbsp;comments.&lt;/p&gt;
&lt;h2 id="wordpress-disqus-plugin-permalinks"&gt;WordPress Disqus Plugin&amp;nbsp;Permalinks&lt;/h2&gt;
&lt;p&gt;Fearing the worst, I pulled up the same post on my now-Disqus-powered WordPress blog,
and took a peek at the source. The javascript over there revealed a&amp;nbsp;problem:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;http://blog.jasonantman.com/2014/01/planning-migration-from-wordpress-to-static-site/&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_identifier&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;1546 http://blog.jasonantman.com/?p=1546&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_container_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;disqus_thread&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_domain&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;disqus.com&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_shortname&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;jasonantman&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="kd"&gt;var&lt;/span&gt; &lt;span class="nx"&gt;disqus_title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Planning Migration from WordPress to Static Site&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;While the &lt;span class="caps"&gt;URL&lt;/span&gt; is correct, the Disqus WordPress plugin uses the WordPress
post &lt;span class="caps"&gt;ID&lt;/span&gt; and permalink for the &amp;#8220;identifier&amp;#8221;, but the Pelican plugin uses the slug.
That&amp;#8217;s a problem, as my Pelican site will have the same URLs, but the WordPress
post-&lt;span class="caps"&gt;ID&lt;/span&gt;-based permalinks are gone (since it&amp;#8217;s a static site, and there&amp;#8217;s no easy
way of replicating things that are query param based). The WordPress post IDs
are thrown out by Pelican, so there&amp;#8217;s no way to connect the&amp;nbsp;two.&lt;/p&gt;
&lt;p&gt;Even worse, I remembered that Disqus&amp;#8217;
&lt;a href="http://help.disqus.com/customer/portal/articles/466255-importing-comments-from-wordpress"&gt;Importing Comments from WordPress help page&lt;/a&gt;
clearly&amp;nbsp;stated:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Imported comments can&amp;#8217;t be permanently deleted. Consider following our &lt;a href="http://help.disqus.com/customer/portal/articles/1053796-best-practices-for-staging-development-and-preview-sites"&gt;guidelines for development sites&lt;/a&gt; to make sure the data you&amp;#8217;re importing is correct. You can &lt;a href="http://disqus.com/register"&gt;register a new forum&lt;/a&gt; if you have imported the wrong&amp;nbsp;comments.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="solution-to-permalink-issue"&gt;Solution to Permalink&amp;nbsp;Issue&lt;/h2&gt;
&lt;p&gt;Not seeing any way around it, I figured it was time to &amp;#8220;bite the bullet&amp;#8221;. I disabled the Disqus plugin
in WordPress and then installed and activated the
&lt;a href="http://wordpress.org/extend/plugins/code-freeze/"&gt;WordPress Code Freeze Plugin&lt;/a&gt;
to disable comments. (&lt;em&gt;Note&lt;/em&gt; ironically, this plugin also uses JavaScript to disable your ability to
deactivate plugins, including itself. So before you activate it, copy the &amp;#8220;Activate&amp;#8221; link and save it
somewhere; changing &lt;code&gt;action=activate&lt;/code&gt; to &lt;code&gt;action=deactivate&lt;/code&gt; will let you get rid of it if you&amp;nbsp;want).&lt;/p&gt;
&lt;p&gt;Disqus has some documentation on &lt;a href="http://help.disqus.com/customer/portal/articles/1104797-importing-exporting"&gt;Importing and Exporting&lt;/a&gt;
which includes &lt;a href="http://help.disqus.com/customer/portal/articles/472150"&gt;Custom &lt;span class="caps"&gt;XML&lt;/span&gt; Imports&lt;/a&gt; based on the
WordPress &lt;span class="caps"&gt;XML&lt;/span&gt; export format. So, I figured that I just had to decide that WordPress commenting would be
turned off, and do a point-in-time migration to Disqus (maybe circling back to hack the Disqus &lt;span class="caps"&gt;WP&lt;/span&gt; plugin
to keep comments working there for the time&amp;nbsp;being).&lt;/p&gt;
&lt;p&gt;Before anything else, I decided to actually set up a test forum/site in Disqus like they suggested.
I updated the &lt;code&gt;DISQUS_SITENAME&lt;/code&gt; in &lt;code&gt;pelicanconf.py&lt;/code&gt;, and then started in on the &lt;span class="caps"&gt;XML&lt;/span&gt; munging. The
&lt;a href="http://help.disqus.com/customer/portal/articles/472150"&gt;Custom &lt;span class="caps"&gt;XML&lt;/span&gt; Imports&lt;/a&gt; documentation implies
that the import engine recognizes a &lt;code&gt;dsq:thread_identifier&lt;/code&gt; &lt;span class="caps"&gt;XML&lt;/span&gt; element that holds the thread identifier,
but that element wasn&amp;#8217;t present in my WordPress &lt;span class="caps"&gt;XML&lt;/span&gt; export. It appeared that Disqus was concatenating the
&lt;code&gt;wp:post_id&lt;/code&gt; and &lt;code&gt;guid&lt;/code&gt; fields (with a space in between) to come up with the&amp;nbsp;identifier.&lt;/p&gt;
&lt;p&gt;So, I wrote a script (&lt;a href="https://github.com/jantman/blog/blob/master/dev/wp-move/wp_comment_xml_munge.py"&gt;wp_comment_xml_munge.py&lt;/a&gt;)
using &lt;a href="http://lxml.de/"&gt;lxml&lt;/a&gt; that adds the &lt;code&gt;dsq:&lt;/code&gt; namespace to the WordPress &lt;span class="caps"&gt;XML&lt;/span&gt; export (unfortunately using
string replacement and a temp file, due to a &lt;a href="https://bugs.launchpad.net/lxml/+bug/555602"&gt;bug in lxml&lt;/a&gt;)
and then adds the &lt;code&gt;dsq:thread_identifier&lt;/code&gt; tag to each post item, setting its value to the same
string as &lt;code&gt;wp:post_name&lt;/code&gt;, the &lt;span class="caps"&gt;URL&lt;/span&gt; slug (and post identifier in&amp;nbsp;Pelican).&lt;/p&gt;
&lt;p&gt;I imported the &lt;span class="caps"&gt;XML&lt;/span&gt; written by the script into my test forum in Disqus and rebuilt the Pelican content.
Magically, the first time I looked, the comments were&amp;nbsp;there.&lt;/p&gt;
&lt;p&gt;Now, time to see if I could get the same effect with the existing Disqus&amp;nbsp;site/forum:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;In the Disqus moderation interface, delete all comments. You&amp;#8217;ll have to do this in batches of 10, as that&amp;#8217;s
   how they&amp;#8217;re paged in the interface. The comments don&amp;#8217;t seem to be permanently deleted, but do show as&amp;nbsp;&amp;#8220;deleted&amp;#8221;.&lt;/li&gt;
&lt;li&gt;Go to &lt;a href="http://import.disqus.com"&gt;import.disqus.com&lt;/a&gt; and select your &amp;#8220;forum&amp;#8221; (site). You should see your existing
   (previous) import, as 100% complete, with the correct count of threads and comments. Do another import with the
   &lt;code&gt;_disqus.xml&lt;/code&gt; munged &lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;export.&lt;/li&gt;
&lt;li&gt;Comments should now be linked to the correct post in&amp;nbsp;Pelican.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;At this point, Pelican seemed to be working, but WordPress was still left with only the old internal commenting,
and that was disabled by the Code Freeze plugin. I probably could have manually patched the Disqus plugin to
reflect the new thread identifiers, but instead, I chose to just push forward with the switch from WordPress to&amp;nbsp;Pelican.&lt;/p&gt;
&lt;p&gt;That only took a few hours, and I&amp;#8217;m happy to say that I&amp;#8217;m now up and running with a Pelican blog, hosted for free
by GitHub&amp;nbsp;Pages.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Sat, 01 Mar 2014 09:09:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2014-03-01:2014/03/wordpress-to-pelican-with-disqus-comments/</guid><category>wordpress</category><category>pelican</category><category>blog</category><category>disqus</category><category>comments</category></item><item><title>Converting WordPress Posts to PelicanÂ MarkDown</title><link>http://blog.jasonantman.com/2014/02/converting-wordpress-posts-to-pelican-markdown/</link><description>&lt;p&gt;A few weeks ago, I
&lt;a href="/2014/01/planning-migration-from-wordpress-to-static-site/"&gt;posted&lt;/a&gt; about my
plans to convert my self-hosted WordPress blog to a static site using a static
blog generator. Since then, I&amp;#8217;ve decided to stop working on my exhaustive
&lt;a href="https://docs.google.com/spreadsheet/ccc?key=0AnHh-ye5DNiNdF9DWkJrT2kzSkNsNVp6cjMzLXJ6VEE&amp;amp;usp=sharing"&gt;static blog generator comparison spreadsheet&lt;/a&gt;
and just try &lt;a href="http://getpelican.com"&gt;Pelican&lt;/a&gt; - mainly because it&amp;#8217;s written in
Python which is my current strongest language, comes highly recommended, seems
to have most of the features I want, and seems to be easily&amp;nbsp;extensible.&lt;/p&gt;
&lt;p&gt;So, I walked through the documentation for the latest version (3.3.0), started
a &lt;a href="https://github.com/jantman/blog"&gt;GitHub repo&lt;/a&gt;, and tweaked a bunch of
settings. The repo is public, so if you want to take a look behind the scenes,
see my &lt;a href="https://github.com/jantman/blog/blob/master/fabfile.py"&gt;fabfile&lt;/a&gt;,
etc. feel&amp;nbsp;free. &lt;/p&gt;
&lt;h2 id="initial-wordpress-import-attempt"&gt;Initial WordPress Import&amp;nbsp;Attempt&lt;/h2&gt;
&lt;p&gt;I used the WordPress &lt;span class="caps"&gt;XML&lt;/span&gt; Export tool, as instructed in the &lt;a href="http://docs.getpelican.com/en/latest/importer.html"&gt;Pelican Importer documentation&lt;/a&gt;.
At first, I attempted to do a more-or-less default import from WordPress using
the &lt;code&gt;pelican-import&lt;/code&gt; tool, which writes rST, and then build the blog. What I
ended up with was thousands of errors complaining about &amp;#8220;Inline interpreted
text or phrase reference start-string without end-string&amp;#8221;, &amp;#8220;Explicit markup
ends without a blank line; unexpected uninden&amp;#8221;, &amp;#8220;malformed hyperlink target&amp;#8221;,
&amp;#8220;Unknown target name&amp;#8221; on all of my links, and a bevy of other Docutils
errors. It was so utterly awful that I gave&amp;nbsp;up.&lt;/p&gt;
&lt;h2 id="wordpress-import-as-markdown"&gt;WordPress Import as&amp;nbsp;MarkDown&lt;/h2&gt;
&lt;p&gt;Next I tried importing as MarkDown instead of rST,&amp;nbsp;using:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;pelican-import --markup markdown --wpfile -o content/ --dir-page jasonantman039sblog.wordpress.2014-01-11.xml
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That built without errors, and the posts looked somewhat right out of the
box, without any of the previous thousands of errors. And the links looked
mostly right - even the captions for images. Though I&amp;#8217;m working at a Python
shop and writing a lot of Python these days, my knowledge of MarkDown is still
much better than rST, so this is fine for me. (I even wrote a &lt;code&gt;fab post&lt;/code&gt; task
that prompts for a title, generates all of the post metadata, writes it to the
right file, and opens up an editor on&amp;nbsp;it.)&lt;/p&gt;
&lt;p&gt;The first problem was that the import script gave me one &amp;#8220;content&amp;#8221; directory
with 346 &amp;#8220;.md&amp;#8221; files in it - not exactly easy to work with. Luckily the
metadata was right, so a quick little
&lt;a href="https://github.com/jantman/blog/blob/master/move_wordpress.sh"&gt;bash script&lt;/a&gt;
moved the posts into a &lt;span class="caps"&gt;YYYY&lt;/span&gt;/&lt;span class="caps"&gt;MM&lt;/span&gt; directory&amp;nbsp;hierarchy.&lt;/p&gt;
&lt;h2 id="obvious-problems-with-imported-posts"&gt;Obvious Problems with Imported&amp;nbsp;Posts&lt;/h2&gt;
&lt;p&gt;After getting the MarkDown import working, and the posts moved to the proper
paths, I was still having some&amp;nbsp;issues&amp;#8230;&lt;/p&gt;
&lt;h3 id="syntax-hilighting-gone"&gt;Syntax Hilighting&amp;nbsp;Gone&lt;/h3&gt;
&lt;p&gt;In WordPress, I was using the
&lt;a href="http://wordpress.org/extend/plugins/wp-syntax/"&gt;&lt;span class="caps"&gt;WP&lt;/span&gt;-Syntax&lt;/a&gt; plugin to perform
syntax hilighting via &lt;a href="http://qbnz.com/highlighter/"&gt;GeSHi&lt;/a&gt;. The plugin uses
pre tags with a &lt;code&gt;lang=&lt;/code&gt; attribute to specify the language,&amp;nbsp;like:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&amp;lt;pre lang=&amp;quot;bash&amp;quot;&amp;gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Unfortunately, these translated to some really ugly MarkDown fenced blocks&amp;nbsp;like:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;~~~~ {lang=&amp;quot;bash&amp;quot;}
cp /boot/efi/EFI/fedora/grub.cfg /boot/efi/EFI/fedora/grub.cfg.bak
echo &amp;#39;GRUB_DISABLE_OS_PROBER=&amp;quot;true&amp;quot;&amp;#39; &amp;gt;&amp;gt; /etc/default/grub
grub2-mkconfig &amp;gt; /boot/efi/EFI/fedora/grub.cfg
~~~~
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;that seem to be just a bit off from what MarkDown/Pygments can handle. The
places where I just used bare &lt;code&gt;&amp;lt;pre&amp;gt;&lt;/code&gt; blocks translated&amp;nbsp;fine.&lt;/p&gt;
&lt;p&gt;http://blog.gastove.com/2013-09-17_enabling_line_numbers_for_pygments.html&lt;/p&gt;
&lt;p&gt;Fixed this by using fenced blocks with the &amp;#8216;lang=&amp;#8217; stuff removed, and in class
syntax like the MarkDown docs suggest. Some four-tab-indents with
:::identifier&amp;nbsp;work.&lt;/p&gt;
&lt;h3 id="broken-links"&gt;Broken&amp;nbsp;Links&lt;/h3&gt;
&lt;p&gt;It seems that something in the conversion process introduced line wraps (could
it really be Pandoc itself???) Unfortunately, this wreaks havoc with any
explicit reference links
that use long (long enough to break across lines) titles, depending on where
they are in the line. It seems that in some places they end up breaking
differently in the link in the text and in the link definition, which MarkDown misses, and
then renders broken links and plain text of the link table at the bottom of
the page. Manually removing the line breaks and any extraneous spaces seems to
fix&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;So, yes, Pandoc was doing this because of the &lt;code&gt;--reference-links&lt;/code&gt; parameter
that &lt;code&gt;pelican-import&lt;/code&gt; was calling it with. There was an
&lt;a href="https://github.com/getpelican/pelican/issues/348"&gt;issue&lt;/a&gt; and
&lt;a href="https://github.com/getpelican/pelican/pull/642"&gt;pull request&lt;/a&gt; to fix this,
but when I started with Pelican the last release was 3.3.0 (4 months ago) and
the &lt;span class="caps"&gt;PR&lt;/span&gt; was merged after that. So, if you&amp;#8217;re having the same problem and the
latest release of Pelican is still 3.3.0, you might as well just apply
&lt;a href="https://github.com/getpelican/pelican/commit/83e4d35b44a422ee8d4b077f505970d03e555f45"&gt;the patch&lt;/a&gt;
yourself - it&amp;#8217;s just a very simple removal of a parameter in
&lt;code&gt;pelican_import.py&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="overall-results"&gt;Overall&amp;nbsp;Results&lt;/h2&gt;
&lt;p&gt;I&amp;#8217;m quite happy with the overall results. I also spent a &lt;em&gt;lot&lt;/em&gt; of time manually fixing
markup issues that didn&amp;#8217;t translate well through Pandoc, but I suppose that&amp;#8217;s to be
expected given that many of my older blog posts had &lt;span class="caps"&gt;HTML&lt;/span&gt;&amp;nbsp;issues.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">Jason Antman</dc:creator><pubDate>Fri, 28 Feb 2014 22:21:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2014-02-28:2014/02/converting-wordpress-posts-to-pelican-markdown/</guid><category>pelican</category><category>wordpress</category><category>blog</category><category>markdown</category></item><item><title>Planning Migration from WordPress to StaticÂ Site</title><link>http://blog.jasonantman.com/2014/01/planning-migration-from-wordpress-to-static-site/</link><description>&lt;p&gt;Right now, this blog, my email, and a whole bunch of other services are
hosted on a &lt;a href="http://linode.com"&gt;Linode&lt;/a&gt; Xen &lt;span class="caps"&gt;VM&lt;/span&gt;. I don&amp;#8217;t really keep up
to date with administration and upgrades the way I used to, and
honestly, I&amp;#8217;d rather spend my time working on other things (like
actually writing all of the blog posts that I&amp;#8217;ve been planning to. The
first thing I&amp;#8217;ve identified for migration is this blog itself. It&amp;#8217;s
currently on WordPress and, frankly, I don&amp;#8217;t either need nor like it.
But there are some features I like. I&amp;#8217;d like to end up with a static
site generator, hosted from either S3 or GitHub Pages. I know that means
I&amp;#8217;ll lost comments (unless I move to a third-party, &lt;span class="caps"&gt;JS&lt;/span&gt;-based comment
system like &lt;a href="http://disqus.com/"&gt;Disqus&lt;/a&gt;, which means I&amp;#8217;ll lose
&lt;em&gt;control&lt;/em&gt; over my comments) but I suppose I can live with that. What I
really want is something simple, static, cheap or free (that I&amp;#8217;ll likely
put behind a small ec2 instance running nginx for&amp;nbsp;redirects/rewrites).&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m still in the planning phase, and trying to come up with a
feature-by-feature comparison of my options. I&amp;#8217;ll likely post that when
I finally have it done (at the moment it&amp;#8217;s in a very rough &lt;a href="https://docs.google.com/spreadsheet/ccc?key=0AnHh-ye5DNiNdF9DWkJrT2kzSkNsNVp6cjMzLXJ6VEE&amp;amp;usp=sharing"&gt;Google Docs
spreadsheet&lt;/a&gt;).
I&amp;#8217;m trying to round up my static site generator options and see which
ones will do most, if not all, of what I want (though I still haven&amp;#8217;t
discounted using hosted wordpress if it comes down to it). Here are the
features I currently &amp;#8220;use&amp;#8221; (have) on my WordPress&amp;nbsp;blog:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;User-defined permalinks to&amp;nbsp;posts&lt;/li&gt;
&lt;li&gt;Overall &lt;span class="caps"&gt;RSS&lt;/span&gt; feed of blog (currently powered by FeedBurner) and of&amp;nbsp;comments)&lt;/li&gt;
&lt;li&gt;Categories (a post can be in multiple&amp;nbsp;categories)&lt;/li&gt;
&lt;li&gt;Tags&lt;/li&gt;
&lt;li&gt;Category and Tag&amp;nbsp;pages&lt;/li&gt;
&lt;li&gt;per-Category and per-Tag feeds&amp;nbsp;(&lt;span class="caps"&gt;RSS&lt;/span&gt;)&lt;/li&gt;
&lt;li&gt;Tag cloud &amp;#8220;widget&amp;#8221; in&amp;nbsp;sidebar&lt;/li&gt;
&lt;li&gt;Themes. I actually like my current &lt;span class="caps"&gt;WP&lt;/span&gt;&amp;nbsp;theme&amp;#8230;&lt;/li&gt;
&lt;li&gt;Visitor statistics (currently self-hosted
    &lt;a href="http://piwik.org/"&gt;Piwik&lt;/a&gt;, formerly Google&amp;nbsp;Analytics)&lt;/li&gt;
&lt;li&gt;Post publishing via cron&amp;#8217;ed script (&lt;em&gt;see below&lt;/em&gt;)&lt;/li&gt;
&lt;li&gt;Draft/Pending status (i.e. let me save a partial post, and let me
    save a complete post but mark it &amp;#8220;pending&amp;#8221; so I can just publish it&amp;nbsp;later)&lt;/li&gt;
&lt;li&gt;Commenting (this will probably be the big sticking&amp;nbsp;point)&lt;/li&gt;
&lt;li&gt;Syntax&amp;nbsp;hilighting&lt;/li&gt;
&lt;li&gt;As &amp;#8220;weird&amp;#8221; as this is, I write all my posts in raw &lt;span class="caps"&gt;HTML&lt;/span&gt;, and am
    perfectly happy doing&amp;nbsp;that.&lt;/li&gt;
&lt;li&gt;&lt;span class="dquo"&gt;&amp;#8220;&lt;/span&gt;Subscribe via Email&amp;#8221; FeedBurner&amp;nbsp;widget&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;XML&lt;/span&gt;&amp;nbsp;Sitemap&lt;/li&gt;
&lt;li&gt;Twitter&amp;nbsp;box/widget&lt;/li&gt;
&lt;li&gt;Pingbacks (not that these are really useful for anything other than
    spam these&amp;nbsp;days)&lt;/li&gt;
&lt;li&gt;Automatic or manual post excerpts for feeds,&amp;nbsp;etc.&lt;/li&gt;
&lt;li&gt;Remote publishing via &lt;span class="caps"&gt;XML&lt;/span&gt;-&lt;span class="caps"&gt;RPC&lt;/span&gt;/Android app (not that I&amp;#8217;ve used it
    more than once or&amp;nbsp;twice)&lt;/li&gt;
&lt;li&gt;Advertising - I currently use Google AdSense on my blog. The revenue
    from my tiny hit count isn&amp;#8217;t enough to offset the cost of a Linode,
    but if I moved to a much less expensive hosting service, it might be
    worth considering (you can&amp;#8217;t run ads on the free hosted WordPress,
    and I doubt you can on GitHub Pages&amp;nbsp;either).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I should be updating this post when I do some more research and have a
comparison of the&amp;nbsp;options.&lt;/p&gt;
&lt;p&gt;Note on &amp;#8220;Post publishing via cron&amp;#8217;ed script&amp;#8221; - sometimes I sit down and
write half a dozen or so blog posts at a time. But I don&amp;#8217;t want them all
to show up immediately, and spam the few people who still use &lt;span class="caps"&gt;RSS&lt;/span&gt;
readers after the death of Google Reader. So I set the posts to
&amp;#8220;Pending&amp;#8221; status, and I have a cron&amp;#8217;ed script that runs every weekday
morning and publishes the one oldest &amp;#8220;pending&amp;#8221; post. Who knows if this
actually does any good or&amp;nbsp;not&amp;#8230;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Wed, 01 Jan 2014 15:15:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2014-01-01:2014/01/planning-migration-from-wordpress-to-static-site/</guid><category>blog</category><category>jekyll</category><category>pelican</category><category>python</category><category>static site</category><category>wordpress</category></item><item><title>Testing GPG KeyÂ Passphrases</title><link>http://blog.jasonantman.com/2013/08/testing-gpg-key-passphrases/</link><description>&lt;p&gt;So hypothetically, you have a &lt;span class="caps"&gt;GPG&lt;/span&gt; public/private keypair (from a backup
or old computer), but you don&amp;#8217;t remember the passphrase. Here&amp;#8217;s a
relatively simple way to find it from a number of possible options. This
&lt;em&gt;requires&lt;/em&gt; that you have a computer secure enough to store the possible
options in a text file. I&amp;#8217;d recommend storing that file on a
ramdisk/tmpfs, and using a temporary &lt;span class="caps"&gt;VM&lt;/span&gt; for this, which you&amp;#8217;ll wipe away
when you&amp;#8217;re&amp;nbsp;done.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Preparation:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;You have an appropriately secure place to do this with &lt;span class="caps"&gt;GPG&lt;/span&gt;
    installed, and a safe place to store a text file of sample
    passphrases (i.e. a&amp;nbsp;ramdisk).&lt;/li&gt;
&lt;li&gt;Copy your backed up public and private keys to &lt;code&gt;~/.gnupg&lt;/code&gt; on that
    host. Let&amp;#8217;s assume they&amp;#8217;re called &lt;code&gt;TestUser_public.key&lt;/code&gt; and
    &lt;code&gt;TestUser_private.key&lt;/code&gt;. We&amp;#8217;re assuming that you &lt;span class="caps"&gt;KNOW&lt;/span&gt;, &lt;span class="caps"&gt;BEYOND&lt;/span&gt; A &lt;span class="caps"&gt;DOUBT&lt;/span&gt;
    that these are your keys (i.e. you got them from a secure offline
    backup medium, you&amp;#8217;ve verified against a printed key fingerprint,
    you&amp;#8217;ve verified the fingerprints against a
    &lt;a href="http://pgp.mit.edu/"&gt;keyserver&lt;/a&gt; that you know is authoritative for
    your keys,&amp;nbsp;etc.).&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;First, we import the public and private keys to&amp;nbsp;&lt;span class="caps"&gt;GPG&lt;/span&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="gp"&gt;testuser:~$&lt;/span&gt; &lt;span class="nb"&gt;cd&lt;/span&gt; .gnupg
&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; gpg --import TestUser_public.key 
&lt;span class="go"&gt;gpg: keyring `/home/testuser/.gnupg/secring.gpg` created&lt;/span&gt;
&lt;span class="go"&gt;gpg: key &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;: public key &amp;quot;Test User (Test User) &amp;quot; imported&lt;/span&gt;
&lt;span class="go"&gt;gpg: Total number processed: 1&lt;/span&gt;
&lt;span class="go"&gt;gpg:               imported: 1  (&lt;span class="caps"&gt;RSA&lt;/span&gt;: 1)&lt;/span&gt;

&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; gpg --allow-secret-key-import --import TestUser_secret.key 
&lt;span class="go"&gt;gpg: key &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;: secret key imported&lt;/span&gt;
&lt;span class="go"&gt;gpg: key &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;: &amp;quot;Test User (Test User) &amp;quot; not changed&lt;/span&gt;
&lt;span class="go"&gt;gpg: Total number processed: 1&lt;/span&gt;
&lt;span class="go"&gt;gpg:              unchanged: 1&lt;/span&gt;
&lt;span class="go"&gt;gpg:       secret keys read: 1&lt;/span&gt;
&lt;span class="go"&gt;gpg:   secret keys imported: 1&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Check that the keys are&amp;nbsp;there:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; gpg --list-keys
&lt;span class="go"&gt;/home/testuser/.gnupg/pubring.gpg&lt;/span&gt;
&lt;span class="go"&gt;--------------------------------&lt;/span&gt;
&lt;span class="go"&gt;pub   2048R/&lt;span class="caps"&gt;17AD8D3D&lt;/span&gt; 2013-08-24&lt;/span&gt;
&lt;span class="go"&gt;uid                  Test User (Test User) &lt;/span&gt;
&lt;span class="go"&gt;sub   2048R/&lt;span class="caps"&gt;40D9F35E&lt;/span&gt; 2013-08-24&lt;/span&gt;

&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; gpg --list-secret-keys
&lt;span class="go"&gt;/home/testuser/.gnupg/secring.gpg&lt;/span&gt;
&lt;span class="go"&gt;--------------------------------&lt;/span&gt;
&lt;span class="go"&gt;sec   2048R/&lt;span class="caps"&gt;17AD8D3D&lt;/span&gt; 2013-08-24&lt;/span&gt;
&lt;span class="go"&gt;uid                  Test User (Test User) &lt;/span&gt;
&lt;span class="go"&gt;ssb   2048R/&lt;span class="caps"&gt;40D9F35E&lt;/span&gt; 2013-08-24&lt;/span&gt;

&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Note the fingerprint of the key which is, in this case, &lt;code&gt;17AD8D3D&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Testing&amp;nbsp;Passphrases:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Now that we have the keys imported, we&amp;#8217;re ready to test some
    passphrases. Enter your passphrases, one per line, in a text file.
    We&amp;#8217;re assuming that we&amp;#8217;re working on a totally secured host
    (ideally, a &lt;span class="caps"&gt;VM&lt;/span&gt; running on a standalone, non-networked machine) that
    will be destroyed when we&amp;#8217;re done. For added security, I&amp;#8217;d put this
    file on a ramdisk. In this example, the actual passphrase for the
    key is &amp;#8220;test&amp;#8221;. Here&amp;#8217;s our text&amp;nbsp;file:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; cat /tmp/passphrases 
&lt;span class="go"&gt;bad&lt;/span&gt;
&lt;span class="go"&gt;notgood&lt;/span&gt;
&lt;span class="go"&gt;notright&lt;/span&gt;
&lt;span class="go"&gt;test&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Next, create a test data file to try to&amp;nbsp;sign/encrypt:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; &lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;test input&amp;quot;&lt;/span&gt; &amp;gt; /tmp/test.in
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Now we run the actual test (see below for more&amp;nbsp;information&amp;#8230;)&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt; &lt;span class="k"&gt;for &lt;/span&gt;p in &lt;span class="sb"&gt;`&lt;/span&gt;cat /tmp/passphrases&lt;span class="sb"&gt;`&lt;/span&gt;; &lt;span class="k"&gt;do &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$p&amp;quot;&lt;/span&gt; | gpg -q --sign --local-user &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt; --passphrase-fd 0 --output /dev/null --yes /tmp/test.in &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;CORRECT&lt;/span&gt; passphrase: $p&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;break&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;; &lt;span class="k"&gt;done&lt;/span&gt;
&lt;span class="go"&gt;Reading passphrase from file descriptor 0&lt;/span&gt;

&lt;span class="go"&gt;You need a passphrase to unlock the secret key for&lt;/span&gt;
&lt;span class="go"&gt;user: &amp;quot;Test User (Test User) &amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;2048-bit &lt;span class="caps"&gt;RSA&lt;/span&gt; key, &lt;span class="caps"&gt;ID&lt;/span&gt; &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;, created 2013-08-24&lt;/span&gt;

&lt;span class="go"&gt;gpg: skipped &amp;quot;&lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;&amp;quot;: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;gpg: signing failed: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;Reading passphrase from file descriptor 0&lt;/span&gt;

&lt;span class="go"&gt;You need a passphrase to unlock the secret key for&lt;/span&gt;
&lt;span class="go"&gt;user: &amp;quot;Test User (Test User) &amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;2048-bit &lt;span class="caps"&gt;RSA&lt;/span&gt; key, &lt;span class="caps"&gt;ID&lt;/span&gt; &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;, created 2013-08-24&lt;/span&gt;

&lt;span class="go"&gt;gpg: skipped &amp;quot;&lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;&amp;quot;: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;gpg: signing failed: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;Reading passphrase from file descriptor 0&lt;/span&gt;

&lt;span class="go"&gt;You need a passphrase to unlock the secret key for&lt;/span&gt;
&lt;span class="go"&gt;user: &amp;quot;Test User (Test User) &amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;2048-bit &lt;span class="caps"&gt;RSA&lt;/span&gt; key, &lt;span class="caps"&gt;ID&lt;/span&gt; &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;, created 2013-08-24&lt;/span&gt;

&lt;span class="go"&gt;gpg: skipped &amp;quot;&lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;&amp;quot;: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;gpg: signing failed: bad passphrase&lt;/span&gt;
&lt;span class="go"&gt;Reading passphrase from file descriptor 0&lt;/span&gt;

&lt;span class="go"&gt;You need a passphrase to unlock the secret key for&lt;/span&gt;
&lt;span class="go"&gt;user: &amp;quot;Test User (Test User) &amp;quot;&lt;/span&gt;
&lt;span class="go"&gt;2048-bit &lt;span class="caps"&gt;RSA&lt;/span&gt; key, &lt;span class="caps"&gt;ID&lt;/span&gt; &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt;, created 2013-08-24&lt;/span&gt;

&lt;span class="go"&gt;&lt;span class="caps"&gt;CORRECT&lt;/span&gt; passphrase: test&lt;/span&gt;
&lt;span class="gp"&gt;testuser:~/.gnupg$&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;And there we have it, the working passphrase. I&amp;#8217;m sure there&amp;#8217;s a
    more efficient way to do this, and probably a more secure way, but
    I&amp;#8217;m not trying to brute-force someone&amp;#8217;s &lt;span class="caps"&gt;GPG&lt;/span&gt; key, I&amp;#8217;m trying to
    remember which one of my (many, many) passwords I used for a &lt;span class="caps"&gt;GPG&lt;/span&gt; key
    that I generated a decade&amp;nbsp;ago.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The actual command that we ran, rewritten with some linebreaks for
legibility,&amp;nbsp;is:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="k"&gt;for &lt;/span&gt;p in &lt;span class="sb"&gt;`&lt;/span&gt;cat /tmp/passphrases&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;do&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$p&amp;quot;&lt;/span&gt; | gpg -q --sign --local-user &lt;span class="caps"&gt;17AD8D3D&lt;/span&gt; --passphrase-fd 0 --output /dev/null --yes /tmp/test.in &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;CORRECT&lt;/span&gt; passphrase: $p&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nb"&gt;break&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;done&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This loops over each line in the passphrases file (each passphrase that
we want to try), and for each one, echoes the password and pipes it to
&lt;span class="caps"&gt;STDIN&lt;/span&gt; of &lt;code&gt;gpg&lt;/code&gt;, which tries to sign /tmp/test.in (sending the output
to /dev/null) using the key with &lt;span class="caps"&gt;ID&lt;/span&gt; &lt;code&gt;17AD8D3D&lt;/code&gt; (from #5 in the
Preparation steps above) and a password provided on &lt;span class="caps"&gt;STDIN&lt;/span&gt;. If the &lt;span class="caps"&gt;GPG&lt;/span&gt;
command succeeds, we echo the passphrase and stop looping through the
passphrases&amp;nbsp;file.&lt;/p&gt;
&lt;p&gt;I hope I wouldn&amp;#8217;t have to say this for anyone who&amp;#8217;s reading my blog, but
this information (as easy as it is to be figured out), is not to be used
for unethical&amp;nbsp;purposes.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Mon, 26 Aug 2013 06:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2013-08-26:2013/08/testing-gpg-key-passphrases/</guid><category>encryption</category><category>gnupg</category><category>gpg</category><category>key</category><category>passphrase</category><category>pgp</category></item><item><title>Quick Tip: Timestamping bashÂ history</title><link>http://blog.jasonantman.com/2013/06/quick-tip-timestamping-bash-history/</link><description>&lt;p&gt;Here&amp;#8217;s a tiny little snippet that I have in my &lt;code&gt;.bashrc&lt;/code&gt; which really
comes in handy when trying to figure out what I did on a system when.
One of the first things I do when (eek) building out or working on a
one-off machine (or setting up a new laptop/desktop, as I am right now)
is set this in bashrc for my user and root, so I can go back and
document the setup process with a little more ease and sanity. Just add
this (it&amp;#8217;s just a &lt;a href="http://linux.die.net/man/3/strftime"&gt;strftime (3)&lt;/a&gt;
format string &lt;a href="http://www.gnu.org/software/bash/manual/bashref.html#index-HISTTIMEFORMAT"&gt;according to the
docs&lt;/a&gt;,
so adjust as desired) to &lt;code&gt;.bashrc&lt;/code&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;&lt;span class="caps"&gt;HISTTIMEFORMAT&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;%F %T &amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;and bash will store commented-out integer timestamps before each line in
&lt;code&gt;.bash_history&lt;/code&gt; like&amp;nbsp;so:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="gp"&gt;#&lt;/span&gt;1370950005
&lt;span class="go"&gt;less .bashrc&lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt;1370950017
&lt;span class="go"&gt;history &lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt;1370950279
&lt;span class="go"&gt;tail -30 .bash_history &lt;/span&gt;
&lt;span class="gp"&gt;#&lt;/span&gt;1370950293
&lt;span class="go"&gt;exit&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the output of &lt;code&gt;history&lt;/code&gt; now uses the specified time&amp;nbsp;format:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt; 997  2013-06-11 07:26:45 less .bashrc
 998  2013-06-11 07:26:57 history 
 999  2013-06-11 07:31:19 tail -30 .bash_history 
1000  2013-06-11 07:31:33 exit
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Tue, 11 Jun 2013 07:09:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2013-06-11:2013/06/quick-tip-timestamping-bash-history/</guid><category>bash</category><category>history</category><category>shell</category><category>timestamp</category></item><item><title>Python script to check a list of URLs for return code, and final return code ifÂ redirected</title><link>http://blog.jasonantman.com/2013/06/python-script-to-check-a-list-of-urls-for-return-code-and-final-return-code-if-redirected/</link><description>&lt;p&gt;Every once in a while I need to add a bunch of redirects in Apache.
Here&amp;#8217;s a handy, dead simple Python script which takes a list of URLs on
&lt;span class="caps"&gt;STDIN&lt;/span&gt;, and for each one prints out either the response code, or, if the
response is a redirect, the response code of what is redirected to.
Pretty useful when you&amp;#8217;ve just added a bunch of redirects and want to
make sure none of them&amp;nbsp;404.&lt;/p&gt;
&lt;p&gt;The latest source of this script lives at
&lt;a href="https://github.com/jantman/misc-scripts/blob/master/check_url_list.py"&gt;https://github.com/jantman/misc-scripts/blob/master/check_url_list.py&lt;/a&gt;.&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/usr/bin/env python&lt;/span&gt;
&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="sd"&gt;Script to check a list of URLs (passed on stdin) for response code, and for response code of the final path in a series of redirects.&lt;/span&gt;
&lt;span class="sd"&gt;Outputs (to stdout) a list of count of a given &lt;span class="caps"&gt;URL&lt;/span&gt;, response code, and if redirected, the final &lt;span class="caps"&gt;URL&lt;/span&gt; and its response code&lt;/span&gt;

&lt;span class="sd"&gt;Optionally, with verbose flag, report on all &lt;span class="caps"&gt;URL&lt;/span&gt; checks on &lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;

&lt;span class="sd"&gt;Copyright 2013 Jason Antman  all rights reserved&lt;/span&gt;
&lt;span class="sd"&gt;This script is distributed under the terms of the GPLv3, as per the&lt;/span&gt;
&lt;span class="sd"&gt;&lt;span class="caps"&gt;LICENSE&lt;/span&gt; file in this repository.&lt;/span&gt;

&lt;span class="sd"&gt;The canonical version of this script can be found at:&lt;/span&gt;

&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;sys&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;urllib2&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_url_nofollow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;urlopen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getcode&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;code&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="n"&gt;urllib2&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HTTPError&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;main&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;urls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stdin&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;readlines&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
        &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;strip&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;+ checking &lt;span class="caps"&gt;URL&lt;/span&gt;: &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;get_url_nofollow&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="n"&gt;sys&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;stderr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;write&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;++ &lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;]))&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;line&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;print&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="si"&gt;%d&lt;/span&gt;&lt;span class="se"&gt;\t&lt;/span&gt;&lt;span class="si"&gt;%s&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;%&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;count&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;urls&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;code&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;main&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Mon, 10 Jun 2013 06:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2013-06-10:2013/06/python-script-to-check-a-list-of-urls-for-return-code-and-final-return-code-if-redirected/</guid><category>http</category><category>python</category><category>redirect</category><category>urllib</category></item><item><title>Modern (0.10.x+) NodeJS RPMs on CentOS/REHL 5 andÂ 6</title><link>http://blog.jasonantman.com/2013/06/modern-0-10-x-nodejs-rpms-on-centosrehl-5-and-6/</link><description>&lt;p&gt;I posted back in January about &lt;a href="/2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/"&gt;&lt;span class="caps"&gt;RPM&lt;/span&gt; Spec Files for nodejs 0.9.5 and v8
on CentOS
6&lt;/a&gt;. In
that post I also said that I was unable to get recent NodeJS to build on
CentOS 5 because of a long chain of dependencies including node-gyp, v8,
http-parser, glibc, etc. I said I couldn&amp;#8217;t get it to build. Well, I have
good news for both distro&amp;nbsp;versions.&lt;/p&gt;
&lt;p&gt;On the CentOS/&lt;span class="caps"&gt;RHEL&lt;/span&gt; 6 side, thanks to a lot of work by &lt;span class="caps"&gt;T. C.
&lt;/span&gt;Hollingsworth and others, NodeJS 0.10.5 is currently in the official
&lt;a href="http://fedoraproject.org/wiki/EPEL"&gt;&lt;span class="caps"&gt;EPEL&lt;/span&gt;&lt;/a&gt; repositories. They seem to be
keeping the packages pretty current, but if you need newer, you can
always grab the SRPMs from &lt;span class="caps"&gt;EPEL&lt;/span&gt; and build the newer versions. This is
great, because it means I no longer need to maintain the spec files and
do my own builds. I don&amp;#8217;t think I really did anything to help get this
package in &lt;span class="caps"&gt;EPEL&lt;/span&gt;, other than ping a few people and comment on a few&amp;nbsp;tickets.&lt;/p&gt;
&lt;p&gt;For CentOS/&lt;span class="caps"&gt;RHEL&lt;/span&gt; 5, I finally have packages, but they&amp;#8217;re not exactly
pretty. The dependency solving issues still stand; they&amp;#8217;re rooted at the
dependency of node-gyp which requires the v8 C++ JavaScript library, and
is required to compile shared object addons. The best solution that I
(and a few others) could find is simply not to build node-gyp, and not
to have support for addons or package any addons; we just have the
binaries that NodeJS&amp;#8217;s Makefile creates, and everything else is
interpreted. A &lt;a href="https://twitter.com/toxigenicpoem"&gt;coworker&lt;/a&gt; found
&lt;a href="https://github.com/kazuhisya/nodejs-rpm"&gt;https://github.com/kazuhisya/nodejs-rpm&lt;/a&gt;
which contains a configure patch and specfile for a dead-simple CentOS
5/6 &lt;span class="caps"&gt;RPM&lt;/span&gt; of NodeJS 0.10.9, which essentially just uses &lt;span class="caps"&gt;EPEL&lt;/span&gt;&amp;#8217;s python26
packages to power the NodeJS build process, configures and uses the
Makefile&amp;#8217;s &lt;code&gt;make binary&lt;/code&gt; command to spit out a NodeJS binary tarball,
and then packages that. That whole process way out of line from the
&lt;a href="http://fedoraproject.org/wiki/Packaging:Guidelines"&gt;Fedora Packaging
Guidelines&lt;/a&gt;, and
also only dumps out nodejs, nodejs-binary and nodejs-debuginfo packages,
so I also can&amp;#8217;t just substitute in a different package name in my puppet
manifests (which install nodejs, nodejs-devel and npm packages). So I
&lt;a href="https://github.com/jantman/nodejs-rpm-centos5"&gt;forked that repository&lt;/a&gt;
and made some changes to the specfile: I gave the package name a prefix
(&amp;#8220;cmgd_&amp;#8221;, since that&amp;#8217;s where I work these days) and some warnings in
the description, to make it abundantly clear that these packages are
very far from what you find in &lt;span class="caps"&gt;EPEL&lt;/span&gt; and other repositories, and broke
npm and the devel files out into their own subpackages. Hopefully this
spec file will be of use to someone else who also has the unfortunate
need of supporting recent NodeJS on CentOS 5. If there&amp;#8217;s enough
interest, I&amp;#8217;ll consider building the packages and putting them in a
repository&amp;nbsp;somewhere.&lt;/p&gt;
&lt;p&gt;You can see the NodeJS 0.10.9 on CentOS 5 spec file, a patch, and the
READMEs at
&lt;a href="https://github.com/jantman/nodejs-rpm-centos5"&gt;https://github.com/jantman/nodejs-rpm-centos5&lt;/a&gt;.
Patches and/or pull requests are greatly appreciated, especially from
anyone who wants to make the spec file more Fedora guidelines&amp;nbsp;compliant.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Thu, 06 Jun 2013 20:47:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2013-06-06:2013/06/modern-0-10-x-nodejs-rpms-on-centosrehl-5-and-6/</guid><category>build</category><category>centos</category><category>EPEL</category><category>node</category><category>nodejs</category><category>package</category><category>packaging</category><category>redhat</category><category>RHEL</category><category>rpm</category><category>specfile</category></item><item><title>Script to easily rebuild aÂ SRPM</title><link>http://blog.jasonantman.com/2013/05/script-to-easily-rebuild-a-srpm/</link><description>&lt;p&gt;Between &lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS 5 and 6 the default &lt;span class="caps"&gt;RPM&lt;/span&gt; compression format was
changed to xz. As such, trying to build a recent Fedora or Cent6 &lt;span class="caps"&gt;SRPM&lt;/span&gt; on
Cent5 will error out with a message like
&lt;code&gt;error: unpacking of archive failed on file foo;51a4c2a5: cpio: MD5 sum mismatch&lt;/code&gt;
because tar on CentOS 5 doesn&amp;#8217;t support&amp;nbsp;xz.&lt;/p&gt;
&lt;p&gt;Here&amp;#8217;s a quick and dirty little script to use &lt;code&gt;rpm2cpio&lt;/code&gt; to rebuild a
&lt;span class="caps"&gt;SRPM&lt;/span&gt; using the host&amp;#8217;s native &lt;span class="caps"&gt;RPM&lt;/span&gt; compression. The latest version will
live at
&lt;a href="https://github.com/jantman/misc-scripts/blob/master/rebuild_srpm.sh"&gt;https://github.com/jantman/misc-scripts/blob/master/rebuild_srpm.sh&lt;/a&gt;&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="c"&gt;#!/bin/bash&lt;/span&gt;
&lt;span class="c"&gt;#&lt;/span&gt;
&lt;span class="c"&gt;# Script to rebuild a &lt;span class="caps"&gt;SRPM&lt;/span&gt; 1:1, useful when you want to build a &lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS 6&lt;/span&gt;
&lt;span class="c"&gt;# &lt;span class="caps"&gt;SRPM&lt;/span&gt; on a &lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS 5 system that doesn&amp;#39;t support newer compression (cpio: &lt;span class="caps"&gt;MD5&lt;/span&gt; sum mismatch)&lt;/span&gt;
&lt;span class="c"&gt;#&lt;/span&gt;
&lt;span class="c"&gt;# by Jason Antman &lt;/span&gt;
&lt;span class="c"&gt;# The latest version of this script will always live at:&lt;/span&gt;
&lt;span class="c"&gt;# &lt;/span&gt;
&lt;span class="c"&gt;#&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -z &lt;span class="s2"&gt;&amp;quot;$1&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$1&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;-h&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;$1&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--help&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;USAGE&lt;/span&gt;: rebuild_srpm.sh  &amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit &lt;/span&gt;1
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; -z &lt;span class="s2"&gt;&amp;quot;$2&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nv"&gt;&lt;span class="caps"&gt;OUTDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nv"&gt;&lt;span class="caps"&gt;OUTDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;$2&amp;quot;&lt;/span&gt;
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="o"&gt;[[&lt;/span&gt; ! -e &lt;span class="s2"&gt;&amp;quot;$1&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;]]&lt;/span&gt;
&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;ERROR&lt;/span&gt;: &lt;span class="caps"&gt;SRPM&lt;/span&gt; file not found: $1&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit &lt;/span&gt;1
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; ! which rpmbuild &amp;amp;&amp;gt; /dev/null
&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rpmbuild could not be found. please install. (sudo yum install rpm-build)&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit &lt;/span&gt;1
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt; ! which rpm2cpio &amp;amp;&amp;gt; /dev/null
&lt;span class="k"&gt;then&lt;/span&gt;
&lt;span class="k"&gt;    &lt;/span&gt;&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;rpm2cpio could not be found. please install. (sudo yum install rpm)&amp;quot;&lt;/span&gt;
    &lt;span class="nb"&gt;exit &lt;/span&gt;1
&lt;span class="k"&gt;fi&lt;/span&gt;

&lt;span class="nv"&gt;&lt;span class="caps"&gt;SRPM&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;dirname &lt;span class="s2"&gt;&amp;quot;$1&amp;quot;&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/&amp;quot;&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;basename &lt;span class="s2"&gt;&amp;quot;$1&amp;quot;&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="nv"&gt;&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;mktemp -d&lt;span class="sb"&gt;`&lt;/span&gt;
&lt;span class="nv"&gt;&lt;span class="caps"&gt;STARTPWD&lt;/span&gt;&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;&lt;span class="nb"&gt;pwd&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Rebuilding $&lt;span class="caps"&gt;SRPM&lt;/span&gt;...&amp;quot;&lt;/span&gt;

&lt;span class="c"&gt;# copy srpm into tempdir&lt;/span&gt;
cp &lt;span class="nv"&gt;$&lt;span class="caps"&gt;SRPM&lt;/span&gt;&lt;/span&gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;

&lt;span class="nb"&gt;pushd&lt;/span&gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt; &amp;amp;&amp;gt;/dev/null

&lt;span class="c"&gt;# setup local build dir structure&lt;/span&gt;
mkdir -p rpm rpm/&lt;span class="caps"&gt;BUILD&lt;/span&gt; rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt; rpm/&lt;span class="caps"&gt;SOURCES&lt;/span&gt; rpm/&lt;span class="caps"&gt;SPECS&lt;/span&gt; rpm/&lt;span class="caps"&gt;SRPMS&lt;/span&gt; rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt;/athlon rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt;/i&lt;span class="se"&gt;\[&lt;/span&gt;3456&lt;span class="se"&gt;\]&lt;/span&gt;86 rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt;/i386 rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt;/noarch rpm/&lt;span class="caps"&gt;RPMS&lt;/span&gt;/x86_64

&lt;span class="c"&gt;# setup rpmmacros file&lt;/span&gt;
cat /dev/null &amp;gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;/.rpmmacros
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;%_topdir        $&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;/rpm&amp;quot;&lt;/span&gt; &amp;gt;&amp;gt; ~/.rpmmacros

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Extracting &lt;span class="caps"&gt;SRPM&lt;/span&gt;...&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;pushd&lt;/span&gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;/rpm/&lt;span class="caps"&gt;SOURCES&lt;/span&gt;/ &amp;amp;&amp;gt;/dev/null
rpm2cpio &lt;span class="nv"&gt;$&lt;span class="caps"&gt;SRPM&lt;/span&gt;&lt;/span&gt; | cpio -idmv &amp;amp;&amp;gt;/dev/null
&lt;span class="nb"&gt;popd&lt;/span&gt; &amp;amp;&amp;gt;/dev/null

&lt;span class="c"&gt;# build the &lt;span class="caps"&gt;SRPM&lt;/span&gt; from the spec and sources&lt;/span&gt;
&lt;span class="c"&gt;# we&amp;#39;re just building a &lt;span class="caps"&gt;SRPM&lt;/span&gt; so we can ignore dependencies&lt;/span&gt;
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Rebuilding &lt;span class="caps"&gt;SRPM&lt;/span&gt;...&amp;quot;&lt;/span&gt;
&lt;span class="nv"&gt;NEW_SRPM&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;rpmbuild -bs --nodeps --macros&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;/.rpmmacros &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;/rpm/&lt;span class="caps"&gt;SOURCES&lt;/span&gt;/*.spec | grep &lt;span class="s2"&gt;&amp;quot;^Wrote: &amp;quot;&lt;/span&gt; | awk &lt;span class="s1"&gt;&amp;#39;{print $2}&amp;#39;&lt;/span&gt;&lt;span class="sb"&gt;`&lt;/span&gt;

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Copying to $&lt;span class="caps"&gt;OUTDIR&lt;/span&gt;&amp;quot;&lt;/span&gt;
cp &lt;span class="nv"&gt;$NEW_SRPM&lt;/span&gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;OUTDIR&lt;/span&gt;&lt;/span&gt;/

&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Wrote file to $&lt;span class="caps"&gt;OUTDIR&lt;/span&gt;/`basename $NEW_SRPM`&amp;quot;&lt;/span&gt;

&lt;span class="c"&gt;# cleanup&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt; &lt;span class="nv"&gt;$&lt;span class="caps"&gt;STARTPWD&lt;/span&gt;&lt;/span&gt;
rm -Rf &lt;span class="nv"&gt;$&lt;span class="caps"&gt;TEMPDIR&lt;/span&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Tue, 28 May 2013 10:26:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2013-05-28:2013/05/script-to-easily-rebuild-a-srpm/</guid><category>lzma</category><category>packaging</category><category>rpm</category><category>rpm2cpio</category><category>rpmbuild</category><category>srpm</category><category>xz</category></item><item><title>Git CheatÂ Sheet</title><link>http://blog.jasonantman.com/2013/05/git-cheat-sheet/</link><description>&lt;p&gt;I use &lt;a href="http://git-scm.com/"&gt;git&lt;/a&gt; quite a bit these days, both with an
internal server at work and with a bunch of my projects and random code
that now live on &lt;a href="https://github.com/jantman/"&gt;my github account&lt;/a&gt;. The
transition from &lt;span class="caps"&gt;SVN&lt;/span&gt; hasn&amp;#8217;t always been easy. Here&amp;#8217;s a quick cheat sheet
of some of the things that I usually&amp;nbsp;forget.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Show diff of the last&amp;nbsp;commit:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git diff HEAD^..HEAD
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Roll back to version xyz of a specific file &lt;em&gt;(where xyz is a &lt;span class="caps"&gt;SHA1&lt;/span&gt;
    commit ref)&lt;/em&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git checkout xyz path/to/file
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Undo any &lt;em&gt;unstaged&lt;/em&gt; changes to your&amp;nbsp;branch:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git checkout -f
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Undo any staged and working directory&amp;nbsp;changes:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git reset --hard
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update submodules after cloning a&amp;nbsp;repository:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git submodule update --init
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rebase on current master to pull in new&amp;nbsp;changes:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git rebase master
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rebase on current master, but for files that changed, take our
    version &lt;em&gt;(for some reason, a plain rebase seems to sometimes show
    conflicts on files that haven&amp;#8217;t changed in ages on master)&lt;/em&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git rebase -s recursive -Xtheirs master
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete a local&amp;nbsp;branch:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git branch -d BranchName
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete a remote branch from&amp;nbsp;origin:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git push origin --delete BranchName
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Roll back your branch to the same state as the branch in&amp;nbsp;origin:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git reset --hard origin/BranchName
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Revert a specific&amp;nbsp;commit:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git revert COMMIT_HASH
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Track an upstream branch (i.e. in a project you&amp;nbsp;forked):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git remote add --track master upstream https://github.com/user/project.git
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Pull in upstream&amp;nbsp;changes:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git checkout master &amp;amp;&amp;amp; git fetch upstream &amp;amp;&amp;amp; git merge upstream/master
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Merge &amp;#8220;stuff&amp;#8221; from someone else&amp;#8217;s fork into&amp;nbsp;yours:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git remote add other-guys-repo URL_TO_REPO
git fetch other-guys-repo
git checkout my_new_branch
git merge other-guys-repo/master
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Prune local branches that have been deleted in the remote&amp;nbsp;(origin):&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;git remote prune origin
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Tue, 14 May 2013 05:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2013-05-14:2013/05/git-cheat-sheet/</guid><category>git</category></item><item><title>Search for a small-scale but automated RPM buildÂ system</title><link>http://blog.jasonantman.com/2013/05/search-for-a-small-scale-but-automated-rpm-build-system/</link><description>&lt;p&gt;&lt;strong&gt;This post is part of a series of older draft posts from a few months
ago that I&amp;#8217;m just getting around to publishing. Unfortunately, I have
yet to find a build system that meets my requirements (see the last&amp;nbsp;paragraph).&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;At work, we have a handful - currently a really small number - of &lt;span class="caps"&gt;RPM&lt;/span&gt;
packages that we need to build and deploy internally for our CentOS
server infrastructure. A number of them are just pulled down from
specific third-party repositories and rebuilt to have the vendor set as
us, and some are internally patched or developed software. We run
websites, and on the product side, we&amp;#8217;re a
Python/&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt; shop (in fact, probably
one of the largest Django apps out there). We don&amp;#8217;t deploy our Django
apps via &lt;span class="caps"&gt;RPM&lt;/span&gt;, so building and distributing RPMs is definitely not one of
our core competencies. In fact, we really only want to do it when we&amp;#8217;re
testing/deploying a new distro, or when an upstream package is&amp;nbsp;updated.&lt;/p&gt;
&lt;p&gt;Last week I pulled a ticket to deploy &lt;a href="http://nodejs.org/"&gt;node.js&lt;/a&gt; to
one of our build hosts, and we&amp;#8217;ve got a few things in the pipeline that
also rely on it. I found the
&lt;a href="https://github.com/puppetlabs/puppetlabs-nodejs"&gt;puppetlabs-nodejs&lt;/a&gt;
module on Github that&amp;#8217;s supposed to install it on &lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS, but it
pulls packages from
&lt;a href="http://patches.fedorapeople.org/oldnode/stable/"&gt;http://patches.fedorapeople.org/oldnode/stable/&lt;/a&gt;,
and the newest version of nodejs there is 0.6.18, which is quite old. I
can&amp;#8217;t find any actively maintained sources of newer nodejs packages for
&lt;span class="caps"&gt;RHEL&lt;/span&gt;/CentOS (yeah, I know, that&amp;#8217;s one down side to the
distributions&amp;#8230;). However, I did find that nodejs 0.9.5 is being &lt;a href="http://koji.fedoraproject.org/koji/packageinfo?packageID=15154"&gt;built
for Fedora 18/19 in the Fedora build
system&lt;/a&gt;,
is already in the Fedora 18 Testing and Fedora Rawhide repos, but is
failing its &lt;span class="caps"&gt;EL6&lt;/span&gt; builds in their system. The decision I&amp;#8217;ve come to is to
use the puppetlabs-nodejs module to install it, but try and rebuild the
Fedora 18 RPMs under CentOS 5 and&amp;nbsp;6.&lt;/p&gt;
&lt;p&gt;So that&amp;#8217;s the background. Now, my current task: to search for an &lt;span class="caps"&gt;RPM&lt;/span&gt;
build system for my current job. My core requirements, in no specific
order,&amp;nbsp;are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Be relatively easy and quick to use for people who have a specfile
    or &lt;span class="caps"&gt;SRPM&lt;/span&gt; and want to be able to &amp;#8220;ensure =&gt; present&amp;#8221; the finished &lt;span class="caps"&gt;RPM&lt;/span&gt;
    on a system. i.e., require as little per-package configuration as&amp;nbsp;possible.&lt;/li&gt;
&lt;li&gt;Be able to handle rebuilding &amp;#8220;all&amp;#8221; of our RPMs when we roll out a
    new distro version. Doesn&amp;#8217;t necessarily need to be automatic, but
    should be relatively&amp;nbsp;simple.&lt;/li&gt;
&lt;li&gt;Ideally, not need to be running constantly - i.e. something that
    will cope well with build hosts being VMs that are shut down when
    they&amp;#8217;re not&amp;nbsp;needed.&lt;/li&gt;
&lt;li&gt;Handle automatically putting successfully built packages into a
    repository, ideally with some sort of (manual) promotion process
    from staging to&amp;nbsp;stable.&lt;/li&gt;
&lt;li&gt;Have minimal external (infrastructure) dependencies that we can&amp;#8217;t
    satisfy with existing&amp;nbsp;systems.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So, the first step was to research existing &lt;span class="caps"&gt;RPM&lt;/span&gt; build systems and how
others do this. Here&amp;#8217;s a list of what I could find online, though most
of these are from distributions and software vendors/projects, not
end-user companies that are only building for internal&amp;nbsp;use.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://fedorahosted.org/koji/wiki"&gt;Koji&lt;/a&gt; is the build system used
    by &lt;a href="http://fedoraproject.org/wiki/Koji"&gt;Fedora&lt;/a&gt; and RedHat. It&amp;#8217;s
    about as full-featured as any can be, and I&amp;#8217;m familiar with it from
    my time at &lt;a href="http://koji.rutgers.edu/koji/"&gt;Rutgers University&lt;/a&gt;, as
    it&amp;#8217;s used to maintain their CentOS/&lt;span class="caps"&gt;RHEL&lt;/span&gt; packages. It&amp;#8217;s based largely
    on Mock. However, &lt;a href="http://fedoraproject.org/wiki/Koji/ServerHowTo"&gt;setting up the build
    server&lt;/a&gt; is no
    trivial task; there are few installations outside of Fedora/RedHat,
    and it relies on either Kerberos or an &lt;span class="caps"&gt;SSL&lt;/span&gt; &lt;span class="caps"&gt;CA&lt;/span&gt; infrastructure to
    authenticate machines and clients. So, it&amp;#8217;s designed for too large a
    scale and too much infrastructure for&amp;nbsp;me.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;PLD&lt;/span&gt; Linux has a &lt;a href="https://www.pld-linux.org/developingpld/builderscript"&gt;builder
    script&lt;/a&gt; that
    seems to automate &lt;code&gt;rpmbuild&lt;/code&gt; as well as fetching sources and
    resolving/building dependencies. I haven&amp;#8217;t looked at the script yet,
    but apparently it&amp;#8217;s in &lt;span class="caps"&gt;PLD&lt;/span&gt;&amp;#8217;s &amp;#8220;rpm-build-tools&amp;#8221;&amp;nbsp;package.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;PLD&lt;/span&gt; Linux also has a &lt;span class="caps"&gt;CVS&lt;/span&gt; repository for something called
    &lt;a href="http://cvs.pld-linux.org/cgi-bin/cvsweb/pld-builder.new"&gt;pld-builder.new&lt;/a&gt;.
    The
    &lt;a href="http://cvs.pld-linux.org/cgi-bin/cvsweb/pld-builder.new/doc/README?rev=1.5"&gt;&lt;span class="caps"&gt;README&lt;/span&gt;&lt;/a&gt;
    and
    &lt;a href="http://cvs.pld-linux.org/cgi-bin/cvsweb/pld-builder.new/doc/ARCHITECTURE?rev=1.6"&gt;&lt;span class="caps"&gt;ARCHITECTURE&lt;/span&gt;&lt;/a&gt;
    files make it sound like a relatively simple mainly-Python system
    that builds &lt;span class="caps"&gt;SRPMS&lt;/span&gt; and binary packages when requested, and most
    importantly, seems like a simple system that uses little more than
    shared filesystem access for communication and&amp;nbsp;coordination.&lt;/li&gt;
&lt;li&gt;&lt;span class="caps"&gt;ALT&lt;/span&gt; Linux has &lt;a href="http://en.altlinux.org/Sisyphus"&gt;Sisyphus&lt;/a&gt;, which
    combines repository management and web interface tools, package
    building and testing tools, and&amp;nbsp;more.&lt;/li&gt;
&lt;li&gt;The Dries &lt;span class="caps"&gt;RPM&lt;/span&gt; repository uses (or at least used&amp;#8230; my reference is
    quite old) &lt;a href="http://dries.ulyssis.org/rpm/pydar2/index.html"&gt;pydar2&lt;/a&gt;,
    &amp;#8220;a distributed client/server program which allows you to build
    multiple spec files on multiple distribution/architecture
    combinations automatically.&amp;#8221; That sounds like it could be what I
    need, but the last update says that it isn&amp;#8217;t finished yet, and that
    was in &lt;strong&gt;2005&lt;/strong&gt;.&lt;/li&gt;
&lt;li&gt;Mandriva Linux has pretty extensive information on their build
    system &lt;a href="http://wiki.mandriva.com/en/Category:Build_System"&gt;on their
    wiki&lt;/a&gt; and a
    &lt;a href="http://wiki.mandriva.com/en/Development/Packaging/BuildSystem/Theory"&gt;build system theory
    page&lt;/a&gt;,
    but it seems to be largely a hodgepodge of shell scripts and
    cronjobs, and is likely not a candidate for use by anyone other than
    its&amp;nbsp;designers.&lt;/li&gt;
&lt;li&gt;Argeo provides the &lt;a href="https://www.argeo.org/wiki/SLC"&gt;&lt;span class="caps"&gt;SLC&lt;/span&gt; framework&lt;/a&gt;
    which has a &amp;#8220;&lt;span class="caps"&gt;RPM&lt;/span&gt; Factory&amp;#8221; component, but I can&amp;#8217;t seem to find much
    more than a wiki page, and can&amp;#8217;t tell if it&amp;#8217;s a build automation
    system or just handles mocking packages and putting them in a repo
    on a single&amp;nbsp;host.&lt;/li&gt;
&lt;li&gt;Dag Wieers&amp;#8217; repositories use (or used) a set of python scripts
    called &lt;a href="http://dag.wieers.com/home-made/dar/"&gt;&lt;span class="caps"&gt;DAR&lt;/span&gt;, &amp;#8220;Dynamic Apt Repository
    builder&amp;#8221;&lt;/a&gt;. They&amp;#8217;re on
    &lt;a href="https://github.com/dagwieers/dar"&gt;github&lt;/a&gt; but are listed as &amp;#8220;old&amp;#8221;
    and haven&amp;#8217;t been updated in at least 2 years. The features sound
    quite interesting, and though it&amp;#8217;s based on the Apt repo format, it
    might provide some good ideas for implementing a similar&amp;nbsp;system.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Update four months later:&lt;/strong&gt; I&amp;#8217;ve yet to find a build system that meets
my requirements above. For the moment I&amp;#8217;m only managing \~20 packages,
so my &amp;#8220;build system&amp;#8221; is a single shell script that reads in some
environment variables and runs through using
&lt;a href="http://fedoraproject.org/wiki/Projects/Mock"&gt;mock&lt;/a&gt; to build them in the
correct order (including pushing the finished RPMs back into the local
repository that mock reads from) and then pushing the finished packages
to our internal repository. Maybe when I have some spare time, I&amp;#8217;ll
consider a project to either make a slightly better (but simple) &lt;span class="caps"&gt;RPM&lt;/span&gt;
build system based on Python, or get our
&lt;a href="http://jenkins-ci.org/"&gt;Jenkins&lt;/a&gt; install to handle this for&amp;nbsp;me.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Mon, 13 May 2013 05:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2013-05-13:2013/05/search-for-a-small-scale-but-automated-rpm-build-system/</guid><category>build</category><category>linux</category><category>nodejs</category><category>package</category><category>packaging</category><category>repository</category><category>rpm</category><category>rpmbuild</category><category>software</category><category>sysadmin</category><category>yum</category></item><item><title>Environment Variable Substitution in Apache httpdÂ Configs</title><link>http://blog.jasonantman.com/2013/05/environment-variable-substitution-in-apache-httpd-configs/</link><description>&lt;p&gt;I&amp;#8217;ve been configuring Apache httpd for over a decade, from a single
personal web server to web farms running thousands of vhosts. In most of
the &amp;#8220;real&amp;#8221; environments I&amp;#8217;ve worked in, we&amp;#8217;ve had some variation of
production, stage/test/&lt;span class="caps"&gt;QA&lt;/span&gt; and development hosts; and usually some method
of managing configurations between them, whether it&amp;#8217;s source control or
generating them from template. And in all of these environments, there
has invariably been drift between the configurations in the various
environments, whether it&amp;#8217;s because of poor tools to maintain a unified
configuration or many of those emergency redirect requests that make it
into production but are never backported. This is made all the worse
because everywhere I&amp;#8217;ve worked, the real difference between what
production and other environments &lt;em&gt;should&lt;/em&gt; be is really just a string
replacement in Apache configurations - &lt;code&gt;/prod/&lt;/code&gt; to &lt;code&gt;/test/&lt;/code&gt; or
&lt;code&gt;www.example.com&lt;/code&gt; to &lt;code&gt;www.dev.example.com&lt;/code&gt; or something along those&amp;nbsp;lines.&lt;/p&gt;
&lt;p&gt;Well a few days ago I was having a discussion with some co-workers that
dovetailed into this topic, and when I started some research, I found
(&lt;em&gt;finally after using httpd for years&lt;/em&gt;) that the &lt;a href="http://httpd.apache.org/docs/2.2/configuring.html#syntax"&gt;Apache httpd 2.2
configuration file syntax
documentation&lt;/a&gt;
states that httpd supports environment variable interpolation anywhere
in the config files (and &lt;a href="http://httpd.apache.org/docs/2.4/configuring.html#syntax"&gt;httpd
2.4&lt;/a&gt; supports
it with Defines as&amp;nbsp;well).&lt;/p&gt;
&lt;p&gt;Yup, that&amp;#8217;s right. All those different Apache configs I&amp;#8217;ve worked with
for years that define separate vhosts, document roots, rewrite targets,
ServerAliases, etc. for &lt;code&gt;www.example.com&lt;/code&gt; and &lt;code&gt;www.qa.example.com&lt;/code&gt; and
&lt;code&gt;www.dev.example.com&lt;/code&gt; really only had to be
&lt;code&gt;www.${ENV_URL_PART}example.com&lt;/code&gt;, and set &lt;code&gt;ENV_URL_PART&lt;/code&gt; in the init
script or sysconfig file. (Of course this all assumes that you have your
different environments served by different httpd instances, which you
do, of&amp;nbsp;course&amp;#8230;)&lt;/p&gt;
&lt;p&gt;For me, this is a very big deal. It means that finally, instead of
maintaining separate sets of configs for different environments which
are (theoretically, except for those emergencies) kept identical by
hand, or updating templates and then re-generating each environment&amp;#8217;s
configs, we can finally follow the same
commit/merge/promotion-between-environments workflow that we use for
other production code and Puppet configuration. It also means that those
pesky little rewrites and other minor tweaks will make it all the way
back to development&amp;nbsp;environments.&lt;/p&gt;
&lt;p&gt;So, here&amp;#8217;s a little example of how this would work in reality. Let&amp;#8217;s
assume that we have 3 main environments, &lt;code&gt;prod&lt;/code&gt;, &lt;code&gt;qa&lt;/code&gt; and &lt;code&gt;dev&lt;/code&gt; (though
this should work for N environments) and that domains are prefixed with
&amp;#8220;qa.&amp;#8221; or &amp;#8220;dev.&amp;#8221; for the respective internal environments. We set
environment variables before httpd is started, on a per-host basis,
depending on what environment that host is in. On RedHat based systems,
we&amp;#8217;d add the variables to &lt;code&gt;/etc/sysconfig/httpd&lt;/code&gt; for&amp;nbsp;production:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;HTTPD_ENV_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prod&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;HTTPD_ENV_URL_PART&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or for&amp;nbsp;&lt;span class="caps"&gt;QA&lt;/span&gt;:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;HTTPD_ENV_NAME&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;qa&amp;quot;&lt;/span&gt;
&lt;span class="nb"&gt;export &lt;/span&gt;&lt;span class="nv"&gt;HTTPD_ENV_URL_PART&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;qa.&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Those variables will now be available to httpd within the configurations
(and also to any applications or scripts that have access to the web
server&amp;#8217;s environment&amp;nbsp;variables).&lt;/p&gt;
&lt;p&gt;Now let&amp;#8217;s look at an example vhost configuration file that uses the
environment&amp;nbsp;variables:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="nb"&gt;ServerName&lt;/span&gt; example.com
&lt;span class="nb"&gt;ServerAlias&lt;/span&gt; www.example.com
&lt;span class="c"&gt;# Aliases including proper environment name&lt;/span&gt;
&lt;span class="nb"&gt;ServerAlias&lt;/span&gt; www.${HTTPD_ENV_NAME}.example.com ${HTTPD_ENV_NAME}.example.com

&lt;span class="nb"&gt;ErrorLog&lt;/span&gt; &lt;span class="sx"&gt;/var/log/httpd/example.com-error_log&lt;/span&gt;
&lt;span class="nb"&gt;CustomLog&lt;/span&gt; &lt;span class="sx"&gt;/var/log/httpd/example.com-access_log&lt;/span&gt; combined

&lt;span class="nb"&gt;DocumentRoot&lt;/span&gt; &lt;span class="sx"&gt;/sites/example.com/&lt;/span&gt;${HTTPD_ENV_NAME}/

&lt;span class="c"&gt;# Environment-specific configuration, if we absolutely need it:&lt;/span&gt;
&lt;span class="nb"&gt;Include&lt;/span&gt; &lt;span class="sx"&gt;/etc/httpd/sites/&lt;/span&gt;${HTTPD_ENV_NAME}/env.conf


&lt;span class="nb"&gt;RewriteEngine&lt;/span&gt; &lt;span class="k"&gt;on&lt;/span&gt;
&lt;span class="nb"&gt;RewriteRule&lt;/span&gt; &lt;span class="sx"&gt;/foobar/.&lt;/span&gt;* http://www.${HTTPD_ENV_URL_PART}example.com/baz/ [R=302,L]
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Every instance of &lt;code&gt;${HTTPD_ENV_NAME}&lt;/code&gt; will be replaced with the value
set in the sysconfig file, and likewise with every instance of
&lt;code&gt;${HTTPD_ENV_URL_PART}&lt;/code&gt;. This way, we can have one set of configurations
and use our normal source control branch/promotion process to both test
and promote changes through the environments along with application
code, and ensure that any straight-to-production emergency changes
(everyone has customer-ordered rewrites like that, right?) make it back
to development and&amp;nbsp;qa.&lt;/p&gt;
&lt;p&gt;One caveat is that, if the environment variable is not defined, the
&lt;code&gt;${VAR_NAME}&lt;/code&gt; will be left as a literal string in the configuration
file. There doesn&amp;#8217;t seem to be any way to protect against this in httpd
2.2, other than making sure the variables are set before the server
starts (and maybe setting logical default values, like an empty string,
in your init script which should be overridden by the sysconfig&amp;nbsp;file).&lt;/p&gt;
&lt;p&gt;If you&amp;#8217;re running httpd 2.4+, you can turn on
&lt;a href="http://httpd.apache.org/docs/2.4/mod/mod_info.html"&gt;mod_info&lt;/a&gt; and
browse to &lt;code&gt;http://servername/server-info?config&lt;/code&gt; to dump the current
configuration, which will show the variable&amp;nbsp;substitution.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Sat, 11 May 2013 12:01:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2013-05-11:2013/05/environment-variable-substitution-in-apache-httpd-configs/</guid><category>apache</category><category>environment</category><category>httpd</category><category>variable</category></item><item><title>RPM Spec Files for nodejs 0.9.5 and v8 on CentOSÂ 6</title><link>http://blog.jasonantman.com/2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/</link><description>&lt;p&gt;The latest version of nodejs that I could find as an &lt;span class="caps"&gt;RPM&lt;/span&gt; for CentOS was
0.6.16, from
&lt;a href="http://patches.fedorapeople.org/oldnode/stable/"&gt;http://patches.fedorapeople.org/oldnode/stable/&lt;/a&gt;.
That&amp;#8217;s the one that puppetlabs currently uses in their
&lt;a href="https://github.com/puppetlabs/puppetlabs-nodejs"&gt;puppetlabs-nodejs&lt;/a&gt;
module. There is, however, a nodejs 0.9.5 &lt;span class="caps"&gt;RPM&lt;/span&gt; in the Fedora Rawhide (19)
repository. Below are some patches to that specfile, and the specfile
for its v8 dependency, to get them to build on CentOS 6. You can also
find the full specfiles on my &lt;a href="https://github.com/jantman/specfiles"&gt;github specfile
repository&lt;/a&gt;. I had originally
wanted to get them built on CentOS 5 as well, but after following the
dependency tree from nodejs to http-parser to gyp, and then finding
issues in the gyp source that are incompatible with CentOS 5&amp;#8217;s python
2.4, I gave up on that&amp;nbsp;target.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;nodejs.spec&lt;/strong&gt;, diff from Fedora Rawhide nodejs-0.9.5-9.fc18.src.rpm,
buildID=377755 (&lt;a href="https://raw.github.com/jantman/specfiles/master/nodejs.spec"&gt;full
specfile&lt;/a&gt;)&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="gh"&gt;diff --git a/nodejs.spec b/nodejs.spec&lt;/span&gt;
&lt;span class="gh"&gt;index 050ed86..86c0f4b 100644&lt;/span&gt;
&lt;span class="gd"&gt;--- a/nodejs.spec&lt;/span&gt;
&lt;span class="gi"&gt;+++ b/nodejs.spec&lt;/span&gt;
&lt;span class="gu"&gt;@@ -1,6 +1,6 @@&lt;/span&gt;
 Name: nodejs
 Version: 0.9.5
&lt;span class="gd"&gt;-Release: 9%{?dist}&lt;/span&gt;
&lt;span class="gi"&gt;+Release: 10%{?dist}&lt;/span&gt;
 Summary: JavaScript runtime
 License: &lt;span class="caps"&gt;MIT&lt;/span&gt; and &lt;span class="caps"&gt;ASL&lt;/span&gt; 2.0 and &lt;span class="caps"&gt;ISC&lt;/span&gt; and &lt;span class="caps"&gt;BSD&lt;/span&gt;
 Group: Development/Languages
&lt;span class="gu"&gt;@@ -25,7 +25,7 @@ Source6: nodejs-fixdep&lt;/span&gt;
 BuildRequires: v8-devel &amp;gt;= %{v8_ge}
 BuildRequires: http-parser-devel &amp;gt;= 2.0
 BuildRequires: libuv-devel
&lt;span class="gd"&gt;-BuildRequires: c-ares-devel&lt;/span&gt;
&lt;span class="gi"&gt;+BuildRequires: c-ares-devel &amp;gt;= 1.9.0&lt;/span&gt;
 BuildRequires: zlib-devel
 # Node.js requires some features from openssl 1.0.1 for &lt;span class="caps"&gt;SPDY&lt;/span&gt; support
 BuildRequires: openssl-devel &amp;gt;= 1:1.0.1
&lt;span class="gu"&gt;@@ -165,9 +165,13 @@ cp -p common.gypi %{buildroot}%{_datadir}/node&lt;/span&gt;

 %files docs
 %{_defaultdocdir}/%{name}-docs-%{version}
&lt;span class="gd"&gt;-%doc &lt;span class="caps"&gt;LICENSE&lt;/span&gt;&lt;/span&gt;

 %changelog
&lt;span class="gi"&gt;+* Thu Jan 31 2013 Jason Antman  - 0.9.5-10&lt;/span&gt;
&lt;span class="gi"&gt;+- specify build requirement of c-ares-devel &amp;gt;= 1.9.0&lt;/span&gt;
&lt;span class="gi"&gt;+- specify build requirement of libuv-devel 0.9.4&lt;/span&gt;
&lt;span class="gi"&gt;+- remove duplicate %doc &lt;span class="caps"&gt;LICENSE&lt;/span&gt; that was causing cpio &amp;#39;Bad magic&amp;#39; error on CentOS6&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
 * Sat Jan 12 2013 &lt;span class="caps"&gt;T.C.&lt;/span&gt; Hollingsworth  - 0.9.5-9
 - fix brown paper bag bug in requires generation script
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;v8.spec&lt;/strong&gt;, diff from Fedora Rawhide 3.13.7.5-2 (&lt;a href="https://raw.github.com/jantman/specfiles/master/v8.spec"&gt;full
specfile&lt;/a&gt;)&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="gd"&gt;--- v8.spec.orig       2013-01-26 16:03:18.000000000 -0500&lt;/span&gt;
&lt;span class="gi"&gt;+++ v8.spec     2013-01-31 09:04:51.068029459 -0500&lt;/span&gt;
&lt;span class="gu"&gt;@@ -21,9 +21,11 @@&lt;/span&gt;

 # %%global svnver 20110721svn8716

&lt;span class="gi"&gt;+%{!?python_sitelib: %define python_sitelib %(%{__python} -c &amp;quot;import distutils.sysconfig as d; print d.get_python_lib()&amp;quot;)}&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
 Name:          v8
 Version:       %{somajor}.%{sominor}.%{sobuild}.%{sotiny}
&lt;span class="gd"&gt;-Release:       2%{?dist}&lt;/span&gt;
&lt;span class="gi"&gt;+Release:       5%{?dist}&lt;/span&gt;
 Epoch:         1
 Summary:       JavaScript Engine
 Group:         System Environment/Libraries
&lt;span class="gu"&gt;@@ -32,7 +34,7 @@&lt;/span&gt;
 Source0:       http://commondatastorage.googleapis.com/chromium-browser-official/v8-%{version}.tar.bz2
 BuildRoot:     %{_tmppath}/%{name}-%{version}-%{release}-root-%(%{__id_u} -n)
 ExclusiveArch: %{ix86} x86_64 %{arm}
&lt;span class="gd"&gt;-BuildRequires: scons, readline-devel, libicu-devel&lt;/span&gt;
&lt;span class="gi"&gt;+BuildRequires: scons, readline-devel, libicu-devel, ncurses-devel&lt;/span&gt;

 %description
 V8 is Google&amp;#39;s open source JavaScript engine. V8 is written in C++ and is used 
&lt;span class="gu"&gt;@@ -51,8 +53,13 @@&lt;/span&gt;
 %setup -q -n %{name}-%{version}

 # -fno-strict-aliasing is needed with gcc 4.4 to get past some ugly code
&lt;span class="gd"&gt;-PARSED_OPT_FLAGS=`echo \&amp;#39;$RPM_OPT_FLAGS -fPIC -fno-strict-aliasing -Wno-unused-parameter -Wno-error=strict-overflow -Wno-error=unused-local-typedefs -Wno-unused-but-set-variable\&amp;#39;| sed &amp;quot;s/ /&amp;#39;,/g&amp;quot; | sed &amp;quot;s/&amp;#39;,/&amp;#39;, &amp;#39;/g&amp;quot;`&lt;/span&gt;
&lt;span class="gi"&gt;+%if 0%{?el5}&lt;/span&gt;
&lt;span class="gi"&gt;+PARSED_OPT_FLAGS=`echo \&amp;#39;$RPM_OPT_FLAGS -fPIC -fno-strict-aliasing -Wno-unused-parameter -lncurses\&amp;#39;| sed &amp;quot;s/ /&amp;#39;,/g&amp;quot; | sed &amp;quot;s/&amp;#39;,/&amp;#39;, &amp;#39;/g&amp;quot;`&lt;/span&gt;
&lt;span class="gi"&gt;+sed -i &amp;quot;s|&amp;#39;-O3&amp;#39;,|$PARSED_OPT_FLAGS,|g&amp;quot; SConstruct&lt;/span&gt;
&lt;span class="gi"&gt;+%else&lt;/span&gt;
&lt;span class="gi"&gt;+PARSED_OPT_FLAGS=`echo \&amp;#39;$RPM_OPT_FLAGS -fPIC -fno-strict-aliasing -Wno-unused-parameter -Wno-error=strict-overflow -Wno-unused-but-set-variable\&amp;#39;| sed &amp;quot;s/ /&amp;#39;,/g&amp;quot; | sed &amp;quot;s/&amp;#39;,/&amp;#39;, &amp;#39;/g&amp;quot;`&lt;/span&gt;
 sed -i &amp;quot;s|&amp;#39;-O3&amp;#39;,|$PARSED_OPT_FLAGS,|g&amp;quot; SConstruct
&lt;span class="gi"&gt;+%endif&lt;/span&gt;

 # clear spurious executable bits
 find . \( -name \*.cc -o -name \*.h -o -name \*.py \) -a -executable   
&lt;span class="gu"&gt;@@ -198,6 +205,17 @@&lt;/span&gt;
 %{python_sitelib}/j*.py*

 %changelog
&lt;span class="gi"&gt;+* Thu Jan 31 2013 Jason Antman  - 1:3.13.7.5-5&lt;/span&gt;
&lt;span class="gi"&gt;+- remove -Werror=unused-local-typedefs on cent6&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+* Wed Jan 30 2013 Jason Antman  - 1:3.13.7.5-4&lt;/span&gt;
&lt;span class="gi"&gt;+- define python_sitelib if it isn&amp;#39;t already (CentOS 5)&lt;/span&gt;
&lt;span class="gi"&gt;+&lt;/span&gt;
&lt;span class="gi"&gt;+* Wed Jan 30 2013 Jason Antman  - 1:3.13.7.5-3&lt;/span&gt;
&lt;span class="gi"&gt;+- pull 3.13.7.5-2 &lt;span class="caps"&gt;SRPM&lt;/span&gt; from Fedora 19 Koji most recent build&lt;/span&gt;
&lt;span class="gi"&gt;+- add ncurses-devel BuildRequires&lt;/span&gt;
&lt;span class="gi"&gt;+- modify PARSED_OPT_FLAGS to work with g++ 4.1.2 on CentOS 5&lt;/span&gt;
&lt;span class="gi"&gt;+ &lt;/span&gt;
 * Sat Jan 26 2013 &lt;span class="caps"&gt;T.C.&lt;/span&gt; Hollingsworth  - 1:3.13.7.5-2
 - rebuild for icu-50
 - ignore new &lt;span class="caps"&gt;GCC&lt;/span&gt; 4.8 warning
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Thu, 31 Jan 2013 14:13:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2013-01-31:2013/01/rpm-spec-files-for-nodejs-0-9-5-and-v8-on-centos-5/</guid><category>build</category><category>centos</category><category>node</category><category>nodejs</category><category>package</category><category>packaging</category><category>redhat</category><category>RHEL</category><category>rpm</category><category>specfile</category></item><item><title>Fedora Linux and OSX Dual Boot on Mid-2010 (6,2) 15â MacBook ProÂ Laptop</title><link>http://blog.jasonantman.com/2013/01/fedora-linux-and-osx-dual-boot-on-mid-2010-62-15-macbook-pro-laptop/</link><description>&lt;p&gt;As part of the transition from a contractor to a full-time employee of
&lt;a href="http://www.cmgdigital.com"&gt;Cox Media Group Digital &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Strategy&lt;/a&gt; (check
out our &lt;a href="https://github.com/cmgdigital"&gt;github&lt;/a&gt;), I&amp;#8217;ve been issued a
&lt;a href="http://support.apple.com/kb/SP582"&gt;Mid-2010 (6,2)&lt;/a&gt; 15&amp;#8221; &lt;a href="http://en.wikipedia.org/wiki/Macbook_pro#Technical_specifications_2"&gt;MacBook
Pro&lt;/a&gt;
laptop, to replace my current &lt;a href="http://support.apple.com/kb/SP11"&gt;Early-2008
(3,1)&lt;/a&gt; MacPro desktop. The desktop is
currently running &lt;a href="http://fedoraproject.org/"&gt;Fedora&lt;/a&gt; 17, dual-boot with
with Mac &lt;span class="caps"&gt;OS&lt;/span&gt; X (left in place for firmware updates and emergencies) using
the &lt;a href="http://www.rodsbooks.com/refind/index.html"&gt;rEFInd boot manager&lt;/a&gt; to
choose between the two OSes. It took me two days to get this working
right on my desktop, but it had been my plan to duplicate this setup on
my laptop. I found a lot of conflicting information online, but I
decided to give it a&amp;nbsp;try.&lt;/p&gt;
&lt;p&gt;Well, I have Fedora 18 and &lt;span class="caps"&gt;OS&lt;/span&gt; X 10.8 dual-booting on the laptop, but not
as planned. After a day and a half of research, troubleshooting and
re-installs, here&amp;#8217;s what I found to actually work, in the hope that
nobody else will go through the ordeal I went through. Following that
are some notes about the new Fedora 18 installer (Anaconda 18),
especially important for anyone who&amp;#8217;s used Linux for a while. To those
who are new to Linux, don&amp;#8217;t be dissuaded by the above. Most of the
frustration I experienced is because I&amp;#8217;ve been using Linux for a
relatively long time (about 10 years), had my own ideas about exactly
how I wanted things setup (which are decidedly &lt;em&gt;not&lt;/em&gt; supported by
Fedora), and had some assumptions about the installation process based
on earlier&amp;nbsp;versions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How to get it&amp;nbsp;working:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Forget about rEFInd. This had been the original advice from &lt;a href="http://mjg59.dreamwidth.org/"&gt;Matthew
Garrett&lt;/a&gt;,
&lt;a href="https://twitter.com/mjg59"&gt;@mjg59&lt;/a&gt;, kernel coder, contributor to the
Anaconda project, and all-around authority on booting Linux on &lt;span class="caps"&gt;EFI&lt;/span&gt;/&lt;span class="caps"&gt;UEFI&lt;/span&gt;
hardware. My advice, and the method that worked for&amp;nbsp;me:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Shrink your Mac partitions and leave as much free space as you want
    for Fedora. using the Disk Utility tool in &lt;span class="caps"&gt;OS&lt;/span&gt; X (I also created an
    &lt;span class="caps"&gt;8GB&lt;/span&gt; &lt;span class="caps"&gt;VFAT&lt;/span&gt; partition that both OSes can read/write&amp;nbsp;to).&lt;/li&gt;
&lt;li&gt;&lt;a href="http://fedoraproject.org/en/get-fedora"&gt;Download Fedora 18&lt;/a&gt; 64-bit
    &lt;span class="caps"&gt;DVD&lt;/span&gt; image, I chose the &lt;span class="caps"&gt;KDE&lt;/span&gt; version. Verify the sha256 sum if you
    want (they don&amp;#8217;t have a readily visible link to the checksum file.
    Copy the download link, paste it into your address bar and remove
    the filename. You should get a directory index that includes a
    &lt;code&gt;-CHECKSUM&lt;/code&gt; file.&lt;/li&gt;
&lt;li&gt;Per the Installation Guide&amp;#8217;s &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/18/html/Installation_Guide/Making_USB_Media-UNIX_Linux.html"&gt;Making Fedora &lt;span class="caps"&gt;USB&lt;/span&gt; Media
    page&lt;/a&gt;,
    use &lt;code&gt;liveusb-creator&lt;/code&gt; to setup the installation image on the &lt;span class="caps"&gt;USB&lt;/span&gt;
    flash drive (I needed to start it with the &lt;code&gt;--reset-mbr&lt;/code&gt; option).
    You can also use other tools (dd if you&amp;#8217;re not on a Fedora-based
    distro), or a &lt;span class="caps"&gt;DVD&lt;/span&gt;, but this is the method I&amp;nbsp;chose.&lt;/li&gt;
&lt;li&gt;Due to a &lt;a href="https://fedorahosted.org/liveusb-creator/ticket/810"&gt;bug in
    liveusb-creator&lt;/a&gt;,
    you may need to manually edit &lt;code&gt;/EFI/boot/grub.cfg&lt;/code&gt; on the created
    &lt;span class="caps"&gt;USB&lt;/span&gt; stick if grub gives you a file not found error. If that happens,
    please see my bug report above for the action to take (in short, you
    need to mount the &lt;span class="caps"&gt;USB&lt;/span&gt; stick, &lt;code&gt;chmod u+w /EFI/boot/grub.cfg&lt;/code&gt; then
    edit that file and replace every occurrence of &amp;#8220;isolinux&amp;#8221; with
    &amp;#8220;syslinux&amp;#8221; and every occurrence of
    &amp;#8220;root=live:&lt;span class="caps"&gt;LABEL&lt;/span&gt;=Fedora-18-x86_64-Live-&lt;span class="caps"&gt;KDE&lt;/span&gt;.iso&amp;#8221; with&amp;nbsp;&amp;#8220;root=live:&lt;span class="caps"&gt;LABEL&lt;/span&gt;=&lt;span class="caps"&gt;LIVE&lt;/span&gt;&amp;#8221;).&lt;/li&gt;
&lt;li&gt;Boot the &lt;span class="caps"&gt;USB&lt;/span&gt; drive (use the alt key when you turn on the laptop to
    select the &lt;span class="caps"&gt;USB&lt;/span&gt; drive) and just install Fedora normally, letting it
    do its thing. Select a boot disk and let it put &lt;span class="caps"&gt;GRUB2&lt;/span&gt; on the &lt;span class="caps"&gt;EFI&lt;/span&gt;&amp;nbsp;partition.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When you boot, it will boot to &lt;span class="caps"&gt;GRUB&lt;/span&gt;. There will be some options for Mac
&lt;span class="caps"&gt;OS&lt;/span&gt; there, but they don&amp;#8217;t work (more on that below). If you want to boot
Mac, hold down the alt/option key when you power on the laptop, which
will bring you to the boot disk selector and you can pick the Mac disk.
I know it&amp;#8217;s not pretty or ideal, but it&amp;#8217;s the best option right&amp;nbsp;now.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Making it&amp;nbsp;Better:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span class="caps"&gt;GRUB2&lt;/span&gt; tries to automatically detect other OSes and configure them in the
boot loader (this is done through &lt;code&gt;/etc/grub.d/30_os-prober&lt;/code&gt;, commonly
just referred to as &lt;code&gt;os-prober&lt;/code&gt;). It tries to boot Mac directly through
the xnu_kernel64 module, which not only isn&amp;#8217;t installed on the boot
partition by default, but just doesn&amp;#8217;t work with at least Mountain Lion
(10.8). So getting &lt;span class="caps"&gt;GRUB&lt;/span&gt; to boot Mac means either having the bugs in the
xnu module fixed, or figuring out how to setup a chainloader to boot
from &lt;span class="caps"&gt;GRUB&lt;/span&gt; to Mac. The latter is probably the method I&amp;#8217;ll investigate,
but for now, since I rarely use Mac, I&amp;#8217;m happy having to use the alt key
at boot to get there. To remove the annoying, broken Mac &lt;span class="caps"&gt;OS&lt;/span&gt; options from
the grub screen, run the following commands as root (they assume you
have your &lt;span class="caps"&gt;EFI&lt;/span&gt; partition mounted at &lt;code&gt;/boot/efi&lt;/code&gt; which I believe Fedora
should do by&amp;nbsp;default:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;cp /boot/efi/EFI/fedora/grub.cfg /boot/efi/EFI/fedora/grub.cfg.bak
&lt;span class="nb"&gt;echo&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;GRUB_DISABLE_OS_PROBER=&amp;quot;true&amp;quot;&amp;#39;&lt;/span&gt; &amp;gt;&amp;gt; /etc/default/grub
grub2-mkconfig &amp;gt; /boot/efi/&lt;span class="caps"&gt;EFI&lt;/span&gt;/fedora/grub.cfg
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Thoughts on the Fedora 18 Anaconda&amp;nbsp;Installer&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I found a couple of issues with the new Anaconda 18 installer that were
either unweildy or confusing for someone who&amp;#8217;s been installing Linux for
a long time. Overall, the new installer is very nice. It has a clean,
even elegant &lt;span class="caps"&gt;UI&lt;/span&gt;, a relatively nice flow from start to completion, and is
certainly beginner-friendly. It has fewer options than any Linux
installer I&amp;#8217;ve ever used before - not even options for package
selection, firewall or SELinux configuration, etc. - but I guess this is
in line with the goal of making Fedora a desktop &lt;span class="caps"&gt;OS&lt;/span&gt; for the masses. I
would have appreciated an &amp;#8220;advanced mode&amp;#8221; installer that was more like
Fedora 17 (or even much older versions), but I guess I&amp;#8217;m an edge case,
at least in the Fedora community. However, I did find two things
especially difficult, both related to the fact that my laptop has two
main drives (a &lt;span class="caps"&gt;500GB&lt;/span&gt; hard drive and a &lt;span class="caps"&gt;120GB&lt;/span&gt;&amp;nbsp;&lt;span class="caps"&gt;SSD&lt;/span&gt;):&lt;/p&gt;
&lt;p&gt;First, the installer prompted me to select a &amp;#8220;boot disk&amp;#8221;. I guess I
should have read the installation guide, but I assumed that nomenclature
translated to either &amp;#8220;which disk should the automatic partitiioning put
yout &lt;code&gt;/boot&lt;/code&gt; partition on&amp;#8221; or &amp;#8220;which disk should I set the bootable flag
on in the partition table&amp;#8221;. In fact, it means &amp;#8220;which disk should I put
&lt;span class="caps"&gt;GRUB&lt;/span&gt; on the &lt;span class="caps"&gt;EFI&lt;/span&gt; partition of&amp;#8221;. I installed, rebooted, and was shocked -
and somewhat distressed - to boot directly to &lt;span class="caps"&gt;GRUB2&lt;/span&gt; instead of the
rEFInd installation I had setup. The installer didn&amp;#8217;t have any of the
previously-customary &amp;#8220;warning: this will overwrite your &lt;span class="caps"&gt;MBR&lt;/span&gt;/&lt;span class="caps"&gt;EFI&lt;/span&gt; boot
partition&amp;#8221; notices, so I felt safe letting it continue. It turned out
that this was the way I ended up going, and it also turns out that
there&amp;#8217;s a bug in Anaconda that makes it fail installation if you tell it
not to write a bootloader to disk (though it&amp;#8217;s patched by one line of
Python code). But I was deeply distressed that - contrary to the
experience of every, admittedly more complicated, Linux installer I&amp;#8217;d
used before - the Fedora 18 installer overwrote my &lt;span class="caps"&gt;EFI&lt;/span&gt; bootloader
(analogous to overwriting the &lt;span class="caps"&gt;MBR&lt;/span&gt; on a &lt;span class="caps"&gt;BIOS&lt;/span&gt; boot machine) without ever
warning me or asking for a&amp;nbsp;confirmation.&lt;/p&gt;
&lt;p&gt;Secondly, the partitioning tool is clearly designed for only one
destination disk. The overview screen lists configured partitions by
label and mount point, but not by physical device, so figuring out which
partitions are on which physical disks takes a click on each and every
partition to view that information in the detail panel. When you create
a new partition, it&amp;#8217;s automatically put in a &lt;span class="caps"&gt;LVM&lt;/span&gt; volume group spanning
all disks. Changing the target of the automatically created volume group
requires a few clicks, as does changing the physical disks backing any
new volume groups. To assign a newly created partition to a specific
disk, you have to click on an unlabeled &amp;#8220;tool&amp;#8221; icon under the list of
partitions, far away from the information on the partition in question.
It&amp;#8217;s a nice interface for someone who clicks the &amp;#8220;partition
automatically&amp;#8221; button, or who just knows they want to add &amp;#8220;an extra
partition&amp;#8221;, but for anyone who has a specific layout in mind (like
having &lt;code&gt;/&lt;/code&gt;, &lt;code&gt;/boot&lt;/code&gt; and &lt;code&gt;/var&lt;/code&gt;, specifically sized, on the &lt;span class="caps"&gt;SSD&lt;/span&gt; and
&lt;code&gt;/home&lt;/code&gt; on the rotating disk) it takes about 4-5 more clicks and dialogs
to add a partition than the last Fedora installer did. Mainly, it&amp;#8217;s
lacking any sort of Advanced Mode for partitioning that allows the user
to quickly and accurately layout a more complex partitioning&amp;nbsp;scheme.&lt;/p&gt;
&lt;p&gt;Below are some screenshots from the Fedora 17 and Fedora 18 Installation
Guides, which contrast both the overview of all partitions and the
individual partition&amp;nbsp;settings:&lt;/p&gt;
&lt;p&gt;Fedora 18 Overview, from &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/18/html/Installation_Guide/s1-diskpartitioning-x86.html"&gt;9.13. Creating a Custom Partition
Layout&lt;/a&gt;:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="http://www.dedoimedo.com/images/computers_years/2013_1/fedora-18-installer-configure-partitions.jpg" /&gt;  &lt;/p&gt;
&lt;p&gt;Fedora 17 Overview, from &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/17/html/Installation_Guide/s1-diskpartitioning-x86.html"&gt;9.14. Creating a Custom Layout or Modifying
the Default
Layout&lt;/a&gt;:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="http://docs.fedoraproject.org/en-US/Fedora/17/html/Installation_Guide/images/diskpartitioning/ddmain.png" /&gt;  &lt;/p&gt;
&lt;p&gt;Fedora 18 Partition Creation/Editing, from &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/18/html/Installation_Guide/Create_LVM-x86.html"&gt;9.13.3. Create &lt;span class="caps"&gt;LVM&lt;/span&gt; Logical
Volume&lt;/a&gt;:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="http://docs.fedoraproject.org/en-US/Fedora/18/html/Installation_Guide/images/diskpartitioning/lvm-pv.png" /&gt;  &lt;/p&gt;
&lt;p&gt;Fedora 17 Partition Creation/Editing, from &lt;a href="http://docs.fedoraproject.org/en-US/Fedora/17/html/Installation_Guide/Adding_Partitions-x86.html"&gt;9.14.2. Adding
Partitions&lt;/a&gt;:  &lt;/p&gt;
&lt;p&gt;&lt;img alt="image" src="http://docs.fedoraproject.org/en-US/Fedora/17/html/Installation_Guide/images/diskpartitioning/part-add.png" /&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Mon, 21 Jan 2013 12:13:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2013-01-21:2013/01/fedora-linux-and-osx-dual-boot-on-mid-2010-62-15-macbook-pro-laptop/</guid><category>bootloader</category><category>efi</category><category>fedora</category><category>gpt</category><category>grub</category><category>installation</category><category>laptop</category><category>mac</category><category>macbook</category><category>os x</category></item><item><title>Fedora Init Script SpecificationÂ Summary</title><link>http://blog.jasonantman.com/2013/01/fedora-init-script-specification-summary/</link><description>&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; 2014-12-16: I&amp;#8217;m leaving this here for historical reasons, and since
some older &lt;span class="caps"&gt;OS&lt;/span&gt; versions still need properly-written init scripts. Since starting
to use &lt;a href="https://www.archlinux.org/"&gt;Arch Linux&lt;/a&gt; on my laptop, I&amp;#8217;ve become a
convert to the &lt;a href="http://www.freedesktop.org/wiki/Software/systemd/"&gt;systemd&lt;/a&gt;
world. I know this is a &lt;a href="http://en.wikipedia.org/wiki/Systemd#Criticism"&gt;hot topic&lt;/a&gt;
and has sparked a lot of controversy. While I agree with some of the arguments
in principal, I strongly feel that systemd provides the interface that
Linux needs in modern times, and provides a unified solution to many problems
that were previously solved in myriad ways inside init scripts. In short,
if your distro supports systemd, I&amp;#8217;d recommend to skip past this page
and go ahead and write a &lt;a href="http://www.freedesktop.org/software/systemd/man/systemd.unit.html"&gt;unit file&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;ve been deploying some new software lately (specifically
&lt;a href="https://github.com/marisaseal/selenesse"&gt;selenesse&lt;/a&gt;, which combines
&lt;a href="http://seleniumhq.org/"&gt;Selenium&lt;/a&gt; and &lt;a href="http://fitnesse.org/"&gt;fitnesse&lt;/a&gt;,
&lt;a href="http://en.wikipedia.org/wiki/Xvfb"&gt;xvfb&lt;/a&gt;). None of these seem to come
with init scripts to run as daemons, and the quality of the few
Fedora/RedHat/CentOS init scripts I was able to find was quite poor. The
Fedora project has a &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript"&gt;Specification for SysV-style Init Scripts in their
Packaging wiki&lt;/a&gt;,
which specifies what a Fedora/RedHat/CentOS init script should look
like, in excruciating detail. What follows is an overview of the more
important points, which I&amp;#8217;m using to develop or modify the scripts I&amp;#8217;m
currently working&amp;nbsp;on.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Scripts must be put in &lt;code&gt;/etc/rc.d/init.d&lt;/code&gt;, not in the &lt;code&gt;/etc/init.d&lt;/code&gt;
    symlink. They should have 0755&amp;nbsp;permissions.&lt;/li&gt;
&lt;li&gt;Scripts must have a Fedora-style &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#Chkconfig_Header"&gt;chkconfig
    header&lt;/a&gt;
    (&amp;#8220;chkconfig:&amp;#8221;, &amp;#8220;description:&amp;#8221; lines), and may have an &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#LSB_Header"&gt;&lt;span class="caps"&gt;LSB&lt;/span&gt;-style
    header&lt;/a&gt;
    (&lt;span class="caps"&gt;BEGIN&lt;/span&gt; &lt;span class="caps"&gt;INIT&lt;/span&gt; &lt;span class="caps"&gt;INFO&lt;/span&gt;/&lt;span class="caps"&gt;END&lt;/span&gt; &lt;span class="caps"&gt;INIT&lt;/span&gt; &lt;span class="caps"&gt;INFO&lt;/span&gt;). See &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#Initscript_template"&gt;Initscript
    template&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;Scripts &lt;strong&gt;must&lt;/strong&gt; make use of a lockfile in &lt;code&gt;/var/lock/subsys/&lt;/code&gt;, and
    the name of the lockfile must be the same as the name of the init
    script. (There is a technical reason for this relating to how sysv
    init terminates daemons at shutdown). The lockfile should be touched
    when the daemon successfully starts, and removed when it
    successfully&amp;nbsp;stops.&lt;/li&gt;
&lt;li&gt;Init scripts should not depend on any environment variables set
    outside the script. They should operate gracefully with an
    empty/uninitialized environment (or only &lt;span class="caps"&gt;LANG&lt;/span&gt; and &lt;span class="caps"&gt;TERM&lt;/span&gt; set and a &lt;span class="caps"&gt;CWD&lt;/span&gt;
    of &lt;code&gt;/&lt;/code&gt;, as enforced by &lt;code&gt;service(8)&lt;/code&gt;, or with a full environment if
    they are called directly by a&amp;nbsp;user.&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#Required_Actions"&gt;Required&amp;nbsp;actions&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;all of the following actions are required, and have specific&amp;nbsp;definitions:&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;start&lt;/strong&gt;: starts the&amp;nbsp;service&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;stop&lt;/strong&gt;: stops the&amp;nbsp;service&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;restart&lt;/strong&gt;: stop and restart the service if the service is
    already running, otherwise just start the&amp;nbsp;service&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;condrestart (and try-restart)&lt;/strong&gt;: restart the service if the
    service is already running, if not, do&amp;nbsp;nothing&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;reload&lt;/strong&gt;: reload the configuration of the service without
    actually stopping and restarting the service (if the service
    does not support this, do&amp;nbsp;nothing)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;force-reload&lt;/strong&gt;: reload the configuration of the service and
    restart it so that it takes&amp;nbsp;effect&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;status&lt;/strong&gt;: print the current status of the&amp;nbsp;service&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;usage&lt;/strong&gt;: by default, if the initscript is run without any
    action, it should list a &amp;#8220;usage message&amp;#8221; that has all actions
    (intended for&amp;nbsp;use)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;There are specified exit codes for &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#Exit_Codes_for_the_Status_Action"&gt;status
    actions&lt;/a&gt;
    and &lt;a href="http://fedoraproject.org/wiki/Packaging:SysVInitScript#Exit_Codes_for_non-Status_Actions"&gt;non-status
    actions&lt;/a&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;They must &amp;#8220;behave sensibly&amp;#8221;. I&amp;#8217;ve found this to be one of the
    biggest problems with homegrown init scripts. If &lt;code&gt;servicename start&lt;/code&gt;
    is called while the service is already running, it should simply
    exit 0. Likewise if the service is already stopped. Init scripts
    &lt;strong&gt;must not kill unrelated processes&lt;/strong&gt;. I don&amp;#8217;t know how many times
    I&amp;#8217;ve seen scripts that kill every java or python process on a&amp;nbsp;machine.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I intend to use this as a quick checklist when developing or evaluating
init scripts for RedHat/Fedora based systems. In my experience, the
biggest problems with most init scripts revolve around poor handling of
&lt;span class="caps"&gt;PID&lt;/span&gt; files and lockfiles,&amp;nbsp;mainly:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Killing processes other than the one that the script started (i.e.
    killing all java or python processes), usually because the &lt;span class="caps"&gt;PID&lt;/span&gt; isn&amp;#8217;t
    tracked at&amp;nbsp;start&lt;/li&gt;
&lt;li&gt;Starting a second instance of the subsystem because lockfiles aren&amp;#8217;t
    used, or the status function is&amp;nbsp;broken.&lt;/li&gt;
&lt;li&gt;improper exit&amp;nbsp;codes&lt;/li&gt;
&lt;li&gt;either explicitly relying on environment variables (and therefore
    breaking when called through &lt;code&gt;service(8)&lt;/code&gt;), or conversely, not
    cleaning/resetting environment variables that are used by dependent
    code or&amp;nbsp;processes.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Thu, 03 Jan 2013 11:30:00 -0500</pubDate><guid>tag:blog.jasonantman.com,2013-01-03:2013/01/fedora-init-script-specification-summary/</guid><category>centos</category><category>fedora</category><category>init</category><category>redhat</category><category>startup</category></item><item><title>Random Links for Wednesday, OctoberÂ 24th</title><link>http://blog.jasonantman.com/2012/10/random-links-for-wednesday-october-24th/</link><description>&lt;p&gt;Some random interesting links from Slashdot for&amp;nbsp;today:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://news.slashdot.org/story/12/10/23/2038220/the-greatest-battle-of-the-personal-computing-revolution-lies-ahead"&gt;The Greatest Battle of the Personal Computing Revolution Lies Ahead
    -
    Slashdot&lt;/a&gt;.
    A bit of a rant, but makes some good points that are close to my
    heart, and unfortunately far from the thoughts of many&amp;nbsp;non-techies.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tos-dr.info/"&gt;Terms of Service; Didn&amp;#8217;t Read&lt;/a&gt; - an
    interesting&amp;nbsp;project&lt;/li&gt;
&lt;li&gt;&lt;a href="http://yro.slashdot.org/story/12/10/21/208206/how-patent-trolls-harm-the-economy"&gt;How Patent Trolls Harm the Economy -
    Slashdot&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;another issue close to my&amp;nbsp;heart&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.datacenterknowledge.com/archives/2012/10/17/how-google-cools-its-armada-of-servers/"&gt;How Google Cools Its Armada of Servers Â» Data Center&amp;nbsp;Knowledge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.google.com/about/datacenters/gallery/#/"&gt;Data centers â Google Data
    centers&lt;/a&gt; - A
    photo tour of Google data centers, by Google, along with a &lt;a href="https://plus.google.com/+google/posts/Gk8ScjPX23n"&gt;Google+
    post&lt;/a&gt; about the
    architecture photographer who did this&amp;nbsp;work.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://tech.slashdot.org/story/12/10/22/0518231/darpa-funds-a-300-software-defined-radio-for-hackers"&gt;&lt;span class="caps"&gt;DARPA&lt;/span&gt; Funds a $300 Software-Defined Radio For Hackers -
    Slashdot&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;way&amp;nbsp;cool.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://map.honeynet.org/"&gt;Honeynet Map&lt;/a&gt; - &amp;#8220;realtime&amp;#8221; map of
    cybersecurity incidents, from the Honeynet&amp;nbsp;Project.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://science.slashdot.org/story/12/10/17/1741225/malware-is-rampant-on-medical-devices-in-hospitals"&gt;Malware Is &amp;#8216;Rampant&amp;#8217; On Medical Devices In Hospitals -
    Slashdot&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;a bit scary, but unfortunately not that hard to guess. I&amp;#8217;ve seen
(probably unpatched) Windows 2000 workstations on hospital&amp;nbsp;networks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;To top off the scary posts: &lt;a href="http://news.slashdot.org/story/12/10/17/0325236/researcher-reverse-engineers-pacemaker-transmitter-to-deliver-deadly-shocks"&gt;Researcher Reverse-Engineers Pacemaker
    Transmitter To Deliver Deadly Shocks -&amp;nbsp;Slashdot&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Wed, 24 Oct 2012 12:01:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-10-24:2012/10/random-links-for-wednesday-october-24th/</guid><category>appliancization</category><category>cooling</category><category>datacenter</category><category>google</category><category>healthcare</category><category>legal</category><category>links</category><category>malware</category><category>pacemaker</category><category>patents</category><category>radio</category><category>SDR</category><category>security</category></item><item><title>Readable Nagios LogÂ Timestamps</title><link>http://blog.jasonantman.com/2012/10/readable-nagios-log-timestamps/</link><description>&lt;p&gt;If you&amp;#8217;re like me and most humans, the Nagios logfile timestamp (a unix
timestamp) isn&amp;#8217;t terribly useful when trying to grep through the logs
and correlate&amp;nbsp;events:  &lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="gp"&gt;#&lt;/span&gt; head -2 nagios.log
&lt;span class="go"&gt;[1350360000] &lt;span class="caps"&gt;LOG&lt;/span&gt; &lt;span class="caps"&gt;ROTATION&lt;/span&gt;: &lt;span class="caps"&gt;DAILY&lt;/span&gt;&lt;/span&gt;
&lt;span class="go"&gt;[1350360000] &lt;span class="caps"&gt;LOG&lt;/span&gt; &lt;span class="caps"&gt;VERSION&lt;/span&gt;: 2.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here&amp;#8217;s a nifty Perl one-liner that you can pipe your logs&amp;nbsp;through:  &lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="n"&gt;perl&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;pe&lt;/span&gt; &lt;span class="s"&gt;&amp;#39;s/(\\d+)/localtime($1)/e&amp;#39;&lt;/span&gt;  
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to get nicer output&amp;nbsp;like:  &lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="c1"&gt;# head -2 nagios.log&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Tue&lt;/span&gt; &lt;span class="n"&gt;Oct&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="mo"&gt;00&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;LOG&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;ROTATION&lt;/span&gt;:&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;DAILY&lt;/span&gt;&lt;/span&gt;
&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Tue&lt;/span&gt; &lt;span class="n"&gt;Oct&lt;/span&gt; &lt;span class="mi"&gt;16&lt;/span&gt; &lt;span class="mo"&gt;00&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="mo"&gt;00&lt;/span&gt; &lt;span class="mi"&gt;2012&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;LOG&lt;/span&gt;&lt;/span&gt; &lt;span class="n"&gt;&lt;span class="caps"&gt;VERSION&lt;/span&gt;:&lt;/span&gt; &lt;span class="mf"&gt;2.0&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Wed, 17 Oct 2012 05:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-10-17:2012/10/readable-nagios-log-timestamps/</guid><category>icinga</category><category>Nagios</category><category>perl</category><category>timestamp</category></item><item><title>Custom Tombstone and Road SignÂ Pictures</title><link>http://blog.jasonantman.com/2012/10/custom-tombstone-and-road-sign-pictures/</link><description>&lt;p&gt;On the lighter side, I found a few web sites by &lt;a href="http://www.pixbytom.com/"&gt;Tom
Blackwell&lt;/a&gt; that do some fun stuff with text
overlays on images. seems like a nice little tool for those
end-of-project powerpoints, or to send out the monthly &amp;#8220;most rolled-back
commits&amp;#8221;&amp;nbsp;medal&amp;#8230;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.tombstonebuilder.com/index.php"&gt;Custom Tombstone&amp;nbsp;Maker&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Image of tombstone, with 'Your Text Goes Here' carved into it" src="/GFX/my_tombstone.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.customroadsign.com/"&gt;CustomRoadSign.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Highway sign with 'Your Text Goes Here' written on it" src="/GFX/menusign.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.custommotelsign.com/"&gt;CustomMotelSign.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Motel-style sign with 'Your Text Goes Here' written on it" src="/GFX/motelsign.jpg" /&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="http://www.getamedal.com/"&gt;GetAMedal.com&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt="Gold medal with 'Your Text Goes Here' written on it" src="/GFX/medal.jpg" /&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Tue, 16 Oct 2012 07:02:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-10-16:2012/10/custom-tombstone-and-road-sign-pictures/</guid><category>graphics</category><category>humor</category><category>road sign</category><category>sign</category><category>tombstone</category></item><item><title>All-Mechanical Computer InstructionalÂ Video</title><link>http://blog.jasonantman.com/2012/10/all-mechanical-computer-instructional-video/</link><description>&lt;p&gt;I saw a link to &lt;a href="http://www.youtube.com/watch?v=s1i-dnAH9Y4"&gt;this YouTube
video&lt;/a&gt; shared on &lt;a href="http://everythingsysadmin.com/2012/10/mechanical-computer-instructio.html"&gt;Tom
Limoncelli&amp;#8217;s
blog&lt;/a&gt;.
It&amp;#8217;s a 1953 &lt;span class="caps"&gt;US&lt;/span&gt; Navy instructional video about an all-mechanical fire
control computer. Yes, I really mean a &lt;em&gt;computer&lt;/em&gt; that can solve
continuously changing 25-variable fire control problems using only
mechanical means (gears, cams, etc.). Think about it for a minute - it&amp;#8217;s
truly mind-boggling. And really gives one an amazing appreciation for
the power of a simple pocket calculator, and the amazing engineering
that went into solving these problems before electronic computers. I&amp;#8217;m
usually not much of a math geek, but I watched the whole 40 minute video
and was in awe of both the simple ability to use three arms and a pin to
multiply numbers, and the amazingly precise engineering and machining it
would take to translate various rotation inputs into landing a shell on
a moving ship miles away. It&amp;#8217;s a really good watch, and will probably
leave you astonished by both how far technology has come (and what we
take for granted every day), and by the fact that feats of engineering
like this one worked quite&amp;nbsp;well.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Fri, 12 Oct 2012 20:54:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-10-12:2012/10/all-mechanical-computer-instructional-video/</guid><category>mechanical computer</category></item><item><title>Pretty-Print a JSON response at the commandÂ line</title><link>http://blog.jasonantman.com/2012/10/pretty-print-a-json-response-at-the-command-line/</link><description>&lt;p&gt;I&amp;#8217;ve been doing some work with &lt;a href="http://www.rabbitmq.com/"&gt;RabbitMQ&lt;/a&gt;
lately, and have been doing some testing against its &lt;span class="caps"&gt;HTTP&lt;/span&gt;-based &lt;span class="caps"&gt;API&lt;/span&gt;,
which returns results in &lt;span class="caps"&gt;JSON&lt;/span&gt;. If you&amp;#8217;re looking to pretty-print a &lt;span class="caps"&gt;JSON&lt;/span&gt;
response for easier viewing, here&amp;#8217;s a nice way to do it at the command
line using Python and
&lt;a href="http://docs.python.org/library/json.html"&gt;json.tool&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;&lt;code&gt;curl http://username:pass@hostname:55672/api/overview | python -m json.tool&lt;/code&gt;&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Tue, 09 Oct 2012 14:44:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-10-09:2012/10/pretty-print-a-json-response-at-the-command-line/</guid><category>curl</category><category>json</category><category>python</category><category>rabbitmq</category></item><item><title>Nagstamon on FedoraÂ 17</title><link>http://blog.jasonantman.com/2012/10/nagstamon-on-fedora-17/</link><description>&lt;p&gt;Since I started my last job, I&amp;#8217;ve been using
&lt;a href="http://nagstamon.ifw-dresden.de/"&gt;Nagstamon&lt;/a&gt; on my workstation; it&amp;#8217;s a
really handy little system tray application that monitors a
Nagios/Icinga instance and shows status updates/summary in a handy
fashion, including flashing and (optionally) a sound alert when
something changes. Unfortunately, there doesn&amp;#8217;t seem to be a Fedora 17
package for it, though there is an entry on the &lt;a href="http://fedoraproject.org/wiki/Package_maintainers_wishlist#N-O"&gt;Fedora package
maintainers
wishlist&lt;/a&gt;.
The closest I was able to find is a
&lt;a href="http://pkgs.org/centos-6-rhel-6/repoforge-i386/nagstamon-0.9.7.1-2.el6.rf.noarch.rpm.html"&gt;repoforge/RPMforge&lt;/a&gt;
package of Nagstamon 0.9.7.1, along with a &lt;a href="http://apt.sw.be/source/nagstamon-0.9.7.1-2.rf.src.rpm"&gt;source
&lt;span class="caps"&gt;RPM&lt;/span&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are the steps to build that package on&amp;nbsp;F17:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Download and install
    &lt;a href="http://apt.sw.be/source/rpm-macros-rpmforge-0-6.rf.src.rpm"&gt;rpm-macros-rpmforge&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;As root, edit &lt;code&gt;/etc/rpm/macros.rpmforge&lt;/code&gt; and comment out the &lt;code&gt;%dist&lt;/code&gt;
    macro, so we&amp;#8217;ll still have the default &amp;#8220;fc17&amp;#8221; dist&amp;nbsp;tag.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;wget http://apt.sw.be/source/nagstamon-0.9.7.1-2.rf.src.rpm&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;rpmbuild &amp;#8212;rebuild&amp;nbsp;nagstamon-0.9.7.1-2.rf.src.rpm&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Hopefully this will help someone else as well. At the moment, Nagstamon
is actually up to version 0.9.9, so hopefully I&amp;#8217;ll build a newer package
sometime&amp;nbsp;soon.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Fri, 05 Oct 2012 07:37:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-10-05:2012/10/nagstamon-on-fedora-17/</guid><category>fedora</category><category>nagios. icinga</category><category>nagstamon</category><category>package</category><category>rpm</category></item><item><title>NewÂ Job</title><link>http://blog.jasonantman.com/2012/09/new-job/</link><description>&lt;p&gt;Today is my last day in my almost-year-long stint as a System
Administrator at &lt;a href="http://www.techtarget.com/"&gt;TechTarget&lt;/a&gt;. Monday, I
start a new contract-to-perm position as a Linux Engineer with &lt;a href="http://cmgdigital.com/"&gt;Cox
Media Group Digital &lt;span class="amp"&gt;&amp;amp;&lt;/span&gt; Strategy&lt;/a&gt;. I can&amp;#8217;t say a
whole lot about the new job, other than it will hopefully be a great
change for me, and they make heavy use of
&lt;a href="https://www.djangoproject.com/"&gt;Django&lt;/a&gt;. If you want to get a bit of an
idea of what they&amp;#8217;re about, here&amp;#8217;s a &lt;a href="https://github.com/coxmediagroup/jobs/blob/master/ethos.rst"&gt;document on their departmental
ethos&lt;/a&gt;.
Hopefully I&amp;#8217;ll be able to post more useful information here, and post
more often, in the future. I&amp;#8217;m really psyched about the new&amp;nbsp;gig.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Fri, 28 Sep 2012 12:36:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-09-28:2012/09/new-job/</guid><category>cmg</category><category>coxmedia</category><category>job</category><category>techtarget</category></item><item><title>Some questions from a tech interview with a big InternetÂ company</title><link>http://blog.jasonantman.com/2012/09/some-questions-from-a-tech-interview-with-a-big-internet-company/</link><description>&lt;p&gt;A while back, I did a technical phone screen with a big online &amp;#8220;social&amp;#8221;
company (I won&amp;#8217;t say who, but they&amp;#8217;re a household name, growing fast,
and doing cool things; that doesn&amp;#8217;t leave &lt;em&gt;too&lt;/em&gt; many options). I rarely
remember to write down interview questions, but I was cleaning out my
desk this morning and came by a ripped-out sheet of notebook paper with
a handful of the interview questions written on it. Most of them weren&amp;#8217;t
terribly difficult, or terribly unusual for competent technical
interviewers, but since I happen to actually have the list written down,
I though I&amp;#8217;d share it. I don&amp;#8217;t remember why the programming questions
are all Python; likely, I was asked to choose between Python (which I&amp;#8217;ve
used, though not lately), Ruby (which I can barely muddle my way through
reading on a good day), and something else I don&amp;#8217;t know. Here are some
of&amp;nbsp;them&amp;#8230;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What is an inode? What does it&amp;nbsp;store?&lt;/li&gt;
&lt;li&gt;What is a hard&amp;nbsp;link?&lt;/li&gt;
&lt;li&gt;What is the difference between a hard link and a soft&amp;nbsp;link?&lt;/li&gt;
&lt;li&gt;What is a list in&amp;nbsp;Python?&lt;/li&gt;
&lt;li&gt;Name some data structures that you&amp;#8217;d use in Python. Describe them,
    and tell me why you would use&amp;nbsp;them.&lt;/li&gt;
&lt;li&gt;How would you list all the man pages containing the keyword&amp;nbsp;&amp;#8220;date&amp;#8221;?&lt;/li&gt;
&lt;li&gt;If the &lt;code&gt;chmod&lt;/code&gt; binary had its permissions set to &lt;code&gt;000&lt;/code&gt;, how would
    you fix&amp;nbsp;it?&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Wed, 12 Sep 2012 05:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-09-12:2012/09/some-questions-from-a-tech-interview-with-a-big-internet-company/</guid><category>hiring</category><category>interview</category><category>questions</category><category>sysadmin</category></item><item><title>Dumping all Macros from an RPM SpecÂ File</title><link>http://blog.jasonantman.com/2012/09/dumping-all-macros-from-an-rpm-spec-file/</link><description>&lt;p&gt;I&amp;#8217;ve been doing a lot of &lt;span class="caps"&gt;RPM&lt;/span&gt; packaging lately, and on different (and
very old) distros and versions. Sometimes I lose track of all of the
macros used in specfiles (&lt;code&gt;_bindir _sbindir dist _localstatedir&lt;/code&gt;, etc).
There&amp;#8217;s no terribly easy way to dump a list of all of the available
macros. There is, however, a bit of a kludge. Insert the following code
in your specfile before the &lt;code&gt;%prep&lt;/code&gt; or &lt;code&gt;%setup&lt;/code&gt; lines:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;%dump
exit 1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The &lt;code&gt;%dump&lt;/code&gt; macro will dump all defined macros to &lt;span class="caps"&gt;STDERR&lt;/span&gt;. The &lt;code&gt;exit 1&lt;/code&gt;
will prevent rpmbuild from going on and trying to build the package. If
you want to view the output nicely, you can pipe it through a pager like
less: &lt;code&gt;rpmbuild -ba filename.spec 2&amp;gt;&amp;amp;1 | less&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Just make sure to remove those two lines when you want to actually build
the&amp;nbsp;package.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Mon, 10 Sep 2012 10:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-09-10:2012/09/dumping-all-macros-from-an-rpm-spec-file/</guid><category>packaging</category><category>rpm</category><category>rpmbuild</category></item><item><title>Getting oVirt up andÂ running</title><link>http://blog.jasonantman.com/2012/09/getting-ovirt-up-and-running/</link><description>&lt;p&gt;The bulk of this post was written way back in April 2012. If you&amp;#8217;re just
coming here, and looking to setup oVirt, you should probably &lt;a href="#postscript"&gt;skip down
to the postscript&lt;/a&gt; for an update, and ignore most of the
content here (as it&amp;#8217;s applicable to an older oVirt&amp;nbsp;version).&lt;/p&gt;
&lt;p&gt;I recently started setting up &lt;a href="http://www.ovirt.org"&gt;oVirt&lt;/a&gt;, the
community version of Red Hat Enterprise Virtualization, at work for some
testing (mainly a &amp;#8220;sandbox&amp;#8221; &lt;span class="caps"&gt;VM&lt;/span&gt; environment, and because
&lt;a href="http://theforeman.org/"&gt;Foreman&lt;/a&gt;
&lt;a href="http://blog.theforeman.org/2012/03/vnc-support-built-in-foreman.html"&gt;supports&lt;/a&gt;
it). To start with, I had two nodes, each with two dual-core Xeon
processors (&lt;span class="caps"&gt;VT&lt;/span&gt;-x capable) with &lt;span class="caps"&gt;20GB&lt;/span&gt; &lt;span class="caps"&gt;RAM&lt;/span&gt;, one with &lt;span class="caps"&gt;600GB&lt;/span&gt; internal storage
and one with &lt;span class="caps"&gt;140GB&lt;/span&gt; internal. While oVirt&amp;#8217;s documentation isn&amp;#8217;t exactly
wonderful, I found a blgo post by Jason Brooks, &lt;a href="http://blog.jebpages.com/archives/how-to-get-up-and-running-with-ovirt/"&gt;How to Get Up and
Running with
oVirt&lt;/a&gt;,
which gives a great walkthrough of getting the oVirt Engine setup on a
machine, and also setting up that same machine as a &lt;span class="caps"&gt;VM&lt;/span&gt; host. As oVirt is
still fairly young, this is all done on Fedora. I performed my
installation via Cobbler, though I&amp;#8217;m afraid to admit it was an entirely
manual, interactive&amp;nbsp;install.&lt;/p&gt;
&lt;p&gt;I did run into a few bumps during Jason&amp;#8217;s tutorial. In step 15, adding
the data &lt;span class="caps"&gt;NFS&lt;/span&gt; export as a Storage Domain, I was unable to add the &lt;span class="caps"&gt;NFS&lt;/span&gt;
export. I found the &lt;a href="http://www.ovirt.org/wiki/Troubleshooting_NFS_Storage_Issues"&gt;Troubleshooting &lt;span class="caps"&gt;NFS&lt;/span&gt; Storage Issues page on the
oVirt
wiki&lt;/a&gt;,
ensured that SELinux was disabled and that the export had the correct
permissions, confirmed that &lt;code&gt;/etc/nfsmount.conf&lt;/code&gt; specified &lt;code&gt;Nfsvers=3&lt;/code&gt;,
rebooted, and then ran the &lt;code&gt;nfs-check.py&lt;/code&gt; script. At this point, I was
able to add the other storage domains in steps 15 and&amp;nbsp;16.&lt;/p&gt;
&lt;p&gt;My second issue was that even on Fedora 16, I simply can&amp;#8217;t get the spice
client (through the &lt;code&gt;spice-xpi&lt;/code&gt; browser plugin) to work. As far as I can
tell from the logs, it looks like &lt;code&gt;spicec&lt;/code&gt; is being sent a value of
&amp;#8220;None&amp;#8221; for the secured port parameter, instead of the correct port
number. I assume this is a bug in oVirt, but I&amp;#8217;ll revisit this problem
when I have time. In the mean time, I changed my test &lt;span class="caps"&gt;VM&lt;/span&gt; to use &lt;span class="caps"&gt;VNC&lt;/span&gt;,
which is launched by installing the &lt;code&gt;ovirt-engine-cli&lt;/code&gt; package (see
below) on your client computer, connecting to the oVirt &lt;span class="caps"&gt;API&lt;/span&gt; with&amp;nbsp;ovirt-shell:  &lt;/p&gt;
&lt;p&gt;&lt;code&gt;ovirt-shell --connect --url=https://ovirt-engine.example.com:8443/api --user=admin@internal --password adminpassword&lt;/code&gt;&lt;br /&gt;
and then running &lt;code&gt;console vm_name&lt;/code&gt;. This launches the &lt;code&gt;vncviewer&lt;/code&gt;
binary, which is in the &amp;#8220;tigervnc&amp;#8221; package on&amp;nbsp;Fedora.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Installing&amp;nbsp;ovirt-engine-cli&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;To run &lt;code&gt;ovirt-shell&lt;/code&gt; on your workstation (Fedora 16, of course&amp;#8230;)
you&amp;#8217;ll need the ovirt-engine-cli and ovirt-engine-sdk packages. I
manually downloaded them from
&lt;a href="http://www.ovirt.org/releases/nightly/fedora/16/"&gt;http://www.ovirt.org/releases/nightly/fedora/16/&lt;/a&gt;,
versions 2.1.3 and 1.6.2, respecitively. The &lt;span class="caps"&gt;SDK&lt;/span&gt; and &lt;span class="caps"&gt;CLI&lt;/span&gt; are python
based, so there are a few Python dependencies, all of which were
automatically solved by yum. I know there are &lt;span class="caps"&gt;SDK&lt;/span&gt; and &lt;span class="caps"&gt;CLI&lt;/span&gt; packages out
there for other distros, but haven&amp;#8217;t tried them&amp;nbsp;yet.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Installing Linux&amp;nbsp;Guests&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Installing a CentOS 6.2 x86_64 guest was relatively straightforward,
and my usual kickstart infrastructure worked fine. The only catch was
the VirtIO storage interface, which shows up as &lt;code&gt;/dev/vdx&lt;/code&gt; instead of
&lt;code&gt;/dev/sdx&lt;/code&gt;; I just added another kickstart metadata option in Cobbler
that allows me to use &lt;code&gt;sdx&lt;/code&gt; by specifying &amp;#8220;virtual=yes&amp;#8221; (for our VMWare
hosts), or &lt;code&gt;vdx&lt;/code&gt; by specifying&amp;nbsp;&amp;#8220;virtual=ovirt&amp;#8221;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Setting up&amp;nbsp;Authentication&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;As installed, oVirt only has one user, &amp;#8220;admin@internal&amp;#8221;; it requires an
external directory service for user authentication. Currently, it
supports &lt;span class="caps"&gt;IPA&lt;/span&gt;, Red Hat&amp;#8217;s Enterprise Identity Management tool (combines
&lt;span class="caps"&gt;RHEL&lt;/span&gt;, oVirt Directory Server, Kerberos and &lt;span class="caps"&gt;NTP&lt;/span&gt;; perhaps
&lt;a href="http://freeipa.org"&gt;FreeIPA&lt;/a&gt; would work as well?) and Microsoft Active
Directory. As much as I&amp;#8217;d like to give &lt;span class="caps"&gt;IPA&lt;/span&gt; or FreeIPA a try, my company
already has an &lt;span class="caps"&gt;AD&lt;/span&gt; infrastructure, so I opted to go that route.
Documentation is given in the &lt;a href="http://www.ovirt.org/wiki/File:OVirt-3.0-Installation_Guide-en-US.pdf"&gt;oVirt 3.0 Installation
Guide&lt;/a&gt;,
starting on page 96. Unfortunately, I was never about to get &lt;span class="caps"&gt;AD&lt;/span&gt; auth
working correctly, so I just worked with the one admin&amp;nbsp;user.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Adding a&amp;nbsp;Node&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The biggest issue I had was adding the second node to oVirt. I attempted
to use the &lt;span class="caps"&gt;DVD&lt;/span&gt; Import feature of Cobbler on the &lt;a href="http://www.ovirt.org/get-ovirt/"&gt;oVirt Node Image
&lt;span class="caps"&gt;ISO&lt;/span&gt;&lt;/a&gt;, but that failed. I then found the
image&amp;#8217;s &lt;code&gt;LiveOS/livecd-iso-to-pxeboot&lt;/code&gt; script and used that to make a
kernerl and initrd, and kernel parameters, for Cobbler. &lt;span class="caps"&gt;PXE&lt;/span&gt; works&amp;nbsp;fine.&lt;/p&gt;
&lt;p&gt;&lt;a name="postscript"&gt;&lt;/a&gt;&lt;strong&gt;Postscript:&lt;/strong&gt; I ended up blowing away my
oVirt installation in favor of testing other things. At some point, the
engine install got corrupted in a way that I just couldn&amp;#8217;t fix; even
though I spent all day one Saturday working on it, it took more time
than I could allocate to a personal project. So this post is really
semi-complete at best. However, there is some good news. Jason Brooks&amp;#8217;
original post, &lt;a href="http://blog.jebpages.com/archives/how-to-get-up-and-running-with-ovirt/"&gt;How to Get Up and Running with
oVirt&lt;/a&gt;,
was written for oVirt 3.0, as was this post. Since then, there has been
a new release, &lt;a href="http://wiki.ovirt.org/wiki/OVirt_3.1_release_notes"&gt;oVirt
3.1&lt;/a&gt;, which
apparently has a better &lt;span class="caps"&gt;UI&lt;/span&gt; and a better installer. Jason Brooks has a
new post, &lt;a href="http://blog.jebpages.com/archives/up-and-running-with-ovirt-3-1-edition/"&gt;Up and Running with oVirt, 3.1
Edition&lt;/a&gt;,
which covers installation and configuration of both an all-in-one
machine and a separate node. If you&amp;#8217;re looking to try oVirt, I&amp;#8217;d
recommend you give that a shot. Unfortunately (and strangely, given that
this is supposed to be the &amp;#8220;upstream&amp;#8221; of RedHat&amp;#8217;s proprietary &lt;span class="caps"&gt;RHEV&lt;/span&gt;) it&amp;#8217;s
still all based on&amp;nbsp;Fedora.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Fri, 07 Sep 2012 05:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-09-07:2012/09/getting-ovirt-up-and-running/</guid><category>fedora</category><category>kvm</category><category>ovirt</category><category>qemu</category><category>redhat</category><category>rhev</category><category>spice</category><category>virtualization</category></item><item><title>Project - Storing and Analyzing Apache httpd Logs from ManyÂ Hosts</title><link>http://blog.jasonantman.com/2012/09/project-storing-and-analyzing-apache-httpd-logs-from-many-hosts/</link><description>&lt;p&gt;I&amp;#8217;ve recently started casual work on a side-project to collect, store,
and analyze apache logs from a bunch of servers - for the initial
implementation, I&amp;#8217;m looking to handle about 15M access_log lines per
day (that works out to 173 lines/second assuming an even distribution,
which there certainly isn&amp;#8217;t). Here is a selection of links that I&amp;#8217;ve
been using for ideas and inspiration, both for the technical side (data
collection, transport, storage and analysis) and&amp;nbsp;visualization:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://oss.oetiker.ch/rrdtool/gallery/index.en.html"&gt;RRDtool - RRDtool
    Gallery&lt;/a&gt; - I&amp;#8217;m
    starting a graphing/log analysis project, and looked here for some
    inspiration for my proof-of-concept&amp;nbsp;code&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aplawrence.com/Girish/gv-rrdtool.html"&gt;Creating pretty graphs with
    &lt;span class="caps"&gt;RRDTOOL&lt;/span&gt;&lt;/a&gt; from &lt;a href=""&gt;Girish
    Venkatachalam&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;There&amp;#8217;s some good information on RRDtool&amp;#8217;s &amp;#8220;Abberant Behavior
    Detection&amp;#8221; (Holt-Winters prediction, deviation and failure
    detection) on the
    &lt;a href="http://oss.oetiker.ch/rrdtool/doc/rrdtool.en.html"&gt;rrdtool&lt;/a&gt;,
    &lt;a href="http://oss.oetiker.ch/rrdtool/doc/rrdgraph_examples.en.html"&gt;rrdgraph_examples&lt;/a&gt;
    and &lt;a href="http://oss.oetiker.ch/rrdtool/doc/rrdcreate.en.html"&gt;rrdcreate&lt;/a&gt;
    documentation pages, but unfortunately no anchors to link directly&amp;nbsp;to.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://square.github.com/cube/"&gt;Cube&lt;/a&gt; - &amp;#8220;Cube is a system for
    collecting timestamped events and deriving metrics. By collecting
    events rather than metrics, Cube lets you compute aggregate
    statistics post hoc. It also enables richer analysis, such as
    quantiles and histograms of arbitrary event sets. Cube is built on
    MongoDB and available under the Apache License on&amp;nbsp;GitHub.&amp;#8221;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://square.github.com/cubism/"&gt;Cubism.js&lt;/a&gt; - &amp;#8220;Cubism.js is a D3
    plugin for visualizing time series. Use Cubism to construct better
    realtime dashboards, pulling data from Graphite, Cube and other
    sources. Cubism is available under the Apache License on GitHub.&amp;#8221;
    The demo on that page looks pretty&amp;nbsp;cool.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.highcharts.com/demo/"&gt;Highcharts Demo Gallery&lt;/a&gt; - &lt;span class="caps"&gt;JS&lt;/span&gt;
    chart/graph library. It requires a paid license for commercial use
    (though it&amp;#8217;s a bit unclear to me whether an internal ops dashboard
    would fall under this license provision) so I probably wouldn&amp;#8217;t go
    with this one. They have some cool charts, including a &lt;a href="http://www.highcharts.com/demo/dynamic-update/gray"&gt;dynamic line
    chart updating every
    second&lt;/a&gt;, a
    &lt;a href="http://www.highcharts.com/demo/scatter/gray"&gt;scatter plot&lt;/a&gt; and a
    nice &lt;a href="http://www.highcharts.com/demo/line-time-series/gray"&gt;zoomable time-series
    graph&lt;/a&gt;, though
    &lt;span class="caps"&gt;IMHO&lt;/span&gt; it&amp;#8217;s not as nice as the Google Chart Tools (formerly Google
    Visualization) &lt;a href="https://developers.google.com/chart/interactive/docs/gallery/annotatedtimeline"&gt;annotated
    timeline&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://forums.cacti.net/viewtopic.php?t=29963"&gt;[ &lt;span class="caps"&gt;HOWTO&lt;/span&gt; ] Graphing Holt-Winters Predictive
    Analysis&lt;/a&gt; - Cacti&amp;nbsp;forums&lt;/li&gt;
&lt;li&gt;&lt;a href="http://dygraphs.com/"&gt;dygraphs&lt;/a&gt; - an impressive permissive-license
    &lt;span class="caps"&gt;JS&lt;/span&gt; chart library dedicated to visualizing dense time-series data.
    Developed by Google and now used by them (Google Correlate, Google
    Latitude) as well as &lt;span class="caps"&gt;NASA&lt;/span&gt;, 10gen and others. There are some very
    cool demos on that main page, and also on the &lt;a href="http://dygraphs.com/tests/"&gt;tests
    page&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.planetdevops.net/?p=12289"&gt;Graphite, JMXTrans, Ganglia, Logster, Collectd, say what ? Â« Planet&amp;nbsp;DevOps&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://auxesis.github.com/visage/"&gt;Visage&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/kgorman/mongo_graph"&gt;kgorman/mongo_graph&lt;/a&gt; - a
    tool to pull data from MongoDB and put it in &lt;span class="caps"&gt;RRD&lt;/span&gt;&amp;nbsp;files&lt;/li&gt;
&lt;li&gt;&lt;a href="http://web.taranis.org/drraw/"&gt;drraw&lt;/a&gt; - a perl-based graphing
    frontend (web &lt;span class="caps"&gt;UI&lt;/span&gt;) for&amp;nbsp;RRDtool&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/etsy/logster"&gt;etsy/logster Â· GitHub&lt;/a&gt; - Etsy&amp;#8217;s
    Python tool to maintain a pointer on a log file, and parse at a
    regular rate feeding the data into a tool like Graphite or&amp;nbsp;Ganglia.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cebailey59/charcoal"&gt;cebailey59/charcoal&lt;/a&gt; - a
    Sinatra app that allows creation of dashboards from Graphite,
    collectd, or any other service that creates images from &lt;span class="caps"&gt;URL&lt;/span&gt;&amp;nbsp;calls.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/etsy/dashboard"&gt;etsy/dashboard&lt;/a&gt; - some examples
    of how Etsy builds monitoring&amp;nbsp;dashboards.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.devco.net/archives/2011/10/08/gdash-graphite-dashboard.php"&gt;GDash â Graphite Dashboard |
    &lt;span class="caps"&gt;R.I.&lt;/span&gt;Pienaar&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;a Sinatra dashboard app for Graphite, using Twitter bootstrap for&amp;nbsp;visualization.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/paperlesspost/graphiti"&gt;paperlesspost/graphiti&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;a Ruby and JavaScript front-end for&amp;nbsp;Graphite.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://graphite.wikidot.com/screen-shots"&gt;Graphite Screenshots&lt;/a&gt; -
    just two, but they get the idea across pretty&amp;nbsp;well.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://graylog2.org/"&gt;Graylog2&lt;/a&gt; - a centralized log management
    application with a powerful web interface. Stores logs in
    &lt;a href="http://www.elasticsearch.org/"&gt;ElasticSearch&lt;/a&gt; (which is built on
    Lucene, a Java-based index and search server) and statistics/graphs
    in MongoDB. It does analytics, alerting, monitoring/graphing and
    searching all through a web interface, and accepts log data via
    syslog, &lt;span class="caps"&gt;AMQP&lt;/span&gt; and &lt;span class="caps"&gt;GELF&lt;/span&gt; (its own log format). Java server and Ruby on
    Rails web&amp;nbsp;&lt;span class="caps"&gt;UI&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://logstash.net/"&gt;Logstash&lt;/a&gt; - another centralized log project
    that stores and indexes logs, with search via a web &lt;span class="caps"&gt;UI&lt;/span&gt;. &amp;#8220;Ship any
    event to anywhere over any protocol.&amp;#8221; Takes many inputs including
    files, syslog, &lt;span class="caps"&gt;AMQP&lt;/span&gt;, Flume, &lt;span class="caps"&gt;STOMP&lt;/span&gt;, &lt;span class="caps"&gt;HTTP&lt;/span&gt; and even twitter, performs a
    number of filters including timestamp checks, parsing, dropping,
    joins, etc, and then sends logs back on an output including &lt;span class="caps"&gt;AMQP&lt;/span&gt;,
    Graylog2 &lt;span class="caps"&gt;GELF&lt;/span&gt;, &lt;span class="caps"&gt;STOMP&lt;/span&gt;, MongoDB, ElasticSearch, syslog, WebSockets and
    to Nagios. One particularly cool feature is its &amp;#8220;file&amp;#8221; input, which
    continuously tails a file and claims to be log rotation safe. Just&amp;nbsp;cool.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://docs.google.com/present/view?id=dcmwwd94_16dfdxgpw8"&gt;jordansissel&amp;#8217;s Logstash intro
    slides&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://rashidkpc.github.com/Kibana/"&gt;Kibana&lt;/a&gt; - an alternative
    interface for Logstash and
    &lt;a href="http://www.elasticsearch.org/"&gt;ElasticSearch&lt;/a&gt; that allows
    searching, graphing and analysis of log data stored in&amp;nbsp;Logstash.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://pivotallabs.com/talks/139-metrics-metrics-everywhere"&gt;Pivotal Labs: Talks - Metrics Metrics
    Everywhere&lt;/a&gt;
    (Coda&amp;nbsp;Hale)&lt;/li&gt;
&lt;li&gt;&lt;a href="http://aq.iriscouch.com/swinger/_design/swinger/index.html#/preso/aq-mdd/display/1"&gt;PaperlessPost - @quirkey&amp;#8217;s talk on
    metrics&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;very good high level stuff, but slides&amp;nbsp;only&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/paperlesspost/graphiti"&gt;paperlesspost/graphiti&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;graphiti, a &lt;span class="caps"&gt;JS&lt;/span&gt;/Ruby frontend for Graphite that does graphs,
dashboards, and point-in-time snapshots of graphs. Lots of&amp;nbsp;functionality.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://redis.io/"&gt;Redis&lt;/a&gt; - a distributed key/value store that&amp;#8217;s
    really popular with the cool kids. &lt;a href="http://nosql.mypopescu.com/post/8652869828/another-redis-use-case-centralized-logging"&gt;Another Redis Use Case:
    Centralized Logging â¢&amp;nbsp;myNoSQL&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/cebailey59/charcoal"&gt;Charcoal&lt;/a&gt; - a
    &lt;a href="http://www.sinatrarb.com/"&gt;Sinatra&lt;/a&gt; (Ruby) dashboard app (ready for
    use on &lt;a href="http://www.heroku.com/"&gt;Heroku&lt;/a&gt; but usable anywhere).
    Graphite-oriented but will work with any tool that generates images
    from&amp;nbsp;URLs.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/etsy/logster"&gt;etsy/logster&lt;/a&gt; - etsy&amp;#8217;s Logster
    tool, which keeps a tail on log files, parses them, and ships
    metrics to Graphite or&amp;nbsp;Ganglia.&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Thu, 06 Sep 2012 05:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-09-06:2012/09/project-storing-and-analyzing-apache-httpd-logs-from-many-hosts/</guid></item><item><title>Some PowerDNS Links and InterestingÂ Features</title><link>http://blog.jasonantman.com/2012/09/some-powerdns-links-and-interesting-features/</link><description>&lt;p&gt;At $&lt;span class="caps"&gt;WORK&lt;/span&gt; we lost a disk in the &lt;span class="caps"&gt;RAID1&lt;/span&gt; of one of our external nameservers,
and it rekindled an occasional discussion of migration from &lt;a&gt;&lt;span class="caps"&gt;ISC&lt;/span&gt;
&lt;span class="caps"&gt;BIND&lt;/span&gt;&lt;/a&gt; to &lt;a href="http://powerdns.com/content/products.html"&gt;PowerDNS&lt;/a&gt;.
PowerDNS has separate authoritative and recursive servers, and doesn&amp;#8217;t
seem to natively support views or split-horizon the way &lt;span class="caps"&gt;BIND&lt;/span&gt; does, but
it has some really cool features including very mature database
backends, load balancing, Lua scripting support to modify how recursive
queries are answered, and geolocation or &lt;span class="caps"&gt;IP&lt;/span&gt;-range based query&amp;nbsp;results.&lt;/p&gt;
&lt;p&gt;While this project is still just casual research, I thought I&amp;#8217;d share
some of the useful links and information I&amp;#8217;ve&amp;nbsp;found:&lt;/p&gt;
&lt;p&gt;PowerDNS&amp;nbsp;Front-ends:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://www.nicmus.com/community.html"&gt;JPowerAdmin&lt;/a&gt; - One of the two
    most popular, a GPLv3 Java (JBoss &lt;span class="caps"&gt;SEAM&lt;/span&gt;) based web &lt;span class="caps"&gt;UI&lt;/span&gt; with a RESTful
    &lt;span class="caps"&gt;API&lt;/span&gt;, with support for &amp;#8220;multiple&amp;#8221; database backends. Sponsored by
    Nicmus, Inc. &lt;a href="http://www.nicmus.com/JPowerAdmin"&gt;Online demo&lt;/a&gt;
    (demo:demo). Looks nice, simple &lt;span class="caps"&gt;UI&lt;/span&gt;, but no support for&amp;nbsp;split-horizon.&lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.poweradmin.org"&gt;PowerAdmin&lt;/a&gt; - the other most popular,
    though it seems to be undergoing a large overhaul at the moment. Has
    full support for most of PowerDNS&amp;#8217;s features, written in &lt;span class="caps"&gt;PHP&lt;/span&gt;,
    supports &amp;#8220;large&amp;#8221; databases, fine-grained user permissions, &lt;span class="caps"&gt;RFC&lt;/span&gt;
    validation, zone templates. &lt;a href="http://demo.poweradmin.org/"&gt;Online
    demo&lt;/a&gt; (demo:demo). I don&amp;#8217;t really like
    that it manages the SOAs as full text (without any templating,
    dropdowns or default values), and that it doesn&amp;#8217;t prepopulate
    default values for &lt;span class="caps"&gt;TTL&lt;/span&gt; in the new record form, but it looks like a
    good starting place for someone (like me) who&amp;#8217;s handy with&amp;nbsp;&lt;span class="caps"&gt;PHP&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://code.google.com/p/pdns-gui/"&gt;pdns-gui - PowerDNS &lt;span class="caps"&gt;GUI&lt;/span&gt; - Google Project
    Hosting&lt;/a&gt; - &lt;span class="caps"&gt;PHP&lt;/span&gt;/MySQL &lt;span class="caps"&gt;GUI&lt;/span&gt;.
    &lt;a href="http://www.powerdns-gui.org/"&gt;Online demo&lt;/a&gt;. Handles templates
    nicely but won&amp;#8217;t scale to too many of them. Window-based &lt;span class="caps"&gt;UI&lt;/span&gt; is
    visually pleasing but will probably be a problem for big&amp;nbsp;zones.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://code.google.com/p/powerdns-webinterface/"&gt;powerdns-webinterface - PowerDNS Webinterface - Google Project
    Hosting&lt;/a&gt; - A nice
    but relatively simplistic &lt;span class="caps"&gt;UI&lt;/span&gt; written in &lt;span class="caps"&gt;PHP&lt;/span&gt;. It has some nice
    features like multi-user authentication (and logging, though I
    haven&amp;#8217;t looked into how detailed it is), automatic &lt;span class="caps"&gt;SOA&lt;/span&gt; serial
    update, automatic &lt;span class="caps"&gt;PTR&lt;/span&gt; creation, etc. Unfortunately not geared
    towards people with lots of domains and multiple records; it has
    only one template for new domains (and no way to update domains
    created from a template), no easy filtering, and still treats &lt;span class="caps"&gt;SOA&lt;/span&gt;
    like a single text&amp;nbsp;record.&lt;/li&gt;
&lt;li&gt;&lt;a href="http://sourceforge.net/projects/zoneadmin/"&gt;ZoneAdmin |
    SourceForge.net&lt;/a&gt; and
    &lt;a&gt;Project website&lt;/a&gt; - Maybe not the fastest tool to use in bulk,
    but a nice, relatively intuitive and full-featured admin tool.
    &lt;a href="http://open.megabit.net/demos/ZoneAdmin/"&gt;Online demo&lt;/a&gt;&amp;nbsp;(demo:demo).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Some links on PowerDNS&amp;nbsp;split-horizon&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://old.nabble.com/Split-Horizon-Scripts-td32843045.html"&gt;Old Nabble - PowerDNS - Split Horizon&amp;nbsp;Scripts&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It looks to me that split-horizon is going to be the hardest part for
us, at least to also have a web &lt;span class="caps"&gt;UI&lt;/span&gt; to manage it. It looks like with
PowerDNS, the most common way to run split horizon &lt;span class="caps"&gt;DNS&lt;/span&gt; (views) is to run
two separate sets of servers or instances, either on different boxes or
multi-homed; one for internal and one for external. While that sounds
like quite a bit of overhead beyond what &lt;span class="caps"&gt;BIND&lt;/span&gt; does, the real problem is
finding a web &lt;span class="caps"&gt;UI&lt;/span&gt; that supports it; I don&amp;#8217;t care if it&amp;#8217;s in two separate
databases, but what I want is a logical (web &lt;span class="caps"&gt;UI&lt;/span&gt;) view that has zones
made up of resource names (i.e. the leftmost column in a zone file) with
one or two RRs (type, ttl, priority, value) - one for each view. That&amp;#8217;s
the real catch - all of our machines are in private &lt;span class="caps"&gt;IP&lt;/span&gt; space behind a
firewall, so I need to be able to manage the internal and external
records on one screen. While it&amp;#8217;s not exactly scalable, and the code
stagnated quite a bit once I got it to a point that was usable for me,
this was the main goal of my &lt;a href="http://multibindadmin.jasonantman.com/"&gt;MultiBIND
Admin&lt;/a&gt;&amp;nbsp;project.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Wed, 05 Sep 2012 05:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-09-05:2012/09/some-powerdns-links-and-interesting-features/</guid><category>bind</category><category>dns</category><category>multibindadmin</category><category>powerdns</category></item><item><title>Wordpress - Automatically publish a pending post each weekday morning from a PHPÂ script</title><link>http://blog.jasonantman.com/2012/09/wordpress-automatically-publish-a-pending-post-each-weekday-morning-from-a-php-script/</link><description>&lt;p&gt;In an earlier post, &lt;a href="/2012/08/piwik-web-analytics-and-some-unfortunate-stats-about-my-blog/"&gt;Piwik Web Analytics, and some unfortunate stats
about my
blog&lt;/a&gt;,
I mentioned that the &lt;a href="http://feedburner.google.com/"&gt;Feedburner&lt;/a&gt; stats
for this blog show a relatively high subscribe/unsubscribe rate for this
blog. I think a large part of that is my tendency to blog in spurts, and
even worse, my tendency to write drafts and not publish them. In an
effort to combat this, I&amp;#8217;ve been trying to finish blog posts and then
set them to &amp;#8220;Pending&amp;#8221; status, and go back and publish one every day
(well, every day that I have some still sitting unpublished). Of course,
that counts on me logging in to Wordpress every day, which isn&amp;#8217;t
something I do. The following script is, at least for now, the answer
for&amp;nbsp;me.&lt;/p&gt;
&lt;p&gt;This script (a standalone &lt;span class="caps"&gt;PHP&lt;/span&gt; script) uses
&lt;a href="http://core.trac.wordpress.org/browser/trunk/wp-load.php"&gt;&lt;code&gt;wp-load.php&lt;/code&gt;&lt;/a&gt;
to load the wordpress environment, and then finds the oldest post with a
given status (&amp;#8220;pending&amp;#8221; in my case) and attempts to publish it. It only
does this if there has not been another post published in the last 24
hours. The following script can be found in Git at
&lt;a href="https://github.com/jantman/misc-scripts/blob/master/wordpress_daily_post.php"&gt;https://github.com/jantman/misc-scripts/blob/master/wordpress_daily_post.php&lt;/a&gt;&lt;/p&gt;
&lt;!---
sourceinclude
---&gt;

&lt;div class="codehilite"&gt;&lt;pre&gt;&lt;span class="x"&gt;#!/usr/bin/php&lt;/span&gt;
&lt;span class="cp"&gt;&amp;lt;?php&lt;/span&gt;
&lt;span class="sd"&gt;/**&lt;/span&gt;
&lt;span class="sd"&gt; * wordpress_daily_post.php&lt;/span&gt;
&lt;span class="sd"&gt; * Script to publish the oldest post with a given status, if no&lt;/span&gt;
&lt;span class="sd"&gt; * other post has been published in 24 hours. Intended to be run&lt;/span&gt;
&lt;span class="sd"&gt; * via cron on weekdays.&lt;/span&gt;
&lt;span class="sd"&gt; *&lt;/span&gt;
&lt;span class="sd"&gt; * Copyright 2012 Jason Antman &lt;/span&gt;
&lt;span class="sd"&gt; *&lt;/span&gt;
&lt;span class="sd"&gt; * Licensed under the Apache License, Version 2.0 &lt;/span&gt;
&lt;span class="sd"&gt; *&lt;/span&gt;
&lt;span class="sd"&gt; * use it anywhere you want, however you want, provided that this header is left intact,&lt;/span&gt;
&lt;span class="sd"&gt; * and that if redistributed, credit is given to me.&lt;/span&gt;
&lt;span class="sd"&gt; *&lt;/span&gt;
&lt;span class="sd"&gt; * It is strongly requested, but not technically required, that any changes/improvements&lt;/span&gt;
&lt;span class="sd"&gt; * be emailed to the above address.&lt;/span&gt;
&lt;span class="sd"&gt; *&lt;/span&gt;
&lt;span class="sd"&gt; * The latest version of this script will always be available at:&lt;/span&gt;
&lt;span class="sd"&gt; * $HeadURL: http://svn.jasonantman.com/misc-scripts/wordpress_daily_post.php $&lt;/span&gt;
&lt;span class="sd"&gt; * $LastChangedRevision: 40 $&lt;/span&gt;
&lt;span class="sd"&gt; *&lt;/span&gt;
&lt;span class="sd"&gt; * Changelog:&lt;/span&gt;
&lt;span class="sd"&gt; * 2012-09-03 Jason Antman  - 1.0&lt;/span&gt;
&lt;span class="sd"&gt; *  - first version&lt;/span&gt;
&lt;span class="sd"&gt; */&lt;/span&gt;

&lt;span class="c1"&gt;# &lt;span class="caps"&gt;BEGIN&lt;/span&gt; &lt;span class="caps"&gt;CONFIGURATION&lt;/span&gt;&lt;/span&gt;
&lt;span class="nb"&gt;define&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;WP_LOAD_LOC&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;/var/www/vhosts/blog.jasonantman.com/wp-load.php&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// Configure this to the full path of your Wordpress wp-load.php&lt;/span&gt;
&lt;span class="nb"&gt;define&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;SOURCE_POST_STATUS&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;pending&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// post status to publish&lt;/span&gt;
&lt;span class="c1"&gt;# &lt;span class="caps"&gt;END&lt;/span&gt; &lt;span class="caps"&gt;CONFIGURATION&lt;/span&gt;&lt;/span&gt;

&lt;span class="nv"&gt;$&lt;span class="caps"&gt;VERBOSE&lt;/span&gt;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;false&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;$DRY_RUN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;false&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nb"&gt;array_shift&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$argv&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;while&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$argv&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;isset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nv"&gt;$argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;-d&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="nv"&gt;$argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--dry-run&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
    &lt;span class="nv"&gt;$DRY_RUN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;true&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;DRY&lt;/span&gt; &lt;span class="caps"&gt;RUN&lt;/span&gt; &lt;span class="caps"&gt;ONLY&lt;/span&gt; - &lt;span class="caps"&gt;NOT&lt;/span&gt; &lt;span class="caps"&gt;ACTUALLY&lt;/span&gt; &lt;span class="caps"&gt;PUBLISHING&lt;/span&gt;.&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;isset&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="nv"&gt;$argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;-v&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;||&lt;/span&gt; &lt;span class="nv"&gt;$argv&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;--verbose&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
    &lt;span class="nv"&gt;$&lt;span class="caps"&gt;VERBOSE&lt;/span&gt;&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;true&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;WP_LOAD_LOC=&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;WP_LOAD_LOC&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;SOURCE_POST_STATUS=&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;SOURCE_POST_STATUS&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="nb"&gt;array_shift&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$argv&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nv"&gt;$_SERVER&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;HTTP_HOST&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;localhost&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt; &lt;span class="c1"&gt;// needed for wp-includes/ms-settings.php:100&lt;/span&gt;
&lt;span class="k"&gt;require_once&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;WP_LOAD_LOC&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="c1"&gt;# check that we&amp;#39;re running on a weekday&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;N&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
&lt;span class="c1"&gt;#  if($&lt;span class="caps"&gt;VERBOSE&lt;/span&gt;){ fwrite(&lt;span class="caps"&gt;STDERR&lt;/span&gt;, &amp;quot;today is a saturday or sunday, dieing.\n&amp;quot;); }&lt;/span&gt;
&lt;span class="c1"&gt;#  exit(1);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# find the publish date/time of the last published post&lt;/span&gt;
&lt;span class="nv"&gt;$published&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;get_posts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;numberposts&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;orderby&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;post_date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;order&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;span class="caps"&gt;DESC&lt;/span&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;post_status&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;publish&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="nv"&gt;$post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;$published&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="nv"&gt;$pub_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;$post&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;post_date&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;$pub_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;$post&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;&lt;span class="caps"&gt;ID&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;strtotime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$pub_date&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;time&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mi"&gt;86400&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;VERBOSE&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;last post (&lt;span class="caps"&gt;ID&lt;/span&gt; &lt;/span&gt;&lt;span class="si"&gt;$pub_id&lt;/span&gt;&lt;span class="s2"&gt;) within last day (&lt;/span&gt;&lt;span class="si"&gt;$pub_date&lt;/span&gt;&lt;span class="s2"&gt;). Nothing to do. Exiting.&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;VERBOSE&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Found last post (&lt;span class="caps"&gt;ID&lt;/span&gt; &lt;/span&gt;&lt;span class="si"&gt;$pub_id&lt;/span&gt;&lt;span class="s2"&gt;) with post date &lt;/span&gt;&lt;span class="si"&gt;$pub_date&lt;/span&gt;&lt;span class="s2"&gt;.&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;


&lt;span class="c1"&gt;# find the earliest post of status SOURCE_POST_STATUS, if there is one.&lt;/span&gt;
&lt;span class="nv"&gt;$to_post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;get_posts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;numberposts&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;orderby&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;post_date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;order&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;span class="caps"&gt;ASC&lt;/span&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;post_status&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nx"&gt;SOURCE_POST_STATUS&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;count&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$to_post&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="nx"&gt;&lt;span class="caps"&gt;ID&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;$to_pub_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;$post&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;post_date&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;$to_pub_title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;$post&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;post_title&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;$now&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;time&lt;/span&gt;&lt;span class="p"&gt;();&lt;/span&gt;
&lt;span class="nv"&gt;$new_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;date&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Y-m-d H:i:s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nv"&gt;$now&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nv"&gt;$new_date_gmt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;gmdate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Y-m-d H:i:s&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nv"&gt;$now&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;VERBOSE&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Post to publish: &lt;span class="caps"&gt;ID&lt;/span&gt;=&lt;/span&gt;&lt;span class="si"&gt;$to_pub_id&lt;/span&gt;&lt;span class="s2"&gt; &lt;span class="caps"&gt;DATE&lt;/span&gt;=&lt;/span&gt;&lt;span class="si"&gt;$to_pub_date&lt;/span&gt;&lt;span class="s2"&gt; NEW_DATE=&lt;/span&gt;&lt;span class="si"&gt;$new_date&lt;/span&gt;&lt;span class="s2"&gt; &lt;span class="caps"&gt;TITLE&lt;/span&gt;=&lt;/span&gt;&lt;span class="si"&gt;$to_pub_title&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# actually publish it&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;!&lt;/span&gt; &lt;span class="nv"&gt;$DRY_RUN&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt;
  &lt;span class="nv"&gt;$arr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;&lt;span class="caps"&gt;ID&lt;/span&gt;&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nv"&gt;$to_pub_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;post_status&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;publish&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;post_date&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nv"&gt;$new_date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;post_date_gmt&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="nv"&gt;$new_date_gmt&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="nv"&gt;$ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;wp_update_post&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$arr&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="c1"&gt;// publish the post&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$ret&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;ERROR&lt;/span&gt;: Post &lt;/span&gt;&lt;span class="si"&gt;$to_pub_id&lt;/span&gt;&lt;span class="s2"&gt; was not successfully published.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
    &lt;span class="k"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="p"&gt;}&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$&lt;span class="caps"&gt;VERBOSE&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;){&lt;/span&gt; &lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Published post. New &lt;span class="caps"&gt;ID&lt;/span&gt;: &lt;/span&gt;&lt;span class="si"&gt;$ret&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt; &lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;else&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Dry run only, not publishing post.&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# check that the post really was published&lt;/span&gt;
&lt;span class="nv"&gt;$published&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nx"&gt;get_posts&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;array&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;numberposts&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;orderby&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;post_date&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;order&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;&lt;span class="caps"&gt;DESC&lt;/span&gt;&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;post_status&amp;#39;&lt;/span&gt; &lt;span class="o"&gt;=&amp;gt;&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;publish&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;));&lt;/span&gt;
&lt;span class="nv"&gt;$post&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;$published&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="nv"&gt;$pub_date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;$post&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;post_date&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;$pub_id&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;$post&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;&lt;span class="caps"&gt;ID&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;$pub_title&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;$post&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;post_title&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;span class="nv"&gt;$pub_guid&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nv"&gt;$post&lt;/span&gt;&lt;span class="o"&gt;-&amp;gt;&lt;/span&gt;&lt;span class="na"&gt;guid&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="k"&gt;if&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;$pub_title&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="nv"&gt;$to_pub_title&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
  &lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDERR&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;span class="caps"&gt;ERROR&lt;/span&gt;: title of most recent post does not match title of what we wanted to post.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
  &lt;span class="k"&gt;exit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDOUT&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Published post &lt;/span&gt;&lt;span class="si"&gt;$pub_id&lt;/span&gt;&lt;span class="s2"&gt; at &lt;/span&gt;&lt;span class="si"&gt;$pub_date&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDOUT&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Title: &lt;/span&gt;&lt;span class="si"&gt;$pub_title&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDOUT&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n\n\n&lt;/span&gt;&lt;span class="s2"&gt; &lt;span class="caps"&gt;GUID&lt;/span&gt;/Link: &lt;/span&gt;&lt;span class="si"&gt;$pub_guid&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="nb"&gt;fwrite&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nx"&gt;&lt;span class="caps"&gt;STDOUT&lt;/span&gt;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="k"&gt;__FILE__&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; on &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nx"&gt;trim&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;shell_exec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;hostname --fqdn&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot; running as &amp;quot;&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="nb"&gt;get_current_user&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;

&lt;span class="cp"&gt;?&amp;gt;&lt;/span&gt;&lt;span class="x"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;You&amp;#8217;ll need to set &lt;code&gt;WP_LOAD_LOC&lt;/code&gt; (line 29) to the full path of your
Wordpress installation&amp;#8217;s &lt;code&gt;wp-load.php&lt;/code&gt; (it should be in the top-level
directory of your Wordpress installation. I run this script from cron&amp;nbsp;like:&lt;/p&gt;
&lt;div class="codehilite"&gt;&lt;pre&gt;0 6 * * 1-5 /home/jantman/bin/wordpress_daily_post.php --verbose # publish WP pending posts daily
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;so that it runs at &lt;span class="caps"&gt;6AM&lt;/span&gt; (local time) each weekday. Assuming you have cron
setup to send you mail, you&amp;#8217;ll get a daily message saying what was (or
wasn&amp;#8217;t)&amp;nbsp;done.&lt;/p&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Tue, 04 Sep 2012 05:00:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-09-04:2012/09/wordpress-automatically-publish-a-pending-post-each-weekday-morning-from-a-php-script/</guid><category>cron</category><category>PHP</category><category>wordpress</category></item><item><title>Interesting Systems Links for September 3,Â 2012</title><link>http://blog.jasonantman.com/2012/09/interesting-systems-links-for-september-3-2012/</link><description>&lt;p&gt;Here is a small selection of sysadmin links that I recently found, and
wanted to&amp;nbsp;share:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="http://codeascraft.etsy.com/2012/05/22/blameless-postmortems/"&gt;Blameless PostMortems and a Just Culture - Code as
    Craft&lt;/a&gt;&lt;ul&gt;
&lt;li&gt;some really good ideas about a culture that recognizes and seeks
to remedy human errors, rather than punishing and generating&amp;nbsp;fear.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href="http://agilesysadmin.net/pillar-one"&gt;The first pillar: We alert on what we draw - Agile&amp;nbsp;Sysadmin&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.masterzen.fr/2010/11/14/puppet-ssl-explained/"&gt;Puppet &lt;span class="caps"&gt;SSL&lt;/span&gt; explained - Masterzen&amp;#8217;s&amp;nbsp;Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://www.infoq.com/news/2011/05/unix-orchestration"&gt;Unix Orchestration Roundup: Tools for Programmatic Systems&amp;nbsp;Administration&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</description><dc:creator xmlns:dc="http://purl.org/dc/elements/1.1/">admin</dc:creator><pubDate>Mon, 03 Sep 2012 09:42:00 -0400</pubDate><guid>tag:blog.jasonantman.com,2012-09-03:2012/09/interesting-systems-links-for-september-3-2012/</guid><category>devops</category><category>links</category><category>portmortem</category><category>sysadmin</category></item></channel></rss>